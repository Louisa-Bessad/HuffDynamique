<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.4/ http://www.mediawiki.org/xml/export-0.4.xsd" version="0.4" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <base>http://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.16alpha-wmf</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2">Media</namespace>
      <namespace key="-1">Special</namespace>
      <namespace key="0" />
      <namespace key="1">Talk</namespace>
      <namespace key="2">User</namespace>
      <namespace key="3">User talk</namespace>
      <namespace key="4">Wikipedia</namespace>
      <namespace key="5">Wikipedia talk</namespace>
      <namespace key="6">File</namespace>
      <namespace key="7">File talk</namespace>
      <namespace key="8">MediaWiki</namespace>
      <namespace key="9">MediaWiki talk</namespace>
      <namespace key="10">Template</namespace>
      <namespace key="11">Template talk</namespace>
      <namespace key="12">Help</namespace>
      <namespace key="13">Help talk</namespace>
      <namespace key="14">Category</namespace>
      <namespace key="15">Category talk</namespace>
      <namespace key="100">Portal</namespace>
      <namespace key="101">Portal talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Geometric algorithms</title>
    <id>702582</id>
    <revision>
      <id>328369219</id>
      <timestamp>2009-11-28T12:25:28Z</timestamp>
      <contributor>
        <username>Charvest</username>
        <id>8623105</id>
      </contributor>
      <text xml:space="preserve">This category deals with '''[[algorithm]]s''' in '''[[geometry]]'''. See also &quot;[[Computational geometry]]&quot;.

[[Category:Algorithms]]
[[Category:Computational geometry]]
[[Category:Computer graphics algorithms]]

[[ar:تصنيف:خوارزميات هندسة رياضية]]
[[es:Categoría:Algoritmos geométricos]]
[[eo:Kategorio:Geometriaj algoritmoj]]
[[fr:Catégorie:Algorithme géométrique]]
[[it:Categoria:Algoritmi geometrici]]
[[he:קטגוריה:גאומטריה חישובית]]
[[ru:Категория:Геометрические алгоритмы]]
[[sr:Категорија:Геометријски алгоритми]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Quantum algorithms</title>
    <id>702600</id>
    <revision>
      <id>306273412</id>
      <timestamp>2009-08-05T20:31:43Z</timestamp>
      <contributor>
        <username>RobinK</username>
        <id>10041416</id>
      </contributor>
      <text xml:space="preserve">{{catmore|Quantum algorithm}}
[[Algorithm]]s for computations on a [[quantum computer]].

[[Category:Algorithms]]
[[Category:Quantum information science]]

[[es:Categoría:Algoritmos cuánticos]]
[[eo:Kategorio:Kvantumaj algoritmoj]]
[[ko:분류:양자 알고리즘]]
[[pl:Kategoria:Algorytmy kwantowe]]
[[pt:Categoria:Algoritmos quânticos]]
[[ru:Категория:Квантовые алгоритмы]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Compression algorithms</title>
    <id>702589</id>
    <revision>
      <id>324627507</id>
      <timestamp>2009-11-08T11:10:31Z</timestamp>
      <contributor>
        <ip>85.227.157.208</ip>
      </contributor>
      <text xml:space="preserve">This category deals with '''[[algorithm]]s''' for '''[[data compression]]'''.

[[Category:Algorithms]]
[[Category:Data compression]]

[[cs:Kategorie:Kompresní algoritmy]]
[[es:Categoría:Algoritmos de compresión]]
[[fr:Catégorie:Algorithme de compression]]
[[ko:분류:압축 알고리즘]]
[[pl:Kategoria:Algorytmy kompresji]]
[[pt:Categoria:Algoritmos de compressão]]
[[ru:Категория:Алгоритмы сжатия]]
[[sk:Kategória:Kompresné algoritmy]]
[[sv:Kategori:Komprimeringsalgoritmer]]
[[tr:Kategori:Veri sıkıştırma algoritmaları]]
[[zh:Category:压缩算法]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Cryptographic algorithms</title>
    <id>704483</id>
    <revision>
      <id>326563388</id>
      <timestamp>2009-11-18T17:02:45Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor/>
      <comment>Adding [[Template:Commons_category|Commons category]] link to [[Commons:Category:Cryptographic algorithms]]</comment>
      <text xml:space="preserve">This [[Wikipedia:Category|category]] contains articles on '''[[algorithms]]''' in '''[[cryptography]]'''.
{{Commons category|Cryptographic algorithms}}

[[Category:Cryptography|Algorithms]]
[[Category:Algorithms]]

[[ar:تصنيف:خوارزميات تعمية]]
[[cs:Kategorie:Kryptografické algoritmy]]
[[de:Kategorie:Kryptologisches Verfahren]]
[[es:Categoría:Algoritmos criptográficos]]
[[eo:Kategorio:Ĉifrikaj algoritmoj]]
[[fr:Catégorie:Algorithme de cryptographie]]
[[ko:분류:암호 알고리즘]]
[[id:Kategori:Algoritma kriptografi]]
[[it:Categoria:Algoritmi crittografici]]
[[ja:Category:暗号]]
[[no:Kategori:Kryptografiske algoritmer]]
[[pl:Kategoria:Algorytmy kryptograficzne]]
[[pt:Categoria:Algoritmos de criptografia]]
[[sk:Kategória:Šifrovacie algoritmy]]
[[sl:Kategorija:Šifrirni algoritmi]]
[[tr:Kategori:Kriptografik algoritmalar]]
[[uk:Категорія:Криптографічні алгоритми]]
[[vi:Thể loại:Các thuật toán mật mã]]</text>
    </revision>
  </page>
  <page>
    <title>British Museum algorithm</title>
    <id>920295</id>
    <revision>
      <id>261540134</id>
      <timestamp>2009-01-02T22:32:10Z</timestamp>
      <contributor>
        <ip>86.41.79.40</ip>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve">The '''British Museum algorithm''' is a general approach to find a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities are enormous.

For instance, one may, in theory, find the smallest program that solves a particular problem in the following way: Generate all possible source codes of length one character. Check each one to see if it solves the problem. (Note: the [[halting problem]] makes this check troublesome.) If not, generate and check all programs of two characters, three characters, etc. Conceptually, this finds the smallest program, but in practice it tends to take an unacceptable amount of time (more than the lifetime of the universe, in many instances).

Similar arguments can be made to show that optimizations, theorem proving, language recognition, etc. is possible or impossible. 

== See also == 
* [[Branch and bound]]
* [[Breadth-first search]] 
* [[Brute-force search]]
* [[Exhaustive search]]
* [[British Museum]]

== Sources ==

* Original text by {{DADS|British Museum technique|britishMuseum}}.

[[Category:Algorithms]]
[[Category:British Museum|Algorithm]]

[[fr:Algorithme du British Museum]]
[[nl:British Museum-algoritme]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Search algorithms</title>
    <id>1406201</id>
    <revision>
      <id>319201599</id>
      <timestamp>2009-10-11T09:54:47Z</timestamp>
      <contributor>
        <username>Chaojoker</username>
        <id>510326</id>
      </contributor>
      <minor/>
      <comment>+hy</comment>
      <text xml:space="preserve">{{catmore}}

[[Category:Algorithms]]
[[Category:Searching]]
[[Category:Artificial intelligence]]

[[ar:تصنيف:خوارزميات بحث]]
[[cs:Kategorie:Vyhledávací algoritmy]]
[[de:Kategorie:Suchalgorithmus]]
[[es:Categoría:Algoritmos de búsqueda]]
[[eo:Kategorio:Serĉaj algoritmoj]]
[[fr:Catégorie:Algorithme de recherche]]
[[hy:Կատեգորիա:Փնտրման ալգորիթմներ]]
[[ko:분류:검색 알고리즘]]
[[id:Kategori:Algoritma pencarian]]
[[nl:Categorie:Zoekalgoritme]]
[[ja:Category:検索アルゴリズム]]
[[no:Kategori:Søkealgoritmer]]
[[pt:Categoria:Algoritmos de busca]]
[[ru:Категория:Алгоритмы поиска]]
[[sk:Kategória:Vyhľadávacie algoritmy]]
[[tr:Kategori:Arama algoritmaları]]
[[uk:Категорія:Алгоритми пошуку]]
[[vi:Thể loại:Giải thuật tìm kiếm]]
[[zh:Category:搜尋演算法]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Checksum algorithms</title>
    <id>1063613</id>
    <revision>
      <id>255807849</id>
      <timestamp>2008-12-04T08:44:44Z</timestamp>
      <contributor>
        <username>Zorrobot</username>
        <id>7218328</id>
      </contributor>
      <minor/>
      <comment>robot  Adding: [[sv:Kategori:Kontrollsummor]]</comment>
      <text xml:space="preserve">Checksum algorithms which are believed to be [[tamper-evident]] are sometimes called [[:Category:Cryptographic_hash_functions|cryptographic hash functions]], and these are listed in a separate category.

[[Category:Algorithms]]
[[category:error detection and correction]]

[[es:Categoría:Algoritmos de suma de verificación]]
[[eo:Kategorio:Kontrolsumaj algoritmoj]]
[[fr:Catégorie:Somme de contrôle]]
[[ko:분류:체크섬 알고리즘]]
[[sv:Kategori:Kontrollsummor]]
[[zh:Category:校验和算法]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Routing algorithms</title>
    <id>1882018</id>
    <revision>
      <id>264341030</id>
      <timestamp>2009-01-15T22:25:11Z</timestamp>
      <contributor>
        <ip>142.167.214.138</ip>
      </contributor>
      <comment>Fixed a typo where &quot;algrorithms&quot; was spelled &quot;alrorotihms&quot;.</comment>
      <text xml:space="preserve">: ''This category contains algorithms for [[routing]]''.
 
[[Category:Routing]]
[[Category:Algorithms]]
[[Category:Network layer protocols]]

[[eo:Kategorio:Vojfaradaj algoritmoj]]
[[ru:Категория:Алгоритмы маршрутизации]]
[[vi:Thể loại:Giải thuật định tuyến]]
[[zh:Category:路由算法]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Error detection and correction</title>
    <id>1061200</id>
    <revision>
      <id>319767693</id>
      <timestamp>2009-10-14T06:34:59Z</timestamp>
      <contributor>
        <username>ArthurBot</username>
        <id>8001475</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[sk:Kategória:Kontrolné súčty]]</comment>
      <text xml:space="preserve">{{catmore}}

[[Category:Algorithms]]
[[Category:Information theory]]
[[Category:Telecommunications]]
[[Category:Coding theory]]
[[Category:Data transmission]]
[[Category:Logical Link Control]]
[[Category:Transport layer protocols]]

[[da:Kategori:Fejldetektering og korrektion]]
[[es:Categoría:Detección y corrección de errores]]
[[eo:Kategorio:Erara detekto kaj korektado]]
[[fr:Catégorie:Détection et correction d'erreur]]
[[ja:Category:誤り検出訂正]]
[[ru:Категория:Обнаружение и устранение ошибок]]
[[sk:Kategória:Kontrolné súčty]]
[[th:หมวดหมู่:การตรวจหาและการแก้ไขความผิดพลาด]]
[[tr:Kategori:Hata tanılama ve düzeltme]]
[[zh:Category:错误检测与校正]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Scheduling algorithms</title>
    <id>1690419</id>
    <revision>
      <id>268030339</id>
      <timestamp>2009-02-02T13:35:31Z</timestamp>
      <contributor>
        <username>VolkovBot</username>
        <id>3035831</id>
      </contributor>
      <minor/>
      <comment>robot  Adding: [[cs:Kategorie:Algoritmy plánování procesů]] Removing: [[de:Scheduling#Scheduling-Verfahren]]</comment>
      <text xml:space="preserve">{{catmore|Scheduling algorithm}}

[[Algorithm]]s for [[scheduling]] tasks and processes in [[operating system]]s.

[[Category:Queueing theory]]
[[Category:Scheduling (computing)]]
[[Category:Algorithms]]

[[cs:Kategorie:Algoritmy plánování procesů]]</text>
    </revision>
  </page>
  <page>
    <title>Maze generation algorithm</title>
    <id>200877</id>
    <revision>
      <id>321859718</id>
      <timestamp>2009-10-25T01:18:30Z</timestamp>
      <contributor>
        <ip>155.33.98.210</ip>
      </contributor>
      <comment>/* External links */</comment>
      <text xml:space="preserve">{{Cleanup|date=December 2006}}

There are a number of different '''maze generation [[algorithm]]s''', that is, automated methods for the creation of [[maze]]s.

[[Image:Prim Maze.svg|right|frame|This maze generated by modified version of [[Prim's algorithm]], below.]]

== Graph theory based methods ==
A maze can be generated by starting with a predetermined arrangement of cells (most commonly a rectangular grid but other arrangements are possible) with wall sites between them. This predetermined arrangement can be considered as a [[connected graph]] with the edges representing possible wall sites and the nodes representing cells. The purpose of the maze generation algorithm can then be considered to be making a subgraph where it is challenging to find a route between two particular nodes.

If the subgraph is not [[connected graph|connected]], then there are regions of the graph that are wasted because they do not contribute to the search space.  If the graph contains loops, then there may be multiple paths between the chosen nodes.  Because of this, maze generation is often approached as generating a random [[spanning tree (mathematics)|spanning tree]].  Loops which can confound naive maze solvers may be introduced by adding random edges to the result during the course of the algorithm.

=== Depth-first search ===
[[Image:Isometric DFS Maze.PNG|thumb|An [[isometric]] maze created using the DFS method of generation.]]
This algorithm is a randomized version of the [[depth-first search]] algorithm. Frequently implemented with a stack, this approach is one of the simplest ways to generate a maze using a computer. Consider the space for a maze being a large grid of cells (like a large chess board), each cell starting with four walls. Starting from a random cell, the computer then selects a random neighbouring cell that has not yet been visited. The computer removes the 'wall' between the two cells and adds the new cell to a stack (this is analogous to drawing the line on the floor). The computer continues this process, with a cell that has no unvisited neighbours being considered a dead-end. When at a dead-end it backtracks through the path until it reaches a cell with an unvisited neighbour, continuing the path generation by visiting this new, unvisited cell (creating a new junction). This process continues until every cell has been visited, causing the computer to backtrack all the way back to the beginning cell. This approach guarantees that the maze space is completely visited.

As stated, the algorithm is very simple and does not produce overly-complex mazes. More specific refinements to the algorithm can help to generate mazes that are harder to solve.

# Start at a particular cell and call it the &quot;exit.&quot;
# Mark the current cell as visited, and get a list of its neighbors.  For each neighbor, starting with a randomly selected neighbor:
## If that neighbor hasn't been visited, remove the wall between this cell and that neighbor, and then [[recursion|recurse]] with that neighbor as the current cell.

As given above this algorithm involves deep recursion which may cause stack overflow issues on some computer architectures. The algorithm can be rearranged into a loop by storing backtracking information in the maze itself. This also provides a quick way to display a solution, by starting at any given point and backtracking to the exit.

Mazes generated with a depth-first search have a low branching factor and contain many long corridors, which makes depth-first a good algorithm for generating mazes in [[video game]]s.

In mazes generated by that algorithm, it will typically be relatively easy to find the way to the square that was first picked at the beginning of the algorithm, since most paths lead to or from there, but it is hard to find the way out.

==== Recursive backtracker ====
The depth-first search algorithm of maze generation is frequently implemented using [[backtracking]]:

# Mark the current cell as 'Visited'
# If the current cell has any neighbours which have not been visited 
##  Choose randomly one of the unvisited neighbours
##  add the current cell to the stack
##  remove the wall between the current cell and the chosen cell
##  Make the chosen cell the current cell
##  Recursively call this function
# else
##  remove the last current cell from the stack
##  Backtrack to the previous execution of this function

=== Randomized Kruskal's algorithm ===
This algorithm is simply a randomized version of [[Kruskal's algorithm]].

# Create a list of all walls, and create a set for each cell, each containing just that one cell.
# For each wall, in some random order:
## If the cells divided by this wall belong to distinct sets:
### Remove the current wall.
### Join the sets of the formerly divided cells.

There are several data structures that can be used to model the sets of cells.  An efficient implementation using a [[disjoint-set data structure]] can perform each union and find operation on two sets in nearly-constant [[amortized time]] (specifically, &lt;math&gt;O(\alpha(V))&lt;/math&gt; time; &lt;math&gt;\alpha(x) &lt; 5&lt;/math&gt; for any plausible value of &lt;math&gt;x&lt;/math&gt;), so the running time of this algorithm is essentially proportional to the number of walls available to the maze.

It matters little whether the list of walls is initially randomized or if a wall is randomly chosen from a nonrandom list, either way is just as easy to code.

Because the effect of this algorithm is to produce a minimal spanning tree from a graph with equally-weighted edges, it tends to produce regular patterns which are fairly easy to solve.

=== Randomized Prim's algorithm ===
This algorithm is a randomized version of [[Prim's algorithm]].  

# Start with a grid full of walls.
# Pick a cell, mark it as part of the maze. Add the walls of the cell to the wall list.
# While there are walls in the list:
## Pick a random wall from the list. If the cell on the opposite side isn't in the maze yet:
### Make the wall a passage and mark the cell on the opposite side as part of the maze.
### Add the neighboring walls of the cell to the wall list.

Like the depth-first algorithm, it will usually be relatively easy to find the way to the starting cell, but hard to find the way anywhere else.

Note that simply running classical Prim's on a graph with random weights would create mazes stylistically identical to Kruskal's, because they are both minimal spanning tree algorithms.  Instead, this algorithm introduces stylistic variation because the edges closer to the starting point have a lower effective weight.

==== Modified version ====
Although the classical Prim's algorithm keeps a list of edges, for maze generation we could instead maintain a list of adjacent cells.  If the randomly chosen cell has multiple edges that connect it to the existing maze, select one of these edges at random.  This will tend to branch slightly more than the edge-based version above.

==Recursive division method==
{| class=&quot;wikitable&quot; align=&quot;right&quot;
|+ '''Illustration of Recursive Division'''
|-
! width=&quot;110px&quot; | ''original chamber''
! width=&quot;110px&quot; | ''division by two walls''
! width=&quot;110px&quot; | ''holes in walls''
! width=&quot;110px&quot; | ''continue subdividing...''
! width=&quot;110px&quot; | ''completed''
|-
| align=&quot;center&quot; | [[Image:Chamber.png|101px]]
| align=&quot;center&quot; | [[Image:Chamber division.png|101px]]
| align=&quot;center&quot; | [[Image:Chamber divided.png|101px]]
| align=&quot;center&quot; | [[Image:Chamber subdivision.png|101px]]
| align=&quot;center&quot; | [[Image:Chamber finished.png|101px]]
|}

Mazes can be created with ''recursive division'', an algorithm which works as follows: Begin with the maze's space with no walls. Call this a chamber. Divide the chamber with a randomly positioned wall (or multiple walls) where each wall contain a randomly positioned passage opening within it. Then recursively repeat the process on the subchambers until all chambers are minimum sized. This method results in mazes with long straight walls crossing their space, making it easier to see which areas to avoid.

For example, in a rectangular maze, build at random points two walls that are perpendicular to each other. These two walls divide the large chamber into four smaller chambers separated by four walls. Choose three of the four walls at random, and open a one cell-wide hole at a random point in each of the three. Continue in this manner recursively, until every chamber has a width of one cell in either of the two directions.
&lt;br clear=all&gt;
== Simple algorithms ==
Other algorithms exist that require only enough memory to store one line of a 2D maze or one plane of a 3D maze. They prevent loops by storing which cells in the current line are connected through cells in the previous lines, and never remove walls between any two cells already connected.

Most maze generation algorithms require maintaining relationships between cells within it, to ensure the end result will be solvable. Valid simply connected mazes can however be generated by focusing on each cell independently. A binary tree maze is a standard orthogonal maze where each cell always has a passage leading up or leading left, but never both. To create a binary tree maze, for each cell flip a coin to decide whether to add a passage leading up or left. Always pick the same direction for cells on the boundary, and the end result will be a valid simply connected maze that looks like a [[binary tree]], with the upper left corner its root.

A related form of flipping a coin for each cell is to create an image using a random mix of forward slash and backslash characters. This doesn't generate a valid simply connected maze, but rather a selection of closed loops and unicursal passages.

== Non-cell-based algorithm ==
A maze can also be generated without the use of cells. In year 2000, a shareware called AmorphousMaze, appeared on the Internet that creates mazes with walls placed at totally random angles. [http://www.puz.com/sw/amorphous/theory/amz1.jpg Sample picture]. The algorithm is based on extending the wall by a small segment at a time without crossing over a pre-existing one. [http://www.puz.com/sw/amorphous/theory/index.htm Algorithm detail]. The disadvantage of this algorithm is that the number of tests for intersection is &lt;math&gt;O(E^2)&lt;/math&gt;, where &lt;math&gt;E&lt;/math&gt; is the number of line segments being drawn.

==See also==
* [[Mazes]]
* [[Maze solving algorithm]]

==External links==
* [http://www.astrolog.org/labyrnth/algrithm.htm#perfect Think Labyrinth: Maze algorithms] (details on these and other maze generation algorithms)
* [http://homepages.cwi.nl/~tromp/maze.html Explanation of an Obfuscated C maze algorithm] (a program to generate mazes line-by-line, obfuscated in a single physical line of code)
* [http://www.mazeworks.com/mazegen/ Maze generation and solving Java applet]
* [http://www.ccs.neu.edu/home/snuffy/maze/ Maze generating Java applets with source code.]
* [http://chiesaclan.spaces.live.com/blog/cns!842434EBE9688900!632.entry Maze generator and solver, in C#] - print out mazes in various shapes on paper.
* [http://solidify.com/public/simple_maze.pdf A simple method of making a circular maze]

[[Category:Mazes]]
[[Category:Algorithms]]
[[Category:Random graphs]]

[[he:אלגוריתמים לייצור מבוכים]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Classification algorithms</title>
    <id>1991254</id>
    <revision>
      <id>319364179</id>
      <timestamp>2009-10-12T04:04:39Z</timestamp>
      <contributor>
        <username>Maksim-bot</username>
        <id>637745</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[fr:Catégorie:Algorithme de classification]]</comment>
      <text xml:space="preserve">This category is about statistical classification algorithms. For more information, see '''[[Statistical classification]]'''.

[[Category:Machine learning]]
[[Category:Knowledge discovery in databases]]
[[Category:Algorithms]]

[[ar:تصنيف:خوارزميات تصنيف]]
[[es:Categoría:Algoritmos de clasificación]]
[[fr:Catégorie:Algorithme de classification]]
[[ko:분류:분류 알고리즘]]
[[ja:Category:分類アルゴリズム]]
[[pt:Categoria:Algoritmos de classificação]]
[[fi:Luokka:Luokittelualgoritmit]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Optimization algorithms</title>
    <id>752914</id>
    <revision>
      <id>319520421</id>
      <timestamp>2009-10-12T23:15:41Z</timestamp>
      <contributor>
        <username>Chobot</username>
        <id>259798</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[it:Categoria:Algoritmi di ottimizzazione]]</comment>
      <text xml:space="preserve">An '''[[Optimization (mathematics)|optimization]] [[algorithm]]''' is a numerical method or [[algorithm]] for finding a value ''x'' such that ''f(x)'' is as small (or as large) as possible, for a given [[function (mathematics)|function]] ''f'', possibly with some constraints on ''x''. Here, ''x'' can be a scalar or vector of continuous or discrete values. If ''x'' is continuous, then the study of the algorithm is part of [[numerical analysis]].

[[Category:Algorithms]]
[[Category:Mathematical optimization]]
[[Category:Artificial intelligence]]
[[Category:Operations research]]

[[de:Kategorie:Optimierungsalgorithmus]]
[[fr:Catégorie:Algorithme d'optimisation]]
[[ko:분류:최적화 알고리즘]]
[[it:Categoria:Algoritmi di ottimizzazione]]
[[ja:Category:最適化アルゴリズム]]
[[pt:Categoria:Algoritmos de otimização]]
[[ru:Категория:Алгоритмы оптимизации]]
[[tr:Kategori:Optimizasyon algoritmaları]]
[[uk:Категорія:Методи оптимізації]]
[[zh:Category:最优化算法]]</text>
    </revision>
  </page>
  <page>
    <title>Timeline of algorithms</title>
    <id>416776</id>
    <revision>
      <id>313314963</id>
      <timestamp>2009-09-12T04:42:14Z</timestamp>
      <contributor>
        <username>Gehilfen</username>
        <id>10498765</id>
      </contributor>
      <comment>/* 1980s */ link to Vladimir Rokhlin (American scientist)</comment>
      <text xml:space="preserve">The following '''[[Chronology|timeline]]''' outlines the development of '''[[algorithm]]s''' (mainly &quot;mathematical recipes&quot;) since their inception.

==Before Modern Era==
* Before - [[Writing]] about &quot;[[recipes]]&quot; (on cooking, rituals, agriculture and other themes)
* c. [[1600 BC]] - [[Babylonia]]ns develop earliest known algorithms for [[factorization]] and finding [[square root]]s
* c. [[300 BC]] - [[Euclidean algorithm|Euclid's algorithm]]
* c. [[200 BC]] - the [[Sieve of Eratosthenes]]
* [[263]] AD - [[Gaussian elimination]] described by [[Liu Hui]]
* [[628]] - [[Chakravala method]] described by [[Brahmagupta]]
* c. [[820]] - [[Al-Khawarizmi]] described algorithms for solving [[linear equation]]s and [[quadratic equation]]s in his ''[[The Compendious Book on Calculation by Completion and Balancing|Algebra]]''; the word ''algorithm'' comes from his name
* [[825]] - [[Al-Khawarizmi]] described the [[algorism]], algorithms for using the Hindu-[[Arabic numerals]], in his treatise ''On the Calculation with Hindu Numerals'', which was [[Latin translations of the 12th century|translated into Latin]] as ''Algoritmi de numero Indorum'', where &quot;Algoritmi&quot;, the translator's rendition of the author's name gave rise to the word [[algorithm]] ([[Latin]] ''algorithmus'') with a meaning &quot;calculation method&quot;
* c. [[850]] - [[Cryptanalysis]] and [[frequency analysis]] algorithms developed by [[Al-Kindi]] (Alkindus) in ''A Manuscript on Deciphering Cryptographic Messages'', which contains algorithms on breaking [[encryption]]s and [[cipher]]s.&lt;ref&gt;Simon Singh, ''The Code Book'', pp. 14-20&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.muslimheritage.com/topics/default.cfm?ArticleID=372 |title= Al-Kindi, Cryptgraphy, Codebreaking and Ciphers |accessdate=2007-01-12 |format= HTML |work= }}&lt;/ref&gt;
* c. [[1025]] - [[Ibn al-Haytham]] (Alhazen), was the first mathematician to derive the formula for the sum of the [[Quartic|fourth]] [[Exponentiation|powers]], and in turn, he develops an algorithm for determining the general formula for the sum of any [[integral]] powers, which was fundamental to the development of [[Integral|integral calculus]]&lt;ref name=Katz&gt;Victor J. Katz (1995). &quot;Ideas of Calculus in Islam and India&quot;, ''Mathematics Magazine'' '''68''' (3), p. 163-174.&lt;/ref&gt;
* c. [[1400]] - [[Ahmad al-Qalqashandi]] gives a list of [[cipher]]s in his ''Subh al-a'sha'' which include both [[Substitution cipher|substitution]] and [[Transposition cipher|transposition]], and for the first time, a cipher with multiple substitutions for each [[plaintext]] letter; he also gives an exposition on and worked example of [[cryptanalysis]], including the use of tables of [[letter frequencies]] and sets of letters which can not occur together in one word

==Before 1940==
* [[1614]] - [[John Napier]] develops method for performing calculations using [[logarithm]]s
* [[1671]] - [[Newton's method|Newton-Raphson method]] developed by [[Isaac Newton]]
* [[1690]] - [[Newton's method|Newton-Raphson method]] independently developed by [[Joseph Raphson]]
* [[1706]] -  [[John Machin]] develops a quickly converging inverse-tangent series for π and computes π to 100 decimal places,
* [[1789]] -  [[Jurij Vega]] improves Machin's formula and computes π to 140 decimal places,
&lt;!-- FFT?? 1965 -- * [[1805]] - [[Cooley-Tukey FFT algorithm|Cooley-Tukey algorithm]] known by [[Carl Friedrich Gauss]] --&gt;
* [[1805]] - [[Cooley-Tukey_FFT_algorithm#History|FFT-like algorithm]] known by [[Carl Friedrich Gauss]]
* [[1903]] - A [[Fast Fourier Transform]] algorithm presented by [[Carle David Tolme Runge]]
* [[1926]] - [[Borůvka's algorithm]]
* [[1934]] - [[Delaunay triangulation]] developed by [[Boris Delaunay]]
* [[1936]] - [[Turing machine]], an [[abstract machine]] developed by [[Alan Turing]], with [[Turing_machine#Models_equivalent_to_the_Turing_machine_model|others]] developed the modern notion of ''algorithm''.

==1940s==
* [[1942]] -  A [[Fast Fourier Transform]] algorithm developed by [[G.C. Danielson]] and [[Cornelius Lanczos]]
* [[1945]] - [[Merge sort]] developed by [[John von Neumann]]
* [[1947]] - [[Simplex algorithm]] developed by [[George Dantzig]]

==1950s==
* [[1952]] - [[Huffman coding]] developed by [[David A. Huffman]]
* [[1953]] - [[Simulated annealing]] introduced by [[Nicholas Metropolis]] 
* [[1954]] - [[Radix sort]] computer algorithm developed by [[Harold H. Seward]]
* [[1956]] - [[Kruskal's algorithm]] developed by [[Joseph Kruskal]]
* [[1957]] - [[Prim's algorithm]] developed by [[Robert Prim]]
* 1957 - [[Bellman-Ford algorithm]] developed by [[R. Bellman]] and [[L. R. Ford, Jr.]]
* [[1959]] - [[Dijkstra's algorithm]] developed by [[Edsger Dijkstra]]
* 1959 - [[Shell sort]] developed by [[D. L. Shell]]
* 1959 - [[De Casteljau's algorithm]] developed by [[Paul de Casteljau]]

==1960s==
* [[1962]] - [[Quicksort]] developed by [[C. A. R. Hoare]]
* 1962 - [[Ford-Fulkerson algorithm]] developed by [[L. R. Ford, Jr.]] and [[D. R. Fulkerson]]
* 1962 - [[Bresenham's line algorithm]] developed by [[Jack E. Bresenham]]
* [[1964]] - [[Heapsort]] developed by [[J. W. J. Williams]]
* [[1964]] - [[multigrid methods]] first proposed by [[R. P. Fedorenko]]
* [[1965]] - [[Cooley-Tukey FFT algorithm|Cooley-Tukey algorithm]] rediscovered by [[James Cooley]] and [[John Tukey]]
* 1965 - [[Levenshtein distance]] developed by [[Vladimir Levenshtein]]
* 1965 - [[CYK algorithm|Cocke-Younger-Kasami (CYK) algorithm]] independently developed by [[T. Kasami]]
* 1966 - [[Dantzig algorithm]] for shortest path in a graph with negative edges
* [[1967]] - [[Viterbi algorithm]] proposed by [[Andrew Viterbi]]
* 1967 - [[CYK algorithm|Cocke-Younger-Kasami (CYK) algorithm]] independently developed by [[D. H. Younger]]
* [[1968]] - [[A* search algorithm|A* graph search algorithm]] described by [[Peter E. Hart|Peter Hart]], [[Nils Nilsson]], and [[Bertram Raphael]].

==1970s==
* [[1970]] - [[Knuth-Bendix completion algorithm]] developed by [[Donald Knuth]] and [[P. B. Bendix]]
* 1970 - [[BFGS method]] of the [[Quasi-Newton method|quasi-Newton]] class
* [[1972]] - [[Graham scan]] developed by [[Ronald Graham]]
* [[1973]] - [[RSA]] encryption algorithm discovered by [[Clifford Cocks]]
* 1973 - [[Jarvis march]] algorithm developed by  [[R. A. Jarvis]]
* [[1974]] - [[Pollard's p - 1 algorithm|Pollard's ''p''&amp;nbsp;&amp;minus;&amp;nbsp;1 algorithm]] developed by [[John Pollard (mathematician)|John Pollard]]
* [[1975]] - [[Genetic algorithm]]s popularized by [[John Henry Holland|John Holland]] 
* 1975 - [[Pollard's rho algorithm]] developed by [[John Pollard (mathematician)|John Pollard]]
* 1975 - [[Aho-Corasick algorithm]] developed by [[Alfred V. Aho]] and [[Margaret J. Corasick]]
* [[1976]] - [[Salamin-Brent algorithm]] independently discovered by [[Eugene Salamin (mathematician)|Eugene Salamin]] and [[Richard Brent (scientist)| Richard Brent]]
* 1976 - [[Knuth-Morris-Pratt algorithm]] developed by [[Donald Knuth]] and [[Vaughan Pratt]] and independently by [[J. H. Morris]]
* [[1977]] - [[Boyer-Moore string search algorithm]] for searching the occurrence of a string into another string.
* [[1977]] - [[RSA]] encryption algorithm rediscovered by [[Ron Rivest]], [[Adi Shamir]], and [[Len Adleman]]
* 1977 - [[LZ77]] algorithm developed by [[Abraham Lempel]] and [[Jacob Ziv]]
* 1977 - [[multigrid methods]] developed independently by [[Achi Brandt]] and [[Wolfgang Hackbusch]]
* [[1978]] - [[LZ78]] algorithm developed from [[LZ77]] by [[Abraham Lempel]] and [[Jacob Ziv]]
* 1978 - [[Bruun's FFT algorithm|Bruun's algorithm]] proposed for powers of two by [[G. Bruun]]
* [[1979]] - Khachiyan's [[ellipsoid method]] developed by [[Leonid Khachiyan]]

==1980s==
* [[1981]] - [[Quadratic sieve]] developed by [[Carl Pomerance]]
* [[1983]] - [[Simulated annealing]] developed by [[S. Kirkpatrick]], [[C. D. Gelatt]] and [[M. P. Vecchi]]
* [[1984]] - [[LZW (algorithm)|LZW]] algorithm developed from [[LZ78]] by [[Terry Welch]]
* 1984 - [[Karmarkar's interior-point algorithm]] developed by [[Narendra Karmarkar]]
* [[1985]] - [[Simulated annealing]] independently developed by [[V. Cerny]]
* [[1986]] - [[Blum Blum Shub]] proposed by [[L. Blum]], [[M. Blum]], and [[M. Shub]]
* [[1987]] - [[Fast multipole method]] developed by [[Leslie Greengard]] and [[Vladimir Rokhlin (American scientist)|Vladimir Rokhlin]]
* [[1988]] - [[Special number field sieve]] developed by [[John Pollard (mathematician)|John Pollard]]

==1990s==
* [[1990]] - [[General number field sieve]] developed from [[Special number field sieve|SNFS]] by [[Carl Pomerance]], [[Joe Buhler]], [[Hendrik Lenstra]], and [[Leonard Adleman]]
* 1991 - [[Lock-free and wait-free algorithms| Wait-free synchronization]] developed by [[Maurice Herlihy]]
* [[1992]] - [[Deutsch-Jozsa algorithm]] proposed by [[D. Deutsch]] and [[R. Jozsa]]
* [[1994]] - [[Shor's algorithm]] developed by [[Peter Shor]]
* 1994 - [[Burrows-Wheeler transform]] developed by [[Michael Burrows]] and [[David Wheeler (computer scientist)|David Wheeler]]
* [[1996]] - [[Bruun's FFT algorithm|Bruun's algorithm]] generalized to arbitrary even composite sizes by [[H. Murakami]]
* 1996 - [[Grover's algorithm]] developed by [[Lov K. Grover]]
* 1996 - [[RIPEMD-160]] developed by  [[Hans Dobbertin]], [[Antoon Bosselaers]], and [[Bart Preneel]]
* [[1998]] - [[rsync algorithm]] developed by [[Andrew Tridgell]]
* [[1999]] - [[Yarrow algorithm]] designed by [[Bruce Schneier]], [[John Kelsey (cryptanalyst)|John Kelsey]], and [[Niels Ferguson]]

==2000s==
* [[2001]] - [[LZMA]] compression algorithm
* [[2002]] - [[AKS primality test]] developed by [[Manindra Agrawal]], [[Neeraj Kayal]] and [[Nitin Saxena]]

==References==
{{Reflist}}

[[Category:Algorithms]]
[[Category:Computing timelines|Algorithms]]
[[Category:Mathematics timelines|Algorithms]]

[[tr:Algoritmaların tarihsel sıralaması]]</text>
    </revision>
  </page>
  <page>
    <title>Astronomical algorithm</title>
    <id>853888</id>
    <revision>
      <id>310171931</id>
      <timestamp>2009-08-26T14:43:26Z</timestamp>
      <contributor>
        <username>RobinK</username>
        <id>10041416</id>
      </contributor>
      <text xml:space="preserve">'''Astronomical algorithms''' are the [[algorithm]]s used to calculate [[ephemeris|ephemerides]], [[calendar]]s, and positions (as in [[celestial navigation]] or [[satellite navigation]]). Examples of large and complex [[astronomy|astronomical]] algorithms are those used to calculate the position of the [[Moon]].  A simple example is the [[Julian day#Calculation|calculation of the Julian day]].

[[Numerical model of solar system]] discusses a generalized approach to local astronomical modeling. The ''[[variations séculaires des orbites planétaires]]'' describes an often used model.

== See also ==

* [[Astrodynamics]]
* [[Celestial mechanics]]
* [[Charge-coupled device]] (a data-collecting device that is sometimes aimed at the sky and requires algorithms to process its output)
* [[Doomsday rule]]
* [[List of algorithms]]
* [[List of astronomical objects]]
* [[Jean Meeus|Meeus, Jean]]
* [[Transformation from spherical coordinates to rectangular coordinates]]
* [http://www.sunlit-design.com/products/thesunapi/documentation/funcref.php?indexref=4 An implementation of Astronomical Algorithms. Calculate MJD, Equation of Time and Solar Declination in Excel, CAD or your other programs.]  The Sun API is free and extremely accurate. For Windows Computers.

[[Category:Astrodynamics]]
[[Category:Algorithms]]
[[Category:Calendar algorithms]]


{{algorithm-stub}}

{{astronomy-stub}}</text>
    </revision>
  </page>
  <page>
    <title>Tomasulo algorithm</title>
    <id>390562</id>
    <revision>
      <id>322770634</id>
      <timestamp>2009-10-29T18:07:04Z</timestamp>
      <contributor>
        <username>Jweymarn</username>
        <id>10457867</id>
      </contributor>
      <minor/>
      <comment>/* Stage 1: issue */</comment>
      <text xml:space="preserve">The '''Tomasulo algorithm''' is a hardware [[algorithm]] developed in 1967 by [[Robert Tomasulo]] from [[IBM]]. It allows sequential instructions that would normally be stalled due to certain dependencies to execute non-sequentially ([[out-of-order execution]]). It was first implemented for the [[IBM System/360]] Model 91’s floating point unit.

This algorithm differs from [[scoreboarding]] in that it utilizes [[register renaming]]. Where scoreboarding resolves Write-after-Write (WAW) and Write-after-Read (WAR) [[Hazard (computer architecture)|hazards]] by stalling, register renaming allows the continual issuing of instructions. The Tomasulo algorithm also uses a [[common data bus]] (CDB) on which computed values are broadcast to all the [[reservation stations]] that may need it. This allows for improved parallel execution of instructions which may otherwise stall under the use of scoreboarding.

Robert Tomasulo received the [[Eckert-Mauchly Award]] in 1997 for this algorithm.

==Implementation concepts==
The following are the concepts necessary to the implementation of Tomasulo's Algorithm.

*Instructions are issued sequentially so that the effects of a sequence of instructions such as exceptions raised by these instructions occur in the same order as they would in a non-pipelined processor, regardless of the fact that they are being executed non-sequentially.

*All general-purpose and [[Reservation_stations|reservation station]] registers hold either real or virtual values.  If a real value is unavailable to a destination register during the issue stage, a virtual value is initially used.  The functional unit that is computing the real value is assigned as the virtual value.  The virtual register values are converted to real values as soon as the designated functional unit completes its computation.

*Functional units use [[Reservation_stations|reservation stations]] with multiple slots.  Each slot holds information needed to execute a single instruction, including the operation and the operands.  The functional unit begins processing when it is free and when all source operands needed for an instruction are real.

==Instruction lifecycle==
The three stages listed below are the stages through which each instruction passes from the time it is issued to the time its execution is complete.

===Stage 1: issue===
In the issue stage, instructions are issued for execution if all operands and reservation stations are ready or else they are stalled.  Registers are renamed in this step, eliminating WAR and WAW hazards.

*Retrieve the next instruction from the head of the instruction queue. If the instruction operands are currently in the registers &lt;!-- Unclear --&gt;
**If there is a matching empty reservation station (i.e., functional unit is available) then: issue the instruction
**Else, there is not a matching empty reservation station (i.e., functional unit is not available) then: stall the instruction until a station or buffer is free
*Else, the operands are not in the registers, then: use virtual values, the functional unit calculating the real value, to keep track of the functional units that will produce the operands

===Stage 2: execute===

In the execute stage, the instruction operations are carried out.  Instructions are delayed in this step until all of their operands are available, eliminating RAW hazards.  Program correctness is maintained through effective address calculation to prevent hazards through memory.

#If one or more of the operands is not yet available then: wait for operand to become available on the CDB.
#When all operands are available, then: if the instruction is a load or store
##Compute the effective address when the base register is available, and place it in the load/store buffer
##&amp;nbsp;
##*If the instruction is a load then: execute as soon as the memory unit is available
##*Else, if the instruction is a store then: wait for the value to be stored before sending it to the memory unit
##*Else, the instruction is an ALU operation then: execute the instruction at the corresponding functional unit

===Stage 3: write result===
In the write Result stage, ALU operations results are written back to registers and store operations are written back to memory.
*If the instruction was an ALU operation
**If the result is available, then: write it on the CDB and from there into the registers and any reservation stations waiting for this result
*Else, if the instruction was a store then: write the data to memory during this step

== See also ==
* [[Re-order buffer]]
* [[Instruction level parallelism]]
* [[Out-of-order execution]]

== External links ==
* [http://www.cs.umd.edu/class/fall2001/cmsc411/projects/dynamic/tomasulo.html Dynamic Scheduling - Tomasulo's Algorithm]

== Bibliography ==
* ''[http://domino.research.ibm.com/tchjr/journalindex.nsf/0/ed39cdf7e40549ec85256bfa00683f73?OpenDocument An Efficient Algorithm for Exploiting Multiple Arithmetic Units]'',  IBM Journal of Research and Development, 11(1):25-33, January 1967.
* ''[http://www.dcs.ed.ac.uk/home/hase/webhase/demo/tomasulo.html WebHASE: Tomasulo's Algorithm: HASE Java applet simulation of the Tomasulo's Algorithm]'', Institute for Computing Systems Architecture, Edinburgh University.
* ''[http://www.ecs.umass.edu/ece/koren/architecture/Tomasulo1/tomasulo.htm TOMASULO'S ALGORITHM FOR DYNAMIC SCHEDULING]''
* ''Computer Architecture: A Quantitative Approach'', John L. Hennessy &amp; David A. Patterson


[[Category:Algorithms]]
[[Category:Instruction processing]]

[[de:Tomasulo-Algorithmus]]
[[es:Algoritmo de Tomasulo]]
[[it:Algoritmo di Tomasulo]]
[[ja:Tomasuloのアルゴリズム]]</text>
    </revision>
  </page>
  <page>
    <title>Medical algorithm</title>
    <id>1551981</id>
    <revision>
      <id>272827041</id>
      <timestamp>2009-02-23T22:47:06Z</timestamp>
      <contributor>
        <ip>81.244.234.74</ip>
      </contributor>
      <comment>medical decision making odds algorithm</comment>
      <text xml:space="preserve">{{Original research|date=October 2007}}
A '''medical algorithm''' is any [[computation]], [[formula]], [[statistical survey]], [[nomogram]], or [[look-up table]], useful in [[healthcare]].  [[Medical]] [[algorithm]]s include [[decision tree]] approaches to healthcare treatment (i.e., if [[symptom]]s A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.
Medical algorithms are part of a broader field which is usually fit under the aims of [[medical informatics]] and [[medical decision making]]. Medical decisions occur in several areas of medical activity including medical test selection, [[diagnosis]], therapy and [[prognosis]], and [[automatic control]] of [[medical equipment]].

In relation to [[logic]]-based and [[artificial neural network]]-based medical [[expert systems]], which are also computer applications to the medical decision making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.

Examples of medical algorithms are:
* '''Calculators,'''. e.g., an on-line or stand-alone calculator for [[body mass index]] (BMI) when stature and body weight are given;
* '''Flowcharts,''' e.g., a [[Wiktionary:binary|binary]] [[decision tree]] for deciding what is the [[etiology]] of [[chest pain]]
* '''Look-up tables,''' e.g., for looking up [[food energy]] and nutritional contents of foodstuffs
* '''Nomographs,''' e.g., a moving circular slide to calculate body surface area or drug dosages.

The intended purpose of medical algorithms is to improve and standardize [[decision]]s made in the delivery of [[medical care]].  Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm [[automation]] intended to reduce potential introduction of errors.  Some attempt to predict the outcome, for example [[ICU scoring systems|critical care scoring systems]].

A wealth of medical information exists in the form of published medical algorithms.  These algorithms range from simple [[calculation]]s to complex outcome [[prediction]]s.  Most [[clinician]]s use only a small subset routinely.  [[Computerized]] algorithms can provide timely clinical decision support, improve adherence to [[evidence-based medicine|evidence-based]] [[guideline (medical)|guidelines]], and be a resource for education and research. 

In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.  

A grammar - the [[Arden syntax]] - exists for describing algorithms in terms of [[medical logic module]]s.  An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.

Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as [[Clinical trial protocol|protocol]]s and it is a key task in training to ensure people step outside the protocol when necessary.  In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.

[[Computation]]s obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and [[physician]] judgment.

A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians.  A field which has gained particular attention is the choice of medications for psychiatric conditions.  In the United Kingdom guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices.  In the US there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the [[Texas Medication Algorithm Project]] or otherwise working on their production.

==See also==
* ''[[Treatment Guidelines from The Medical Letter]]''
* [[Medical informatics]]
* [[Medical decision making]]
* [[Odds algorithm]]
* [[Evidence-based medicine]]
* [[Consensus (medical)]]
* [[Guideline (medical)]]
* [[Journal club|Journal Club]]

==External links==

* [http://www.alternativementalhealth.com/articles/fieldmanual.htm AlternativeMentalHealth.com] - 'Alternative Health Medical Evaluation Field Manual', Lorrin M. Koran, MD, [[Stanford University]] Medical Center (1991)
* [http://medal.org MedAl.org] - 'The Medical Algorithms Project'
* [http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=419420 NIH.gov] - 'Automated Medical Algorithms:  Issues for Medical Errors', Kathy A. Johnson, PhD, John R. Svirbely, MD, M. G. Sriram, PHD, Jack W. Smith, MD, PHD, Gareth Kantor, MD, and Jorge Raul Rodriguez, MD, ''[[Journal of the American Medical Informatics Association]]''
* [http://www.regenstrief.org/ Regenstrief Institute]
*[http://www.internal-med.org/palm.htm MedFormula - a palm pilot based medical algorithm/calculator]

[[Category:Medical informatics]]
[[Category:Algorithms]]
[[Category:Knowledge representation]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Selection algorithms</title>
    <id>2597043</id>
    <revision>
      <id>78609107</id>
      <timestamp>2006-09-30T01:08:43Z</timestamp>
      <contributor>
        <username>JonHarder</username>
        <id>629503</id>
      </contributor>
      <comment>catmore</comment>
      <text xml:space="preserve">{{catmore|Selection algorithm}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Approximation algorithms</title>
    <id>2597177</id>
    <revision>
      <id>279209019</id>
      <timestamp>2009-03-23T19:19:04Z</timestamp>
      <contributor>
        <username>Ayda D</username>
        <id>8054396</id>
      </contributor>
      <text xml:space="preserve">[[Category:Algorithms]]

[[eo:Kategorio:Proksimumaj kalkuladaj algoritmoj]]
[[fa:رده:الگوریتم‌های تقریبی]]
[[ko:분류:근사 알고리즘]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Combinatorial algorithms</title>
    <id>2634128</id>
    <revision>
      <id>313377353</id>
      <timestamp>2009-09-12T15:06:34Z</timestamp>
      <contributor>
        <username>Borgx</username>
        <id>271227</id>
      </contributor>
      <minor/>
      <comment>+id</comment>
      <text xml:space="preserve">[[Category:Algorithms]]
[[Category:Combinatorics]]

[[id:Kategori:Algoritma kombinatorial]]
[[tr:Kategori:Birleşimsel algoritmalar]]
[[vi:Thể loại:Thuật toán tổ hợp]]</text>
    </revision>
  </page>
  <page>
    <title>Run-time algorithm specialisation</title>
    <id>2935699</id>
    <revision>
      <id>312699081</id>
      <timestamp>2009-09-09T01:20:27Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Software performance optimization to Software optimization per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2009 September 1]].</comment>
      <text xml:space="preserve">In [[computer science]], '''run-time algorithm specialization''' is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of [[automated theorem proving]] and, more specifically, in the [[Vampire theorem prover]] project.

The idea is inspired by the use of [[partial evaluation]] in optimising program translation. 
Many core operations in theorem provers exhibit the following pattern.
Suppose that we need to execute some algorithm &lt;math&gt;\mathit{alg}(A,B)&lt;/math&gt; in a situation where a value of &lt;math&gt;A&lt;/math&gt; ''is fixed for potentially many different values of'' &lt;math&gt;B&lt;/math&gt;. In order to do this efficiently, we can try to find a specialization of &lt;math&gt;\mathit{alg}&lt;/math&gt; for every fixed &lt;math&gt;A&lt;/math&gt;, i.e., such an algorithm &lt;math&gt;\mathit{alg}_A&lt;/math&gt;, that executing &lt;math&gt;\mathit{alg}_A(B)&lt;/math&gt; is equivalent to executing &lt;math&gt;\mathit{alg}(A,B)&lt;/math&gt;.

The specialized algorithm may be more efficient than the generic one, since it can ''exploit some particular properties'' of the fixed value &lt;math&gt;A&lt;/math&gt;. Typically, &lt;math&gt;\mathit{alg}_A(B)&lt;/math&gt; can avoid some operations that &lt;math&gt;\mathit{alg}(A,B)&lt;/math&gt; would have to perform, if they are known to be redundant for this particular parameter &lt;math&gt;A&lt;/math&gt;. 
In particular, we can often identify some tests that are true or false for &lt;math&gt;A&lt;/math&gt;, unroll loops and recursion, etc. 

== Difference from partial evaluation ==
The key difference between run-time specialization and partial evaluation is that the values of &lt;math&gt;A&lt;/math&gt; on which &lt;math&gt;\mathit{alg}&lt;/math&gt; is specialised are not known statically, so the ''specialization takes place at run-time''.

There is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of &lt;math&gt;\mathit{alg}&lt;/math&gt;. We only have to ''imagine'' &lt;math&gt;\mathit{alg}&lt;/math&gt; ''when we program'' the specialization procedure.
All we need is a concrete representation of the specialized version &lt;math&gt;\mathit{alg}_A&lt;/math&gt;. This also means that we cannot use any universal methods for specializing algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialization procedure for every particular algorithm &lt;math&gt;\mathit{alg}&lt;/math&gt;. An important advantage of doing so is that we can use some powerful ''ad hoc'' tricks exploiting peculiarities of &lt;math&gt;\mathit{alg}&lt;/math&gt; and the representation of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;, which are beyond the reach of any universal specialization methods.

==Specialization with compilation==

The specialized algorithm has to be represented in a form that can be interpreted.
In many situations, usually when &lt;math&gt;\mathit{alg}_A(B)&lt;/math&gt; is to be computed on many values &lt;math&gt;B&lt;/math&gt; in a row, we can write &lt;math&gt;\mathit{alg}_A&lt;/math&gt; as a code of a special [[abstract machine]], and we often say that &lt;math&gt;A&lt;/math&gt; is ''compiled''.  
Then the code itself can be additionally optimized by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine. 

Instructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional Parameters of the instruction, for example a pointer to another
instruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree. 

Interpretation is done by fetching instructions in some order, identifying their type
and executing the actions associated with this type. 
In [[C (programming language)|C]] or C++ we can use a '''switch''' statement to associate 
some actions with different instruction tags. 
Modern compilers usually compile a '''switch''' statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value &lt;math&gt;i&lt;/math&gt; in the &lt;math&gt;i&lt;/math&gt;-th cell of a special array. One can exploit this
by taking values for instruction tags from a small interval of integers.

==Data-and-algorithm specialization==

There are situations when many instances of &lt;math&gt;A&lt;/math&gt; are intended for long-term storage and the calls of &lt;math&gt;\mathit{alg}(A,B)&lt;/math&gt; occur with different &lt;math&gt;B&lt;/math&gt; in an unpredictable order.
For example, we may have to check &lt;math&gt;\mathit{alg}(A_1,B_1)&lt;/math&gt; first, then &lt;math&gt;\mathit{alg}(A_2,B_2)&lt;/math&gt;, then &lt;math&gt;\mathit{alg}(A_1,B_3)&lt;/math&gt;, and so on.
In such circumstances, full-scale specialization with compilation may not be suitable due to excessive memory usage.  
However, we can sometimes find a compact specialized representation &lt;math&gt;A^{\prime}&lt;/math&gt;
for every &lt;math&gt;A&lt;/math&gt;, that can be stored with, or instead of, &lt;math&gt;A&lt;/math&gt;. 
We also define a variant &lt;math&gt;\mathit{alg}^{\prime}&lt;/math&gt; that works on this representation 
and any call to &lt;math&gt;\mathit{alg}(A,B)&lt;/math&gt; is replaced by &lt;math&gt;\mathit{alg}^{\prime}(A^{\prime},B)&lt;/math&gt;, intended to do the same job faster.

== See also ==

* [[Psyco]], a specializing run-time compiler for [[Python (programming language)|Python]]
* [[multi-stage programming]]

==References==

* A. Voronkov, &quot;The Anatomy of Vampire: Implementing Bottom-Up Procedures with Code Trees&quot;, ''Journal of Automated Reasoning'', 15(2), 1995 (''original idea'')

==Further reading==
* A. Riazanov and A. Voronkov, &quot;Efficient Checking of Term Ordering Constraints&quot;, ''Proc. IJCAR'' 2004, Lecture Notes in Artificial Intelligence 3097, 2004 (''compact but self-contained illustration of the method'')
 
* A. Riazanov and A. Voronkov, [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.8542 Efficient Instance Retrieval with Standard and Relational Path Indexing], Information and Computation, 199(1-2), 2005 (''contains another illustration of the method'')

* A. Riazanov, [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.2624 &quot;Implementing an Efficient Theorem Prover&quot;], PhD thesis, The University of Manchester, 2003 (''contains the most comprehensive description of the method and many examples'')

[[Category:Algorithms]]
[[Category:Software optimization]]</text>
    </revision>
  </page>
  <page>
    <title>Hyphenation algorithm</title>
    <id>2842189</id>
    <revision>
      <id>328189373</id>
      <timestamp>2009-11-27T10:35:56Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */accessyear {{cite web}} parameter cleanup using [[Project:AWB]]</comment>
      <text xml:space="preserve">A '''hyphenation algorithm''' is a set of rules (especially one codified for implementation in a computer program) that decides at which points a word can be broken over two lines with a [[hyphen]]. For example, a hyphenation algorithm might decide that ''impeachment'' can be broken as ''impeach-ment'' or ''im-peachment'', but not, say, as ''impe-achment''.

One of the reasons for the complexity of the rules of word-breaking is that different 'dialects' of English tend to differ on the rule: [[American English]] tends to work on sound, while [[British English]] tends to look to the origins of the word and then to sound.  There are also a large number of exceptions which further complicates matters.

Some rules of thumb can be found in the reference &quot;On Hyphenation - Anarchy of Pedantry&quot;.  Among [[algorithmic]] approaches to hyphenation, the one implemented in the [[TeX]] typesetting system is widely used. It is thoroughly documented in the first two volumes of
''[[Computers and Typesetting]]'' and in [[Frank Liang]]'s dissertation. Contrary to the belief that TeX relies on a large dictionary of exceptions, the point of Liang's work was to get the algorithm as accurate as he practically could and keep any exception dictionary small. In TeX's original hyphenation patterns for US English, the exception list contains fourteen words.&lt;ref&gt;{{cite web
 |title       = The Plain TeX hyphenation tables
 |url         = http://www.ctan.org/get/systems/knuth/dist/lib/hyphen.tex
 |accessdate  = June&amp;nbsp;23, 2009
}}
&lt;/ref&gt;

== Hyphenation in Tex ==
Ports of the TeX hyphenation algorithm are available as libraries for several programming languages, including [[Perl]], [[Ruby programming language|Ruby]], [[Python]], and [[PostScript]].

In [[LaTeX]] hyphenation correction can be added by user using: 

&lt;pre&gt;\hyphenation{words}&lt;/pre&gt;

The \hyphenation command declares allowed hyphenation points, where words is a list of words, separated by spaces, in which each hyphenation point is indicated by a - character. For example

&lt;pre&gt;\hyphenation{fortran,er-go-no-mic}&lt;/pre&gt;

indicates that &quot;fortran&quot; cannot be hyphenated and indicates allowed hyphenation points for &quot;ergonomic&quot;.&lt;ref&gt;[http://noodle.med.yale.edu/latex/latex2e-html/ltx-244.html \hyphenation on Hypertext Help with LaTeX]&lt;/ref&gt;

==References==
&lt;references/&gt;

*{{cite web | title=On Hyphenation - Anarchy of Pedantry | work=PC Update, the magazine of Melbourne PC User Group, Australia | url=http://www.melbpc.org.au/pcupdate/9100/9112article4.htm | accessdate=October 6, 2005 }}
*{{cite paper | author=Liang, Franklin Mark | title=Word Hy-phen-a-tion by Com-put-er |
publisher=Stanford University | year=1983 |
url=http://www.tug.org/docs/liang/ }}
*{{cite web | title= TeX-Hyphen | work=Comprehensive Perl Archive Network |
url=http://www.cpan.org/modules/by-module/TeX/ | accessdate=October 18, 2005 }}
*{{cite web | title= text-hyphen | work=RubyForge |
url=http://rubyforge.org/frs/?group_id=294 | accessdate=October 18, 2005 }}
*{{cite web | title= Knuth-Liang hyphenation for the PostScript language | work=anastigmatix.net | url=http://www.anastigmatix.net/postscript/Hyphenate.html | accessdate=October 6, 2005 }}
*{{cite web | title= TeXHyphenator-J: TeX Hyphenator in Java | url=http://texhyphj.sourceforge.net/ | accessdate=September 14, 2006 }}
*{{cite web | title= Hyphenation in Python, using Frank Liang's algorithm | url=http://www.nedbatchelder.com/code/modules/hyphenate.py | accessdate=July 10, 2007 }}
*{{cite web | title= Hyphenator.js-Hyphenation in JavaScript, using Frank Liang's algorithm | url=http://code.google.com/p/hyphenator/ | accessdate=January 3, 2008 }}
*{{cite web | title= Tex::Hyphen - Perl implementation of TeX82 Hyphenation rules | url=http://search.cpan.org/dist/TeX-Hyphen/ }}

== External links ==
* [http://www.ushuaia.pl/hyphen/?ln=en Test TeX/OpenOffice hyphenation algorithm online]

[[Category:Digital typography]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Dictionary of Algorithms and Data Structures</title>
    <id>1661551</id>
    <revision>
      <id>315244372</id>
      <timestamp>2009-09-21T06:05:26Z</timestamp>
      <contributor>
        <username>Cybercobra</username>
        <id>670947</id>
      </contributor>
      <minor/>
      <comment>Reverted 2 edits by [[Special:Contributions/220.225.211.202|220.225.211.202]] identified as [[WP:VAND|vandalism]] to last revision by [[User:Drpaule|Drpaule]]. ([[WP:TW|TW]])</comment>
      <text xml:space="preserve">The '''Dictionary of Algorithms and Data Structures''' is a dictionary style reference for many of the [[algorithm]]s, algorithmic techniques, archetypal problems and [[data structure]]s found in the field of [[computer science]].&lt;ref name=&quot;dads&quot;&gt;[http://www.nist.gov/dads/ Dictionary of Algorithms and Data Structures&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;  The dictionary is maintained by Paul E. Black,&lt;ref name=&quot;dads&quot; /&gt; and is hosted by the Software and Systems Division, Information Technology Laboratory, a part of the [[National Institute of Standards and Technology]].&lt;ref name=&quot;dads&quot; /&gt;

The dictionary is cited in dozens of Wikipedia articles, on topics such as [[Dense graph]], [[AVL tree]], and 
[[Strand sort]].

==References==
{{reflist}}

==External links==
*[http://www.nist.gov/dads/ Dictionary of Algorithms and Data Structures]

[[Category:Algorithms]]
[[Category:Data structures]]
[[Category:Computer science websites]]
[[Category:Government-owned websites]]
[[Category:Online dictionaries]]
[[Category:Wikipedia sources]]

[[ru:Словарь алгоритмов и структур данных]]</text>
    </revision>
  </page>
  <page>
    <title>Merge algorithm</title>
    <id>20362</id>
    <revision>
      <id>306299066</id>
      <timestamp>2009-08-05T23:13:14Z</timestamp>
      <contributor>
        <username>Altenmann</username>
        <id>28438</id>
      </contributor>
      <comment>{{expert-subject|Computer science|date=August 2009}}</comment>
      <text xml:space="preserve">{{expert-subject|Computer science|date=August 2009}}
{{citations missing|date=August 2008}}
'''Merge algorithms''' are a family of [[algorithm]]s that run sequentially over multiple [[sorting algorithm|sorted]] lists, typically producing more sorted lists as output.  This is well-suited for machines with [[tape drive]]s.  Use has declined due to large [[random access memory|random access memories]], and many applications of merge algorithms have faster alternatives when a random-access memory is available.{{Fact|date=May 2009}}

The general merge algorithm has a set&lt;!--non-technical use, don't link--&gt; of [[Pointer (computing)|pointer]]s p&lt;sub&gt;0..n&lt;/sub&gt; that point to positions in a set of lists L&lt;sub&gt;0..n&lt;/sub&gt;.  Initially they point to the first item in each list.  The algorithm is as follows:

While any of p&lt;sub&gt;0..n&lt;/sub&gt; still point to data inside of L&lt;sub&gt;0..n&lt;/sub&gt; instead of past the end:
# do something with the data items p&lt;sub&gt;0..n&lt;/sub&gt; point to in their respective lists
# find out which of those pointers points to the item with the lowest key; advance one of those pointers to the next item in its list

==Analysis==
Merge algorithms generally run in time proportional to the sum of the lengths of the lists; merge algorithms that operate on large numbers of lists at once will multiply the sum of the lengths of the lists by the time to figure out which of the pointers points to the lowest item, which can be accomplished with a [[heap (data structure)|heap]]-based [[priority queue]] in [[Big O notation|O]](log&amp;nbsp;''n'') time, for O(''m''&amp;nbsp;log&amp;nbsp;''n'') time, where ''n'' is the number of lists being merged and ''m'' is the sum of the lengths of the lists. When merging two lists of length ''m'', there is a lower bound of 2''m''&amp;nbsp;&amp;minus;&amp;nbsp;1 comparisons required in the worst case.

The classic merge (the one used in [[merge sort]]) outputs the data item with the lowest key at each step; given some sorted lists, it produces a sorted list containing all the elements in any of the input lists, and it does so in time proportional to the sum of the lengths of the input lists.

In [[parallel computing]], [[Array data structure|array]]s of sorted values may be merged efficiently using an [[all nearest smaller values]] computation.&lt;ref&gt;{{citation|first1=Omer|last1=Berkman|first2=Baruch|last2=Schieber|first3=Uzi|last3=Vishkin|author3-link=Uzi Vishkin|title=Optimal double logarithmic parallel algorithms based on finding all nearest smaller values|journal=Journal of Algorithms|volume=14|pages=344–370|year=1993|issue=3|doi=10.1006/jagm.1993.1018}}.&lt;/ref&gt;

==Language support==

The  [[C++]]'s [[Standard Template Library]] has the function &lt;code&gt;std::merge&lt;/code&gt;, which merges two sorted ranges of iterators, and &lt;code&gt;std::inplace_merge&lt;/code&gt;, which merges two consecutive sorted ranges ''in-place''. In addition, the &lt;code&gt;std::list&lt;/code&gt; (linked list) class has its own &lt;code&gt;merge&lt;/code&gt; method which merges another list into itself. The type of the elements merged must support the less-than (&lt;) operator, or it must be provided with a custom comparator.

[[PHP]] has the array_merge() and array_merge_recursive() functions.

== References ==
{{reflist}}
* [[Donald Knuth]]. ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 158&amp;ndash;160 of section 5.2.4: Sorting by Merging. Section 5.3.2: Minimum-Comparison Merging, pp.197&amp;ndash;207.


[[Category:Algorithms]]
[[Category:Articles with example pseudocode]]

[[ja:マージ]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Evolutionary algorithms</title>
    <id>754055</id>
    <revision>
      <id>306614553</id>
      <timestamp>2009-08-07T15:14:04Z</timestamp>
      <contributor>
        <username>Movses</username>
        <id>1608582</id>
      </contributor>
      <minor/>
      <text xml:space="preserve">An '''evolutionary algorithm''' ('''EA''') is a [[heuristic]] [[optimization (computer science)|optimization]] [[algorithm]] using techniques inspired by mechanisms from [[organic evolution]] such as [[mutation]], [[genetic recombination|recombination]], and [[natural selection]] to find an optimal configuration for a specific system within specific constraints. 

{{catmore}}
{{Commons cat|Evolutionary algorithms}}

[[Category:Machine learning]]
[[Category:Optimization algorithms]]
[[Category:Evolutionary computation|Algorithms]]
[[Category:Artificial life]]
[[Category:Artificial life models| ]]
[[Category:Algorithms]]

[[fa:رده:الگوریتم‌های تکاملی]]
[[pt:Categoria:Algoritmos evolutivos]]
[[ru:Категория:Эволюционные алгоритмы]]
[[vi:Thể loại:Giải thuật tiến hóa]]
[[tr:Kategori:Evrimsel algoritmalar]]
[[uk:Категорія:Еволюційні алгоритми]]
[[zh:Category:进化算法]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Genetic algorithms</title>
    <id>868737</id>
    <revision>
      <id>306469356</id>
      <timestamp>2009-08-06T20:39:10Z</timestamp>
      <contributor>
        <username>Buenasdiaz</username>
        <id>7971019</id>
      </contributor>
      <text xml:space="preserve">A '''genetic algorithm''' ('''GA''') is an [[algorithm]] used to find approximate solutions to difficult-to-solve problems through application of the principles of [[evolutionary biology]] to [[computer science]]. Genetic algorithms use biologically-derived techniques such as [[inheritance]], [[Mutation (genetic algorithm)|mutation]], [[natural selection]], and [[recombination (genetic algorithm)|recombination]]. Genetic algorithms are a particular class of [[evolutionary algorithm]]s.

{{catmore}}

[[Category:Evolutionary algorithms]]
[[Category:Bioinformatics algorithms]]
[[Category:Algorithms]]

[[ca:Categoria:Algorismes genètics]]
[[fa:رده:الگوریتم‌های ژنتیک]]
[[tr:Kategori:Genetik algoritmalar]]
[[vi:Thể loại:Giải thuật di truyền]]
[[uk:Категорія:Генетичні алгоритми]]
[[zh:Category:遗传算法]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Arbitrary precision algorithms</title>
    <id>4859948</id>
    <revision>
      <id>327795205</id>
      <timestamp>2009-11-25T03:17:51Z</timestamp>
      <contributor>
        <username>Rubinbot</username>
        <id>7661631</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[ru:Категория:Длинная арифметика]]</comment>
      <text xml:space="preserve">This category contains articles pertaining to algorithms that are used in [[arbitrary precision arithmetic]]. This includes algorithms for [[multiplication]] and [[division (mathematics)|division]], as well as algorithms for the efficient evaluation of [[mathematical constant]]s and [[special function]]s to high precision. 

See also [[:Category:Number theoretic algorithms]] for arbitrary-precision integer and [[cryptography]] algorithms.

[[Category:Numerical analysis]]
[[Category:Algorithms]]
[[Category:Computer arithmetic]]
[[Category:Computational number theory]]

[[es:Categoría:Algoritmos de precisión arbitraria]]
[[ru:Категория:Длинная арифметика]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Algorithms on strings</title>
    <id>704505</id>
    <revision>
      <id>278691577</id>
      <timestamp>2009-03-21T06:50:30Z</timestamp>
      <contributor>
        <username>DixonD</username>
        <id>4665366</id>
      </contributor>
      <text xml:space="preserve">Pertains to algorithms that operate on [[String (computer science)|string]] datatypes.

[[Category:Algorithms|Strings]]

[[fr:Catégorie:Algorithme sur les chaînes de caractères]]
[[id:Kategori:Algoritma string]]
[[ru:Категория:Строковые алгоритмы]]
[[tr:Kategori:Karakter dizileri üzerine algoritmalar]]
[[uk:Категорія:Рядкові алгоритми]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Numerical analysis</title>
    <id>697506</id>
    <revision>
      <id>320994202</id>
      <timestamp>2009-10-20T13:01:51Z</timestamp>
      <contributor>
        <username>Robertgreer</username>
        <id>2742851</id>
      </contributor>
      <minor/>
      <text xml:space="preserve">{{catmore}} {{seealso|List of numerical analysis topics}} 

{{Commons cat|Numerical analysis}}

[[Category:Computational science]]
[[Category:Applied mathematics]]
[[Category:Mathematical analysis]]
[[Category:Algorithms]]
[[Category:Mathematics of computing]]

&lt;!----&gt;

[[ar:تصنيف:تحليل عددي]]
[[bg:Категория:Числени методи]]
[[cs:Kategorie:Numerická matematika]]
[[de:Kategorie:Numerische Mathematik]]
[[es:Categoría:Análisis numérico]]
[[eo:Kategorio:Cifereca analitiko]]
[[fa:رده:آنالیز عددی]]
[[fr:Catégorie:Calcul numérique]]
[[ko:분류:수치 해석]]
[[id:Kategori:Analisis numerik]]
[[it:Categoria:Analisi numerica]]
[[he:קטגוריה:אנליזה נומרית]]
[[nl:Categorie:Numerieke wiskunde]]
[[ja:Category:数値解析]]
[[nn:Kategori:Numerisk analyse]]
[[pl:Kategoria:Metody numeryczne]]
[[pt:Categoria:Análise numérica]]
[[ro:Categorie:Analiză numerică]]
[[ru:Категория:Численные методы]]
[[sl:Kategorija:Numerična analiza]]
[[sr:Категорија:Нумеричка анализа]]
[[fi:Luokka:Numeerinen analyysi]]
[[sv:Kategori:Numerisk analys]]
[[tr:Kategori:Sayısal analiz]]
[[uk:Категорія:Чисельні методи]]
[[zh:Category:数值分析]]</text>
    </revision>
  </page>
  <page>
    <title>Las Vegas algorithm</title>
    <id>537519</id>
    <revision>
      <id>316048791</id>
      <timestamp>2009-09-25T02:39:04Z</timestamp>
      <contributor>
        <username>Bender2k14</username>
        <id>5560313</id>
      </contributor>
      <minor/>
      <comment>/* Complexity class */ Modified the ZPP link to point to Zero-error_Probabilistic_Polynomial_time instead of the disambigious page for ZPP.</comment>
      <text xml:space="preserve">In [[computing]], a '''Las Vegas algorithm''' is a [[randomized algorithm]] that always gives [[correctness|correct]] results; that is, it always produces the correct result or it informs about the failure.  In other words, a [[Las Vegas, Nevada|Las Vegas]] algorithm does not gamble with the verity of the result; it only gambles with the resources used for the computation. A simple example is randomized [[quicksort]], where the pivot is chosen randomly, but the result is always sorted. The usual definition of a Las Vegas algorithm includes the restriction that the ''expected'' run time always be finite, when the expectation is carried out over the space of random information, or entropy, used in the algorithm.

The name comes from the fact that in Las Vegas, &quot;the house always wins&quot;. {{Fact|date=February 2009}}  Las Vegas algorithms can be used in situations where the number of possible solutions is relatively limited, and where verifying the correctness of a candidate solution is relatively easy while actually calculating the solution is complex.

== Complexity class ==

The [[complexity class]] of [[decision problem]]s that have Las Vegas algorithms with [[expected value|expected]] polynomial runtime is '''[[Zero-error_Probabilistic_Polynomial_time | ZPP]]'''.

It turns out that

:&lt;math&gt;\textrm{ZPP} = \textrm{RP} \cap \,\text{co}\,\textrm{-RP}, \,\!&lt;/math&gt;

which is intimately connected with the way Las Vegas algorithms are sometimes constructed.  Namely the class '''[[RP (complexity)|RP]]''' consists of all decision problems for which a randomized polynomial-time algorithm exists that always answers correctly when the correct answer is &quot;no&quot;, but is allowed to be wrong with a certain probability bounded away from one when the answer is &quot;yes&quot;. When such an algorithm exists for both a problem and its complement (with the answers &quot;yes&quot; and &quot;no&quot; swapped), the two algorithms can be run simultaneously and repeatedly: a few steps of each, taking turns, until one of them returns a definitive answer. This is the standard way to construct a Las Vegas algorithm that runs in expected polynomial time. Note that in general there is no worst case upper bound on the run time of a Las Vegas algorithm.

== Relation to Monte Carlo algorithms ==

Las Vegas algorithms can be contrasted with [[Monte Carlo algorithm]]s, in which the resources used are bounded but the answer is not guaranteed to be correct 100% of the time. By an application of [[Markov's inequality]], a Las Vegas algorithm can be converted into a Monte Carlo algorithm via early termination (assuming the algorithm structure provides for such a mechanism).

== See also ==
* [[Randomness]]
* [[Monte Carlo algorithm]]
* [[Monte Carlo method]]

==References==
* ''Algorithms and Theory of Computation Handbook'', CRC Press LLC, 1999, &quot;Las Vegas algorithm&quot;, in ''Dictionary of Algorithms and Data Structures'' [online], Paul E. Black, ed., U.S. [[National Institute of Standards and Technology]]. 17 July 2006. (accessed May 09, 2009) Available from: [http://www.nist.gov/dads/HTML/lasVegas.html]

[[Category:Randomness]]
[[Category:Algorithms]]

[[de:Las-Vegas-Algorithmus]]
[[es:Algoritmo de Las Vegas]]
[[fr:Algorithme de Las Vegas]]
[[ja:ラスベガス法]]
[[ru:Лас-Вегас (алгоритм)]]
[[simple:Las Vegas algorithm]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Statistical algorithms</title>
    <id>6536797</id>
    <revision>
      <id>105144289</id>
      <timestamp>2007-02-02T18:26:51Z</timestamp>
      <contributor>
        <username>Den fjättrade ankan</username>
        <id>8624</id>
      </contributor>
      <text xml:space="preserve">[[Category:Algorithms|Statistical]]
[[Category:Computational statistics|Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>In-place algorithm</title>
    <id>219861</id>
    <revision>
      <id>326112611</id>
      <timestamp>2009-11-16T05:28:40Z</timestamp>
      <contributor>
        <username>Elsantosneto</username>
        <id>1077605</id>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve">{{Redirect|In-place|execute in place [[file system]]s|execute in place}}

In [[computer science]], an '''in-place algorithm''' (or in [[Latin]] '''in situ''') is an [[algorithm]] which transforms a [[data structure]] using a small, constant amount of extra storage space. The input is usually overwritten by the output as the algorithm executes. An algorithm which is not in-place is sometimes called '''not-in-place''' or '''out-of-place'''.

An algorithm is sometimes informally called in-place as long as it overwrites its input with its output. In reality this is not sufficient (as the case of quicksort demonstrates) nor is it necessary; the output space may be constant, or may not even be counted, for example if the output is to a stream. On the other hand, sometimes it may be more practical to count the output space in determining whether an algorithm is in-place, such as in the first reverse example below; this makes it difficult to strictly define in-place algorithms. In theory applications such as [[log-space reduction]]s, it's more typical to always ignore output space (in these cases it's more essential that the output is ''write-only''). 

== Examples ==

Suppose we want to reverse an [[Array data structure|array]] of ''n'' items. One simple way to do this is:

  '''function''' reverse(a[0..n])
      allocate b[0..n]
      '''for''' i '''from''' 0 '''to''' n
          b[n - i] = a[i]
      '''return''' b

Unfortunately, this requires O(''n'') extra space to create the array ''b'', and allocation is often a slow operation. If we no longer need ''a'', we can instead overwrite it with its own reversal using this in-place algorithm:

  '''function''' reverse-in-place(a[0..n])
      '''for''' i '''from''' 0 '''to''' floor(n/2)
          swap(a[i], a[n-i])

As another example, there are a number of [[sorting algorithm]]s that can rearrange arrays into sorted order in-place, including: [[Bubble sort]], [[Comb sort]], [[Selection sort]], [[Insertion sort]], [[Heapsort]], [[Shell sort]].

[[Quicksort]] is commonly described as an in-place algorithm, but is not in fact one. Most implementations require O(log ''n'') space to support its [[divide and conquer algorithm|divide and conquer]] recursion.

Most [[selection algorithm]]s are also in-place, although some considerably rearrange the input
array in the process of finding the final, constant-sized result.

Some text manipulation algorithms such as [[Trim (programming)|trim]] and reverse may be done in-place.

== In computational complexity ==

In [[computational complexity theory]], in-place algorithms include all algorithms with O(1) space complexity, the class [[Deterministic_space|'''DSPACE''']](1). This class is very limited; it equals the [[regular language]]s.&lt;ref&gt;Maciej Liśkiewicz and Rüdiger Reischuk. [http://citeseer.ist.psu.edu/34203.html The Complexity World below Logarithmic Space]. ''Structure in Complexity Theory Conference'', pp. 64-78. 1994. Online: p. 3, Theorem 2.&lt;/ref&gt; In fact, it does not even include any of the examples listed above.

For this reason, we also consider algorithms in [[L (complexity)|L]], the class of problems requiring O(log ''n'') additional space, to be in-place. Although this seems to contradict our earlier definition, we have to consider that in the abstract world our input can be arbitrarily large. On a real computer, a [[Pointer (computing)|pointer]] requires only a small fixed amount of space, because the amount of physical memory is limited, but in general O(log ''n'') bits are required to specify an index into a list of size ''n''.

Does this mean quicksort is in-place after all? Not at all—technically, it requires O(log&lt;sup&gt;2&lt;/sup&gt; ''n'') space, since each of its O(log ''n'') stack frames contains a constant number of pointers (each of size O(log ''n'')).

Identifying the in-place algorithms with L has some interesting implications; for example, it means that there is a (rather complex) in-place algorithm to determine whether a path exists between two nodes in an [[undirected graph]],&lt;ref&gt;Omer Reingold. [http://www.wisdom.weizmann.ac.il/~reingold/publications/sl.ps Undirected ST-connectivity in Log-Space].  Electronic Colloquium on Computational Complexity. No. 94.&lt;/ref&gt; a problem that requires O(''n'') extra space using typical algorithms such as [[depth-first search]] (a visited bit for each node). This in turn yields in-place algorithms for problems such as determining if a graph is [[bipartite graph|bipartite]] or testing whether two graphs have the same number of [[connected component (graph theory)|connected component]]s. See [[SL (complexity)|SL]] for more information.

== Role of randomness ==

In many cases, the space requirements for an algorithm can be drastically cut by using a [[randomized algorithm]]. For example, say we wish to know if two vertices in a graph of ''n'' vertices are in the same [[Connected component (graph theory)|connected component]] of the graph. There is no known simple, deterministic, in-place algorithm to determine this, but if we simply start at one vertex and perform a [[random walk]] of about 20''n''&lt;sup&gt;3&lt;/sup&gt; steps, the chance that we will stumble across the other vertex provided that it's in the same component is very high. Similarly, there are simple randomized in-place algorithms for primality testing such as the [[Miller-Rabin primality test]], and there are also simple in-place randomized factoring algorithms such as [[Pollard's rho algorithm]]. See [[RL (complexity)|RL]] and [[BPL (complexity)|BPL]] for more discussion of this phenomenon.

== In functional programming ==

[[Functional programming]] languages often discourage or don't support explicit in-place algorithms that overwrite data, since this is a type of [[side effect (computer science)|side effect]]; instead, they only allow new data to be constructed. However, good functional language compilers will often recognize when an object very similar to an existing one is created and then the old one thrown away, and will optimize this into a simple mutation &quot;under-the-hood&quot;.

Note that it is possible in principle to carefully construct in-place algorithms that don't modify data (unless the data is no longer being used), but this is rarely done in practice.  See [[purely functional data structure]]s.

==See also==
*[[Sorting algorithm#Comparison of algorithms|Table of in-place and not-in-place algorithms]]

== References ==
&lt;references/&gt;

[[Category:Algorithms]]

[[de:In-place]]
[[it:Algoritmo in loco]]
[[he:אלגוריתם תוך-מקומי]]
[[ja:In-placeアルゴリズム]]
[[pl:Algorytm in situ]]
[[zh:原地算法]]</text>
    </revision>
  </page>
  <page>
    <title>Pidgin code</title>
    <id>958409</id>
    <revision>
      <id>309592052</id>
      <timestamp>2009-08-23T12:37:32Z</timestamp>
      <contributor>
        <username>Pohta ce-am pohtit</username>
        <id>7417813</id>
      </contributor>
      <comment>{{PL-stub}}</comment>
      <text xml:space="preserve">{{unreferenced|date=June 2009}}
In [[computer programming]], '''pidgin code''' is a mixture of several [[programming language]]s in the same program, or [[pseudocode]] that is a mixture of a programming language with [[natural language]] descriptions. Hence the name: the mixture is a programming language analogy to a [[pidgin]] in [[natural language]]s.

In [[numerical computation]], mathematical style [[pseudocode]] is sometimes called '''pidgin code''', for example ''pidgin [[ALGOL]]'' (the origin of the concept), ''pidgin [[Fortran]]'', ''pidgin [[BASIC]]'', ''pidgin [[Pascal (programming language) | Pascal]]'', and ''pidgin [[C (programming language) | C]]''. It is a compact and often informal notation that blends [[syntax]] taken from a conventional [[programming language]] with mathematical [[notation]], typically using [[set theory]] and [[matrix (mathematics)|matrix]] operations, and perhaps also [[natural language]] descriptions. 

It can be understood by a wide range of mathematically trained people, and is used as a way to describe [[algorithm]]s where the [[control structure]] is made explicit at a rather high level of detail, while some data structures are still left at an abstract level, independent of any specific programming language. 

Normally non-[[ASCII]] [[typesetting]] is used for the mathematical equations, for example by means of [[TeX]] or [[MathML]] markup, or proprietary [[Formula editor]] formats. 

These are examples of articles that contain mathematical style pseudo code:

&lt;table border=0 width=&quot;90%&quot;&gt;&lt;tr valign=top&gt;&lt;td width=&quot;40%&quot;&gt;
*[[Algorithm]]
*[[Conjugate gradient method]]
*[[Ford-Fulkerson algorithm]]
*[[Gauss–Seidel method]]
*[[Generalized minimal residual method]]
*[[Jacobi eigenvalue algorithm]]
*[[Jacobi method]]
&lt;/td&gt;&lt;td&gt;
*[[Karmarkar's algorithm]]
*[[Particle swarm optimization]]
*[[Stone method]]
*[[Successive over-relaxation]]
*[[Symbolic Cholesky decomposition]]
*[[Tridiagonal matrix algorithm]]
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

==See also==
* [[Pseudocode]]
* [[:Wikipedia:Algorithms on Wikipedia]] - Wikipedia guidelines for mathematical style pseudo code.

{{PL-stub}}

[[Category:Programming language topics]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Randomized algorithm</title>
    <id>495383</id>
    <revision>
      <id>325264786</id>
      <timestamp>2009-11-11T16:28:06Z</timestamp>
      <contributor>
        <username>Gwern</username>
        <id>2164608</id>
      </contributor>
      <comment>/* Where randomness helps */ +cm</comment>
      <text xml:space="preserve">{{Probabilistic}}
A '''randomized algorithm''' or '''probabilistic algorithm''' is an [[algorithm]] which employs a degree of [[randomness]] as part of its logic. The algorithm typically uses [[Uniform distribution (discrete)|uniformly random]] bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the &quot;average case&quot; over all possible choices of random bits. Formally, the algorithm's performance will be a [[random variable]] determined by the random bits; thus either the running time, or the output (or both) are random variables.

In both cases, random performance and random output, the term &quot;algorithm&quot; for a procedure is somewhat questionable, as it is no longer formally [[Effective method|effective]].&lt;ref&gt;&quot;Probabilistic algorithms should not be mistaken with methods (which I refuse to call algorithms), which produce a result which has a high probability of being correct. It is essential that an algorithm produces correct results (discounting human or computer errors), even if this happens after a very long time.&quot; Henri Cohen (2000). ''A Course in Computational Algebraic Number Theory''. Springer-Verlag, p. 2.&lt;/ref&gt;
However, in some cases, probabilistic algorithms are the only practical means of solving a problem.&lt;ref&gt;&quot;In [[primality test|testing primality]] of very large numbers chosen at random, the chance of stumbling upon a value that fools the [[Fermat primality test|Fermat test]] is less than the chance that [[cosmic radiation]] will cause the computer to make an error in carrying out a 'correct' algorithm. Considering an algorithm to be inadequate for the first reason but not for the second illustrates the difference between mathematics and engineering.&quot; [[Han Abelson]] and [[Gerald J. Sussman]] (1996). ''[[Structure and Interpretation of Computer Programs]]''. [[MIT Press]], [http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#footnote_Temp_80 section 1.2].&lt;/ref&gt;

In common practice, randomized algorithms are approximated using a [[pseudorandom number generator]] in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.

== Motivation ==

As a motivating example, consider the problem of finding an 'a' in an [[Array data structure|array]] of ''n'' elements, given that half are 'a's and the other half are 'b's. The obvious approach is to look at each element of the array, but this would take very long (n/2 operations) if the array were ordered as 'b's first followed by 'a's. There is a similar drawback with checking in the reverse order, or checking every second element. In fact, with any [[strategy]] at all in which the order in which the elements will be checked is fixed, i.e, a ''deterministic'' algorithm, we cannot guarantee that the algorithm will complete quickly ''for all possible inputs''. On the other hand, if we were to check array elements ''at random'', then we will quickly find an 'a' ''with high probability'', ''whatever be the input''.

Randomized algorithms are particularly useful when faced with a malicious &quot;adversary&quot; or [[attacker]] who deliberately tries to feed a bad input to the algorithm (see [[worst-case complexity]] and [[competitive analysis (online algorithm)]]) such as in the [[Prisoner's_dilemma|Prisoner's dilemma]]. It is for this reason that [[randomness]] is ubiquitous in [[cryptography]]. In cryptographic applications, pseudo-random numbers cannot be used, since the adversary can predict them, making the algorithm effectively deterministic. Therefore either a source of truly random numbers or a [[cryptographically secure pseudo-random number generator]] is required.  Another area in which randomness is inherent is [[quantum computer|quantum computing]].

In the example above, the randomized algorithm always outputs the correct answer, but its running time is a random variable.  Sometimes we want the algorithm to complete in a fixed amount of time (as a function of the input size), but allow a ''small probability of error''. The former type are called [[Las Vegas algorithm]]s, and the latter are [[Monte Carlo algorithm]]s (related to the [[Monte Carlo method]] for simulation). Observe that any Las Vegas algorithm can be converted into a Monte Carlo algorithm (via [[Markov's inequality]]), by having it output an arbitrary, possibly incorrect answer if it fails to complete within a specified time. Conversely, if an efficient verification procedure exists to check whether an answer is correct, then a Monte Carlo algorithm can be converted into a Las Vegas algorithm by running the Monte Carlo algorithm repeatedly till a correct answer is obtained.

== Computational complexity ==&lt;!-- [[Probabilistic complexity]] and  [[Probabilistic computational complexity]] redirect here --&gt;

[[Computational complexity theory]] models randomized algorithms as ''[[probabilistic Turing machine|probabilistic]] [[Turing machine]]s''. Both  [[Las Vegas algorithm|Las Vegas]] and [[Monte Carlo algorithm]]s are considered, and several [[complexity class]]es are studied. The most basic randomized complexity class is [[RP (complexity)|RP]], which is the class of [[decision problem]]s for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2. The complement class for RP is co-RP. Problem classes having (possibly nonterminating) algorithms with [[polynomial time]] average case running time whose output is always correct are said to be in [[ZPP]]. 

The class of problems for which both YES and NO-instances are allowed to be identified with some error is called [[BPP]]. This class acts as the randomized equivalent of [[P_(complexity)|P]], i.e BPP represents the class of efficient randomized algorithms. 

==History==
Historically, the first randomized algorithm was a method developed by [[Michael O. Rabin]] for the [[closest pair of points problem|closest pair problem]] in [[computational geometry]]. The study of randomized algorithms was spurred by the 1977 discovery of a [[Solovay-Strassen primality test|randomized primality test]] (i.e., determining the [[primality test|primality]] of a number) by [[Robert M. Solovay]] and [[Volker Strassen]]. Soon afterwards [[Michael O. Rabin]] demonstrated that the 1976 [[Miller-Rabin primality test|Miller's primality test]] can be turned into a randomized algorithm. At that time, no practical [[deterministic algorithm]] for primality was known. 

The Miller-Rabin primality test relies on a binary relation between two positive integers ''k'' and ''n'' that can be expressed by saying that ''k'' &quot;is a witness to the compositeness of&quot; ''n''.  It can be shown that
* If there is a witness to the compositeness of ''n'', then ''n'' is composite (i.e., ''n'' is not [[prime number|prime]]), and
* If ''n'' is composite then at least three-fourths of the natural numbers less than ''n'' are witnesses to its compositeness, and
* There is a fast algorithm that, given ''k'' and ''n'', ascertains whether ''k'' is a witness to the compositeness of ''n''.
Observe that this implies that the primality problem is in Co-RP.

If one [[random]]ly chooses 100 numbers less than a composite number ''n'', then the probability of failing to find such a &quot;witness&quot; is (1/4)&lt;sup&gt;100&lt;/sup&gt; so that for most practical purposes, this is a good [[primality test]].  If ''n'' is big, there may be no other test that is practical. The probability of error can be reduced to an arbitrary degree by performing enough independent tests. 

Therefore, in practice, there is no penalty associated with accepting a small probability of error, since with a little care the probability of error can be made astronomically small.  Indeed, even though a deterministic polynomial-time primality test has since been [[AKS primality test|found]], it has not replaced the older probabilistic tests in [[cryptography|cryptographic]] [[computer software|software]] nor is it expected to do so for the foreseeable future.

== Applications ==


=== Quicksort ===
[[Quicksort]] is a familiar, commonly-used algorithm in which randomness can be useful. Any deterministic version of this algorithm requires ''[[Big O notation|O]](n&lt;sup&gt;2&lt;/sup&gt;)'' time to sort ''n'' numbers for some well-defined class of degenerate inputs (such as an already sorted array), with the specific class of inputs that generate this behavior defined by the protocol for pivot selection. However, if the algorithm selects pivot elements uniformly at random, it has a provably high probability of finishing in ''O(n log n)'' time regardless of the characteristics of the input.

=== Randomized incremental constructions in geometry ===
In [[computational geometry]], a standard technique to build a structure like a [[convex hull]] or [[Delaunay Triangulation]] is to randomly permute the input points and then insert them one by one into the existing structure. The randomization ensures that the expected number of changes to the structure caused by an insertion is small, and so the expected running time of the algorithm can be upper bounded. This technique is known as [[randomized incremental construction]]&lt;ref&gt;Seidel R. [http://www.cs.berkeley.edu/~jrs/meshpapers/Seidel.ps.gz Backwards Analysis of Randomized Geometric Algorithms]. &lt;/ref&gt;.

=== Graph cuts ===

Randomized algorithms can also be used to solve [[graph theory]] problems, as demonstrated by this randomized [[minimum cut]] algorithm:

 find_min_cut(undirected graph G) {
     while there are more than 2 [[node (computer science)|node]]s in G do {
         pick an [[graph theory|edge]] (u,v) at random in G
         contract the edge, while preserving multi-edges
         remove all [[Glossary of graph theory|loop]]s
     }
     output the remaining edges
 }

Here, contracting an edge (u,v) means adding a new vertex w, replacing any edge (u,x) or (v,x) with (w,x), and then deleting u and v from G. 

Let  ''n = |V[G]|''. It can be shown that this algorithm outputs a minimum cut with probability at least ''n&lt;sup&gt;-2&lt;/sup&gt;'', thus running it ''n&lt;sup&gt;2&lt;/sup&gt;log(n)'' times and taking the smallest output cut, we find a minimum cut with high probability.

==Derandomization==

Randomness can be viewed as a resource, like space and time. Derandomization is then the process of ''removing'' randomness (or using as little of it as possible). From the viewpoint of [[computational complexity]], derandomizing an efficient randomized algorithm is the question, Is [[P_(complexity)|P]] = [[BPP]] ?

There are also specific methods that can be employed to derandomize particular randomized algorithms:
* the [[method of conditional expectation]], and its generalization, [[pessimistic estimator]]s
* [[discrepancy theory]] (which is used to derandomize geometric algorithms)
* the exploitation of limited independence in the random variables used by the algorithm, such as the [[pairwise independence]] used in [[universal hashing]]
* the use of [[expander graph]]s (or [[disperser]]s in general) to ''amplify'' a limited amount of initial randomness (this last approach is also referred to as generating [[pseudorandom]] bits from a random source, and leads to the related topic of pseudorandomness)

== Where randomness helps ==

When the model of computation is restricted to [[Turing machine]]s, it is currently an open question whether the ability to make random choices allows some problems to be solved in polynomial time that cannot be solved in polynomial time without this ability; this is the question of whether P = BPP. However, in other contexts, there are specific examples of problems where randomization yields strict improvements.
* Based on the initial motivating example: given an exponentially-long string of 2&lt;sup&gt;''k''&lt;/sup&gt; characters, half a's and half b's, a [[random access machine]] requires at least 2&lt;sup&gt;''k-1''&lt;/sup&gt; lookups in the worst-case to find the index of an ''a''; if it is permitted to make random choices, it can solve this problem in an expected polynomial number of lookups. &lt;!-- Does the randomized lookup still have the same worst-case performance? --&gt;
* In [[communication complexity]], the equality of two strings can be verified using &lt;math&gt;\log n&lt;/math&gt; bits of communication with a randomized protocol. Any deterministic protocol requires &lt;math&gt;\Theta(n)&lt;/math&gt; bits. 
* The volume of a convex body can be estimated by a randomized algorithm to arbitrary precision in polynomial time&lt;ref&gt;{{citation|last1=Dyer|first1=M.|last2=Frieze|first2=A.|last3=Kannan|first3=R.|year=1991|title=A random polynomial-time algorithm for approximating the volume of convex bodies|journal=[[Journal of the ACM]]|volume=38|issue=1|year=1991|pages=1–17|doi=10.1145/102782.102783}}.&lt;/ref&gt; . [[Imre Bárány|Bárány]] and [[Zoltán Füredi|Füredi]] showed that no deterministic algorithm can do the same&lt;ref&gt;{{citation|last1=Füredi|first1=Z.|author1-link=Zoltán Füredi|last2=Bárány|first2=I.|year=1986|contribution=Computing the volume is difficult|title=[[Symposium on Theory of Computing|Proc. 18th ACM Symposium on Theory of Computing]] (Berkeley, California, May 28 - 30, 1986)|publisher=ACM|location=New York, NY|pages=442–447|doi=10.1145/12130.12176}}.&lt;/ref&gt;. This is true unconditionally, i.e without relying on any complexity-theoretic assumptions.
* A more complexity-theoretic example of a place where randomness appears to help is the class [[IP_(complexity)|IP]]. IP consists of all languages that can be accepted (with high probability) by a polynomially long interaction between an all-powerful prover and a verifier that implements a BPP algorithm. IP = [[PSPACE]]&lt;ref&gt;{{citation|last=Shamir|first=A.|authorlink=Adi Shamir|year=1992|title=IP = PSPACE|journal=Journal of the ACM|volume=39|issue=4|year=1992|pages=869–877|doi=10.1145/146585.146609}}.&lt;/ref&gt;. However, if it is required that the verifier be deterministic, then IP = [[NP_(complexity)|NP]].
* In a Chemical Reaction Network (a finite set of reactions like A+B → 2C + D operating on a finite number of molecules), the ability to ever reach a given target state from an initial state is decidable, while even approximating the probability of ever reaching a given target state (using the standard concentration-based probability for which reaction will occur next) is undecidable.  More specifically, a [[Turing machine]] can be simulated with arbitrarily high probability of running correctly for all time, only if a random CRN is used.  With a simple nondeterministic CRN (any possible reaction can happen next), the computational power is limited to [[Primitive_recursive|primitive recursive functions]].

==See also==
*[[Probabilistic analysis of algorithms]]

==Notes==
{{reflist}}

==References==
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 1990. ISBN 0-262-03293-7. Chapter 5: Probabilistic Analysis and Randomized Algorithms, pp.91&amp;ndash;122.
* Don Fallis. 2000. [http://dx.doi.org/10.1093/bjps/51.2.255 &quot;The Reliability of Randomized Algorithms.&quot;] ''British Journal for the Philosophy of Science'' 51:255-71.
* M. Mitzenmacher and [[Eli Upfal|E. Upfal]]. Probability and Computing : Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, New York (NY), 2005.
* [[Rajeev Motwani]] and P. Raghavan. Randomized Algorithms. Cambridge University Press, New York (NY), 1995.
* [[Rajeev Motwani]] and P. Raghavan. Randomized Algorithms. A survey on Randomized Algorithms. [http://portal.acm.org/citation.cfm?id=234313.234327]
* {{cite book|author = [[Christos Papadimitriou]] | year = 1993 | title = Computational Complexity | publisher = Addison Wesley | edition = 1st edition | isbn = 0-201-53082-1}} Chapter 11: Randomized computation, pp.241&amp;ndash;278.
* M. O. Rabin. (1980), &quot;Probabilistic Algorithm for Testing Primality.&quot; ''Journal of Number Theory'' 12:128-38.

[[Category:Algorithms]]
[[Category:Probabilistic complexity theory]]
[[Category:Analysis of algorithms]]


[[bn:সম্ভাবনাভিত্তিক অ্যালগোরিদম]]
[[cs:Pravděpodobnostní algoritmy]]
[[de:Randomisierter Algorithmus]]
[[es:Algoritmo probabilista]]
[[eo:Hazardigita algoritmo]]
[[fr:Algorithme probabiliste]]
[[ko:확률적 알고리즘]]
[[he:אלגוריתם אקראי]]
[[ja:乱択アルゴリズム]]
[[pl:Algorytm probabilistyczny]]
[[pt:Algoritmo probabilístico]]
[[th:ขั้นตอนวิธีแบบสุ่ม]]
[[zh:随机化算法]]</text>
    </revision>
  </page>
  <page>
    <title>Adaptive algorithm</title>
    <id>8286430</id>
    <revision>
      <id>306754501</id>
      <timestamp>2009-08-08T07:26:52Z</timestamp>
      <contributor>
        <username>Veinor</username>
        <id>429249</id>
      </contributor>
      <text xml:space="preserve">An '''adaptive algorithm''' is an [[algorithm]] that changes its behavior based on the resources available. For example, [[stable partition]], using no additional memory is ''O''(''n'' lg ''n'') but given ''O''(''n'') memory, it can be ''O''(''n'') in time. As implemented by the [[C++ Standard Library]], [http://www.sgi.com/tech/stl/stable_partition.html &lt;code&gt;stable_partition&lt;/code&gt;] is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory.  Another example is [[adaptive sort]], whose behaviour changes upon the presortedness of its input.

[[Category:Algorithms]]


{{soft-eng-stub}}

[[ru:Адаптивный алгоритм]]
[[fr:Algorithme adaptatif]]
[[uk:Адаптивний алгоритм]]</text>
    </revision>
  </page>
  <page>
    <title>Out-of-core algorithm</title>
    <id>1881722</id>
    <revision>
      <id>311682619</id>
      <timestamp>2009-09-03T16:41:45Z</timestamp>
      <contributor>
        <ip>90.185.76.244</ip>
      </contributor>
      <text xml:space="preserve">In computer science and applications, '''out-of-core''' refers to [[algorithms]] which process data that is too large to fit into a computer's [[main memory]] at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory such as hard drive or tape drives.  

A typical example is [[geographic information system]]s, especially [[digital elevation model]]s, where the full data set easily exceeds several [[gigabytes]] or even [[terabytes]] of data.

[[Category:Algorithms]]
[[Category:Articles lacking sources (Erik9bot)]]

[[da:Out-of-core algoritme]]

{{Comp-sci-stub}}</text>
    </revision>
  </page>
  <page>
    <title>Algorithm design</title>
    <id>10433312</id>
    <revision>
      <id>324615390</id>
      <timestamp>2009-11-08T08:57:45Z</timestamp>
      <contributor>
        <ip>122.167.28.21</ip>
      </contributor>
      <comment>/* Notes */</comment>
      <text xml:space="preserve">'''Algorithm design''' is a specific method to create a mathematical process in solving problems.  Applied algorithm design is '''[[algorithm engineering]].'''

Algorithm design is identified and incorporated into many solution theories of [[operation research]], such as [[dynamic programming]] and [[Divide and conquer algorithm|divide-and-conquer]].  Techniques for designing and implementing algorithm designs are algorithm design patterns &lt;ref&gt;{{citation|url=http://ww3.algorithmdesign.net/ch00-front.html|title=Algorithm Design: Foundations, Analysis, and Internet Examples|last1=Goodrich|first1=Michael T.|last2=Tamassia|first2=Roberto|author2-link=Roberto Tamassia|publisher=John Wiley &amp; Sons, Inc.|year=2002|isbn=0-471-38365-1}}.&lt;/ref&gt; , such as template method patterns and decorator patterns, and uses of data structures, and name and sort lists.  Some current day uses of algorithm design can be found in internet retrieval processes of web crawling packet routing and caching. 

Mainframe programming languages such as [[ALGOL]] (for ''Algo''rithmic ''l''anguage), [[FORTRAN]], [[COBOL]], PL/I, [[SAIL]], and SNOBOL are computing tools to implement an &quot;algorithm design&quot;... but, an &quot;algorithm design&quot; (a/d) is not a language. An a/d can be a hand written process, eg. set of equations, a series of mechanical processes done by hand, an analog piece of equipment, or a digital process and/or processor.

== Notes ==
{{reflist}}types of techniques:
1: Divide &amp; conquer
2: Greedy
3: Dynamic programming
4: Backtracking
5: Branch &amp; bond

== Further reading ==
* [http://www.csc.liv.ac.uk/~ped/teachadmin/algor/algor.html  Algorithm Design Paradigms] - Overview by Paul Dunne at the University of Liverpool
* [http://www.cs.sunysb.edu/~algorith/ Stony Brook Algorithm Repository] by [[Steven S. Skiena]], Department of Computer Science , State University of New York

{{Mathanalysis-stub}}

[[Category:Algorithms]]
[[Category:Operations research]]

[[fa:طراحی الگوریتم]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Analysis of algorithms</title>
    <id>3234221</id>
    <revision>
      <id>295740772</id>
      <timestamp>2009-06-11T08:03:50Z</timestamp>
      <contributor>
        <username>Gregbard</username>
        <id>1193764</id>
      </contributor>
      <minor/>
      <comment>Quick-adding category [[:Category:Mathematical analysis|Mathematical analysis]] (using [[WP:HOTCAT|HotCat]])</comment>
      <text xml:space="preserve">{{catmore}}

[[Category:Algorithms]]
[[Category:Computational complexity theory]]
[[Category:Mathematical analysis]]

[[eo:Kategorio:Analitiko de algoritmoj]]</text>
    </revision>
  </page>
  <page>
    <title>XOR swap algorithm</title>
    <id>145555</id>
    <revision>
      <id>328370247</id>
      <timestamp>2009-11-28T12:35:41Z</timestamp>
      <contributor>
        <ip>83.254.57.110</ip>
      </contributor>
      <comment>corrected code style</comment>
      <text xml:space="preserve">In [[computer programming]], the '''XOR swap''' is an [[algorithm]] that uses the [[exclusive disjunction|XOR]] [[bitwise operation]] to [[swap (computer science)|swap]] distinct values of [[variable (programming)|variable]]s having the same [[data type]] without using a temporary variable. Being &quot;distinct&quot; in this case means that the variables are stored at distinct memory addresses; the actual values of the variables do not have to be different.

==The algorithm==
Standard swapping algorithms require the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:
 X := X XOR Y
 Y := X XOR Y
 X := X XOR Y
The algorithm typically corresponds to three [[machine code]] instructions. For example, in IBM [[System/370]] assembly code:
 XR    R1,R2
 XR    R2,R1
 XR    R1,R2
where R1 and R2 are [[processor register|register]]s and each XR operation leaves its result in the register named in the first argument.

However, the problem still remains that if ''x'' and ''y'' use the same storage location, the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be &quot;swapped with itself&quot;.  (Note that this is ''not'' the same as if ''x'' and ''y'' have the same values.  The trouble only comes when ''x'' and ''y'' use the same storage location.)

==Proof that the XOR swap works==
The [[binary operation]] XOR over bit strings of length &lt;math&gt;N&lt;/math&gt; exhibits the following properties (where &lt;math&gt;\oplus&lt;/math&gt; denotes XOR):&lt;ref&gt;
The first three properties, along with the existence of an inverse for each element, are the definition of an [[Abelian group]].  The last property is a structural feature of XOR not necessarily shared by other Abelian groups, nor [[group (mathematics)|groups]] in general.
&lt;/ref&gt;
* '''L1.''' [[Commutative operation|Commutativity]]: &lt;math&gt;A \oplus B = B \oplus A&lt;/math&gt;
* '''L2.''' [[Associativity]]: &lt;math&gt;(A \oplus B) \oplus C = A \oplus (B \oplus C)&lt;/math&gt;
* '''L3.''' [[Identity element|Identity exists]]: there is a bit string, 0, (of length ''N'') such that &lt;math&gt;A \oplus 0 = A&lt;/math&gt; for any &lt;math&gt;A&lt;/math&gt;
* '''L4.''' Each element is its own [[inverse element|inverse]]: for each &lt;math&gt;A&lt;/math&gt;, &lt;math&gt;A \oplus A = 0&lt;/math&gt;.

Suppose that we have two distinct registers &lt;code&gt;R1&lt;/code&gt; and &lt;code&gt;R2&lt;/code&gt; as in the table below, with initial values ''A'' and ''B'' respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.

{| class=&quot;wikitable&quot;
|- 
! Step
! Operation
! Register 1
! Register 2
! Reduction
|-
| 0 || Initial value || &lt;math&gt;\ A&lt;/math&gt; || &lt;math&gt;\ B&lt;/math&gt; || — 
|-
| 1 || &lt;code&gt;R1 := R1 XOR R2&lt;/code&gt;  || &lt;math&gt;\ A \oplus B&lt;/math&gt; || &lt;math&gt;\ B&lt;/math&gt; || — 
|- 
| 2 || &lt;code&gt;R2 := R1 XOR R2&lt;/code&gt;  || &lt;math&gt;\ A \oplus B&lt;/math&gt; || &lt;math&gt;\begin{align} (A \oplus B) \oplus B =&amp; A \oplus (B \oplus B) \\=&amp; A \oplus 0 \\=&amp; A \end{align}&lt;/math&gt; || '''L2&lt;br&gt; L4&lt;br&gt; L3'''
|-
| 3 || &lt;code&gt;R1 := R1 XOR R2&lt;/code&gt;  || &lt;math&gt;\begin{align} (A \oplus B) \oplus A =&amp; A \oplus (A \oplus B) \\=&amp; (A \oplus A) \oplus B \\=&amp; 0 \oplus B \\=&amp; B \end{align}&lt;/math&gt; || &lt;math&gt;\ A&lt;/math&gt; || '''L1&lt;br&gt; L2&lt;br&gt; L4&lt;br&gt; L3'''
|-
|}

==Code example==
A [[C (programming language)|C]] function that implements the XOR swap algorithm:
&lt;!-- This display template is broken in IE 6, the top half of the pretty bordered box is cut off. --&gt;
&lt;source lang=&quot;c&quot;&gt;
 void xorSwap (int *x, int *y) {
     if (x != y) {
         *x ^= *y;
         *y ^= *x;
         *x ^= *y;
     }
 }
&lt;/source&gt;
Note that the code does not swap the integers passed immediately, but first checks if their memory locations are distinct. This is because the algorithm works only when x and y refer to distinct integers (otherwise, it will erroneously set &lt;code&gt;*x = *y = 0&lt;/code&gt;).

The body of this function is sometimes seen incorrectly shortened to &lt;code&gt;if (x != y) *x^=*y^=*x^=*y;&lt;/code&gt;. This code has undefined behavior, since it modifies the [[lvalue]] &lt;code&gt;*x&lt;/code&gt; twice without an intervening [[sequence point]].

==Reasons for use in practice==
In most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which it may be practical include:
* On a processor where the instruction set encoding permits the XOR swap to be encoded in a smaller number of bytes;
* In a region with high [[register pressure]], it may allow the [[register allocator]] to avoid spilling a register.

Because these situations are rare, most optimizing compilers do not generate XOR swap code.

==Reasons for avoidance in practice==
Most modern compilers can optimize away the temporary variable in the naive swap, in which case the naive swap uses the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster.&lt;ref&gt;How to swap two integers in C++: http://big-bad-al.livejournal.com/98093.html&lt;/ref&gt; The XOR swap is also much less readable, and can be completely opaque to anyone who isn't already familiar with the technique.

On modern personal computers, the XOR technique is considerably slower than using a temporary variable to do swapping. One reason is that modern CPUs strive to execute commands in parallel; see [[Instruction pipeline]]. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order. If efficiency is of tremendous concern, it is advised to test the speeds of both the XOR technique and temporary variable swapping on the target architecture.

Benchmarks suggest that in the [[x86]] instruction set, XOR swap is much faster than using the XCHG instruction, and while the C++ std::swap function tends to be faster than XOR swap on laptops, the opposite is true with server hardware.&lt;ref&gt;Benchmark of swapping algorithms in C/C++ and assembly (Norwegian): http://nerdvar.com/vis/66#resultater&lt;/ref&gt; Furthermore, although swapping registers always was the fastest algorithm, it might require manual reorganization of the stack in the [[GNU_Compiler_Collection|GCC]] generated assembly in order to beat memory swapping on some computers, but mostly not.

===The XCHG instruction===
Modern [[optimizing compiler]]s work by translating the code they are given into an [[intermediate representation|internal flow-based representation]] which they transform in many ways before producing their machine-code output. These compilers are more likely to recognize and optimize a conventional (temporary-based) swap than to recognize the high-level language statements that correspond to an XOR swap. Many times, what is written as a swap in high-level code is translated by the compiler into a simple internal note that two variables have swapped memory addresses, rather than any amount of machine code. Other times, when the target architecture supports it, the compiler can use a single XCHG (exchange) instruction which performs the swap in a single operation.

An XCHG operation was available as long ago as 1964, on the [[PDP-6]] (where it was called EXCH) and in 1970 on the [[Datacraft]] 6024 series (where it was called XCHG). The [[Intel 8086]], released in 1978, also included an instruction named XCHG. All three of these instructions swapped registers with registers, or registers with memory, but were unable to swap the contents of two memory locations. The [[Motorola 68000]]'s EXG operation can only swap registers with registers. The [[PDP-10]] inherited the PDP-6's EXCH instruction, but the [[PDP-11]] (the machine on which the [[C (programming language)|C programming language]] was developed) did not.

In modern processors (e.g. [[x86 architecture|x86]]) the XCHG instruction may impose an implicit ''LOCK'' instruction so that the operation is [[atomic operation|atomic]]. One wishing to avoid this lock should only use XCHG to swap [[processor register|register]]s and not [[random access memory|memory]]. However, the ability to atomically swap memory is useful for writing locking primitives used in threaded or multiprocessing applications.

===Aliasing===
The XOR swap is also complicated in practice by [[aliasing (computing)|aliasing]]. As noted above, if an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.

==Variations==
The underlying principle of the XOR swap algorithm can be applied to any reversible binary operation. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:

&lt;source lang=&quot;c&quot;&gt;
 void addSwap (int *x, int *y)
 {
     if (x != y) {
         *x = *x + *y;
         *y = *x - *y;
         *x = *x - *y;
     }
 }
&lt;/source&gt;

Unlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as [[modular arithmetic]] or [[bignum]]s to guarantee that the computation of &lt;code&gt;X + Y&lt;/code&gt; cannot cause an error due to [[integer overflow]]. Therefore, it is seen even more rarely in practice than the XOR swap.

== Notes ==
&lt;references/&gt;

==See also==
*[[Symmetric difference]]
*[[XOR linked list]]

[[Category:Algorithms]]
[[Category:Articles with example C code]]

[[et:XOR vahetus algoritm]]
[[ko:XOR 교체 알고리즘]]
[[he:החלפה בעזרת XOR]]
[[ja:XOR交換アルゴリズム]]
[[pl:Zamiana wartości zmiennych]]
[[pt:Algoritmo Xor Swap]]
[[ru:Алгоритм обмена при помощи исключающего ИЛИ]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Networking algorithms</title>
    <id>12026628</id>
    <revision>
      <id>198423516</id>
      <timestamp>2008-03-15T15:47:48Z</timestamp>
      <contributor>
        <username>Andreas Kaufmann</username>
        <id>72502</id>
      </contributor>
      <text xml:space="preserve">[[Algorithm]]s used for [[computer networking]].

[[Category:Algorithms]]
[[Category:Computer networking|Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Sorting algorithms</title>
    <id>11604026</id>
    <revision>
      <id>315378042</id>
      <timestamp>2009-09-21T22:03:12Z</timestamp>
      <contributor>
        <username>JAnDbot</username>
        <id>1725149</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[lt:Kategorija:Rikiavimo algoritmai]]</comment>
      <text xml:space="preserve">{{catmore|sorting algorithm}}
{{Commons cat|Sort algorithms}}

[[Category:Combinatorial algorithms]]
[[Category:Order theory]]
[[Category:Algorithms]]

[[ar:تصنيف:خوارزميات ترتيب]]
[[cs:Kategorie:Řadicí algoritmy]]
[[de:Kategorie:Sortieralgorithmus]]
[[es:Categoría:Algoritmos de ordenamiento]]
[[eo:Kategorio:Algoritmoj de ordigo]]
[[fr:Catégorie:Algorithme de tri]]
[[ko:분류:정렬 알고리즘]]
[[id:Kategori:Algoritma pengurutan]]
[[is:Flokkur:Röðunarreiknirit]]
[[it:Categoria:Algoritmi di ordinamento]]
[[he:קטגוריה:אלגוריתמי מיון]]
[[lt:Kategorija:Rikiavimo algoritmai]]
[[hu:Kategória:Rendezési algoritmusok]]
[[nl:Categorie:Sorteeralgoritme]]
[[ja:Category:ソート]]
[[no:Kategori:Sorteringsalgoritmer]]
[[pl:Kategoria:Algorytmy sortowania]]
[[pt:Categoria:Algoritmos de ordenação]]
[[ro:Categorie:Algoritmi de sortare]]
[[ru:Категория:Алгоритмы сортировки]]
[[sk:Kategória:Triediacie algoritmy]]
[[sl:Kategorija:Algoritmi za urejanje podatkov]]
[[sr:Категорија:Алгоритми сортирања]]
[[fi:Luokka:Lajittelualgoritmit]]
[[sv:Kategori:Sorteringsalgoritmer]]
[[tr:Kategori:Sıralama algoritmaları]]
[[uk:Категорія:Алгоритми сортування]]
[[vi:Thể loại:Thuật toán sắp xếp]]
[[zh:Category:排序算法]]</text>
    </revision>
  </page>
  <page>
    <title>Heuristic algorithm</title>
    <id>846509</id>
    <revision>
      <id>325927255</id>
      <timestamp>2009-11-15T05:23:13Z</timestamp>
      <contributor>
        <ip>131.215.172.187</ip>
      </contributor>
      <comment>[[WP:UNDO|Undid]] revision 325678941 by [[Special:Contributions/150.135.210.24|150.135.210.24]] ([[User talk:150.135.210.24|talk]])</comment>
      <text xml:space="preserve">In [[computer science]], a '''heuristic algorithm''', or simply a '''heuristic''', is an [[algorithm]] that is able to produce an acceptable solution to a problem in many practical scenarios, in the fashion of a general [[heuristic]], but for which there is no formal proof of its correctness. Alternatively, it may be correct, but may not be proven to produce an optimal solution, or to use reasonable resources. Heuristics are typically used when there is no known method to find an optimal solution, under the given constraints (of [[Computational complexity theory|time, space]] ''etc.'') or at all.

Two fundamental goals in computer science are finding [[algorithm]]s with [[mathematical proof|provably]] &lt;!-- ***NOTE***: this REALLY means &quot;provably&quot;. DO NOT change this to &quot;probably&quot;. --&gt; good [[Run time (computing)|run time]]s and with provably good or &lt;!--optimally--&gt;[[Optimization (computer science)|optimal]] [[solution]] quality. A heuristic is an algorithm that abandons one or both of these goals; for example, it usually finds pretty good solutions, but there is no proof the solutions could not get arbitrarily bad; or it usually runs reasonably quickly, but there is no argument that this will always be the case.

For instance, say you are [[Knapsack problem|packing odd-shaped items]] into a box. Finding a perfect solution is a hard problem: there is no known way to do it that is significantly faster than trying every possible way of packing them. What most people do, then, is &quot;put the largest items in first, then fit the smaller items into the spaces left around them.&quot; This will not necessarily be perfect packing, but it will usually give a packing that is pretty good. It is an example of a heuristic solution.

Several heuristic methods are used by [[antivirus software]] to detect viruses and other malware.

[[Judea Pearl]] states that heuristic methods are based upon intelligent search strategies for computer problem solving, using several alternative approaches&lt;ref&gt;{{cite book
|first=Judea
|last=Pearl
|authorlink=Judea Pearl
|publisher=Addison-Wesley Pub 
|date=April 1984
|title=Heuristics
|isbn=0201055945
}}&lt;/ref&gt;.

Often, one can find specially crafted problem instances where the heuristic will in fact produce very bad results or run very slowly; however, such [[pathological (mathematics)|pathological]] instances might never occur in practice because of their special structure. Therefore, the use of heuristics is very common in real world implementations. For many practical problems, a heuristic algorithm may be the only way to get good solutions in a reasonable amount of time.
There is a class of general heuristic strategies called [[metaheuristic]]s, which often use randomized search for example. They can be applied to a wide range of problems, but good performance is never guaranteed.

==See also==
*[[Heuristic function]]
*[[Artificial Intelligence]]
*[[Expert System]]
*[[Logic Programming]]
*[[Heuristic routing]]

==References==
&lt;references/&gt;

* S. E. Goodman, S. T.  Hedetniemi, ''Introduction to the Design and Analysis of Algorithms'', McGraw-Hill, 1977.
* [[Alfred V. Aho]], [[John E. Hopcroft]], [[Jeffrey D. Ullman]], ''Data Structures and Algorithms'', Addison-Wesley.


[[Category:Algorithms]]
[[Category:Heuristics]]

[[ar:خوارزمية الكشف عن مجريات الأمور]]
[[cs:Heuristické algoritmy]]
[[de:Heuristik#Informatik]]
[[es:Heurística (informática)]]
[[it:Algoritmo euristico]]
[[pt:Heurística (computação)]]
[[ru:Эвристический алгоритм]]
[[sr:Хеуристички алгоритам]]
[[th:ฮิวริสติก (วิทยาการคอมพิวเตอร์)]]
[[tr:buluşsal algoritma (Bilgisayar Bilimleri)]]
[[zh:启发式搜索]]</text>
    </revision>
  </page>
  <page>
    <title>Lookahead</title>
    <id>565954</id>
    <revision>
      <id>307273719</id>
      <timestamp>2009-08-11T00:53:51Z</timestamp>
      <contributor>
        <username>David-Sarah Hopwood</username>
        <id>164240</id>
      </contributor>
      <comment>Move Lookahead vs. Lazy evaluation section (which is mostly about lazy evaluation and *not* lookahead) to bottom. Link [[LALR parser]]s.</comment>
      <text xml:space="preserve">:''For information about the look-ahead feature of some audio compressors, see [[Dynamic_range_compression#Look-ahead|look-ahead]].''
'''Lookahead''' is a tool in [[algorithms]] for looking ahead a few more input items before making a cost effective decision at one stage of the algorithm.

== Applications ==
=== Lookahead in search problems ===
In [[artificial intelligence]], '''lookahead''' is an important component of [[combinatorial search]] which specifies, roughly, how deeply the [[graph (data structure)|graph]] representing the problem is explored. The need for a specific limit on lookahead comes from the large problem graphs in many applications, such as [[computer chess]] and [[computer Go]]. A naive [[breadth-first search]] of these graphs would quickly consume all the memory of any modern computer. By setting a specific lookahead limit, the algorithm's time can be carefully controlled; its time [[exponential growth|increases exponentially]] as the lookahead limit increases.

More sophisticated search techniques such as [[alpha-beta pruning]] are able to eliminate entire subtrees of the search tree from consideration. When these techniques are used, lookahead is not a precisely defined quantity, but instead either the maximum depth searched or some type of average.

=== Lookahead in parsing ===

'''Lookahead''' is also an important concept in [[parser]]s in [[compiler]]s which establishes the maximum number of incoming input tokens the parser can look at to decide which rule it should use. 

Lookahead is especially relevant to [[LL parser|LL]], [[LR parser|LR]], and [[LALR parser]]s, where it is often explicitly indicated by affixing the lookahead to the algorithm name in parentheses, such as LALR(1).

Most [[programming language]]s, the primary target of parsers, are carefully defined in such a way that a parser with limited lookahead, typically one, can parse them, because parsers with limited lookahead are often more efficient. One important change{{Fact|date=December 2008}} to this trend came in 1990 when [[Terence Parr]] created [[ANTLR]] for his Ph.D. thesis, a [[parser generator]] for efficient LL(''k'') parsers, where ''k'' is any fixed value.

Parsers typically have only a few actions after seeing each token. They are shift (add this token to the stack for later reduction), reduce (pop tokens from the stack and form a syntactic construct), end, error (no known rule applies) or conflict (does not know whether to shift or reduce). 

Lookahead has two advantages. 
* It helps the parser take the correct action in case of conflicts. For example, parsing the if statement in the case of an else clause.
* It eliminates many duplicate states and eases the burden of an extra stack. A C language non-lookahead parser will have around 10,000 states. A lookahead parser will have around 300 states.

Example: Parsing the Expression  1 + 2 * 3

  Set of expression parsing rules (called grammar) is as follows, &lt;br&gt;
  Rule1:  E → E + E      Expression is the sum of two expressions.
  Rule2:  E → E * E      Expression is the product of two expressions.
  Rule3:  E → number     Expression is a simple number
  Rule4:  + has less precedence than *

Most programming languages (except for a few such as APL and Smalltalk) and algebraic formulas give higher precedence to multiplication than addition, in which case the correct interpretation of the example above is (1 + (2*3)).
Note that Rule4 above is a semantic rule. It is possible to rewrite the grammar to incorporate this into the syntax. However, not all such rules can be translated into syntax.

'''Simple non-lookahead parser actions'''
# Reduces 1 to expression E on input 1 based on rule3.
# Shift + onto stack on input 1 in anticipation of rule1.
# Reduce stack element 2 to Expression E based on rule3.
# Reduce stack items E+ and new input E to E based on rule1.
# Shift * onto stack on input * in anticipation of rule2.
# Shift 3 onto stack on input 3 in anticipation of rule3.
# Reduce 3 to Expression E on input 3 based on rule3.
# Reduce stack items E* and new input E to E based on rule2.

The parse tree and resulting code from it is not correct according to language semantics. 

To correctly parse without lookahead, there are three solutions:
* The user has to enclose expressions within parentheses. This often is not a viable solution.
* The parser needs to have more logic to backtrack and retry whenever a rule is violated or not complete. The similar method is followed in LL parsers. 
* Alternatively, the parser or grammar needs to have extra logic to delay reduction and reduce only when it is absolutely sure which rule to reduce first. This method is used in LR parsers. This correctly parses the expression but with many more states and increased stack depth.

'''Lookahead parser actions'''
# Shift 1 onto stack on input 1 in anticipation of rule3. It does not reduce immediately.
# Reduce stack item 1 to simple Expression on input + based on rule3. The lookahead is +, so we are on path to E +, so we can reduce the stack to E.
# Shift + onto stack on input + in anticipation of rule1.
# Shift 2 onto stack on input 2 in anticipation of rule3.
# Reduce stack item 2 to Expression on input * based on rule3. The lookahead * expects only E before it.
# Now stack has E + E and still the input is *. It has two choices now, either to shift based on rule2 or reduction based on rule1. Since * has more precedence than + based on rule4, so shift * onto stack in anticipation of rule2.
# Shift 3 onto stack on input 3 in anticipation of rule3.
# Reduce stack item 3 to Expression after seeing end of input based on rule3.
# Reduce stack items E * E to E based on rule2.
# Reduce stack items E + E to E based on rule1.
The parse tree generated is correct and simply more efficient than non-lookahead parsers. This is the strategy followed in [[LALR parser]]s.

== Lookahead vs. Lazy evaluation ==
This is in contrast to another technique called [[lazy evaluation]] that delays the computation until it is really needed. Both techniques are used for economical usage of space or time. Lookahead makes the right decision and so avoids backtracking from undesirable stages at later stages of algorithm and so saves space, at the cost of a slight increase of time due to the overhead of extra lookups. Lazy evaluation normally skips the unexplored algorithmic paths and thus saves both the time and space in general. Some applications of lazy evaluations are demand paging in [[operating systems]] and lazy parse tables in compilers.

In search space exploration, both the techniques are used. When there are already promising paths in the algorithm to evaluate, lazy evaluation is used and to be explored paths will be saved in the queue or stack. When there are no promising paths to evaluate and to check whether the new path can be a more promising path in leading to solution, lookahead is used.  

Compilers also use both the techniques. They will be lazy in generating parse tables from given rules, but they lookahead in parsing given input.

== See also ==
* [[Regular Expression]]

[[Category:Algorithms]]
[[Category:Compiler theory]]
[[Category:Articles lacking sources (Erik9bot)]]

[[de:Lookahead]]
[[ja:先読み]]
[[pl:Podgląd]]
[[sr:Preduvid]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Constraint satisfaction</title>
    <id>4039275</id>
    <revision>
      <id>280036237</id>
      <timestamp>2009-03-27T16:47:09Z</timestamp>
      <contributor>
        <username>Seb35</username>
        <id>304248</id>
      </contributor>
      <minor/>
      <comment>+fr</comment>
      <text xml:space="preserve">{{catmore|Constraint satisfaction}}
{{Commons cat|Constraint satisfaction}}

[[Category:Algorithms]]
[[Category:Artificial intelligence]]
[[Category:Operations research]]

[[fr:Catégorie:Satisfaction de contraintes]]</text>
    </revision>
  </page>
  <page>
    <title>List of algorithm general topics</title>
    <id>632487</id>
    <revision>
      <id>301664582</id>
      <timestamp>2009-07-12T08:35:23Z</timestamp>
      <contributor>
        <username>Kwamikagami</username>
        <id>93143</id>
      </contributor>
      <minor/>
      <comment>del. blank IPA template using [[Project:AutoWikiBrowser|AWB]]</comment>
      <text xml:space="preserve">This is a '''list of [[algorithm]] general topics''', by Wikipedia page. 

* [[Analysis of algorithms]]
* [[Ant colony algorithm]]
* [[Approximation algorithm]]
* [[Best and worst cases]]
* [[Big O notation]]
* [[Combinatorial search]]
* [[Competitive analysis]]
* [[Computability theory]]
* [[Computational complexity theory]]
* [[Embarrassingly parallel|Embarrassingly parallel problem]]
* [[Emergent algorithm]]
* [[Evolutionary algorithm]]
* [[Fast Fourier transform]]
* [[Genetic algorithm]]
* [[Graph exploration algorithm]]
* [[Heuristic]]
* [[Hill climbing]]
* [[Implementation]]
* [[Las Vegas algorithm]]
* [[Lock-free and wait-free algorithms]]
* [[Monte Carlo algorithm]]
* [[Numerical analysis]]
* [[Online algorithm]]
* [[Polynomial time approximation scheme]]
* [[Problem size]]
* [[Pseudorandom number generator]]
* [[Quantum algorithm]]
* [[Random-restart hill climbing]]
* [[Randomized algorithm]]
* [[Running time]]
* [[Sorting algorithm]]
* [[Search algorithm]]
* [[Stable algorithm]]
* [[Super-recursive algorithm]]
* [[Tree search algorithm]]

See also:

* [[list of algorithms]] for specific algorithms
* [[list of computability and complexity topics]] for more abstract theory
* [[list of complexity classes]], [[complexity class]]
* [[list of data structures]].

{{DEFAULTSORT:List Of Algorithm General Topics}}
[[Category:Mathematics-related lists|Algorithm general]]
[[Category:Algorithms|*]]

[[ru:Список основных разделов теории алгоритмов]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Data clustering algorithms</title>
    <id>15325165</id>
    <revision>
      <id>285619165</id>
      <timestamp>2009-04-23T09:01:53Z</timestamp>
      <contributor>
        <username>3mta3</username>
        <id>109644</id>
      </contributor>
      <text xml:space="preserve">This category contains algorithms used for [[cluster analysis]].

[[Category:Algorithms]]
[[Category:Cluster analysis]]</text>
    </revision>
  </page>
  <page>
    <title>Biologically inspired algorithms</title>
    <id>15190073</id>
    <revision>
      <id>250201657</id>
      <timestamp>2008-11-07T09:20:55Z</timestamp>
      <contributor>
        <username>Gergie</username>
        <id>2311742</id>
      </contributor>
      <comment>added EA, PSO and ant colony</comment>
      <text xml:space="preserve">{{Unreferenced|date=January 2008}}
{{expand|date=January 2008}}
'''Biologically inspired algorithms''' is a category of algorithms that imitate the way nature performs. This category has been quite popular, since numerous problems can be solved without rigorous mathematical approaches.{{Fact|date=January 2008}}

In this category of algorithms fall:
*[[Artificial neural network]]s
*[[Genetic algorithm]]s
*[[Evolutionary algorithm]]s
*[[Particle swarm optimization]]
*[[Ant colony optimization]]
*[[Fuzzy logic]]
*and others

[[Category:Algorithms]]
[[Category:Intelligence]]
[[Category:Optimization algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>List of terms relating to algorithms and data structures</title>
    <id>723738</id>
    <revision>
      <id>323725591</id>
      <timestamp>2009-11-03T17:35:59Z</timestamp>
      <contributor>
        <username>ShelfSkewed</username>
        <id>1152308</id>
      </contributor>
      <minor/>
      <comment>Disambiguate [[Edge]] to [[Glossary of graph theory#Basics]]</comment>
      <text xml:space="preserve">The [http://www.nist.gov/dads/ NIST Dictionary of Algorithms and Data Structures] is a reference work maintained by the U.S. [[National Institute of Standards and Technology]]. 
It defines a large number of '''terms relating to algorithms and data structures'''. For algorithms and data structures not necessarily mentioned here, see [[list of algorithms]] and [[list of data structures]].

This list of terms was originally derived from the index of this document, and should be in the public domain, as it was compiled by a Federal Government employee as part of a Federal Government work.
Some of the terms defined are:

__NOTOC__
{{compactTOC}}

== A ==
* [[absolute performance guarantee]]
* [[abstract data type]]
* [[(a,b)-tree]]
* [[accepting state]]
* [[Ackermann's function]]
* [[active data structure]]
* [[acyclic directed graph]]
* [[acyclic graph]]
* [[adaptive heap sort]]
* [[adaptive Huffman coding]]
* [[adaptive k-d tree]]
* [[adaptive sort]]
* [[address-calculation sort]]
* [[adjacency-list representation]]
* [[adjacency-matrix representation]]
* [[adjacent]]
* [[abstract data type|ADT]]
* [[adversary]]
* [[algorithm]]
* [[algorithm BSTW]]
* [[algorithm FGK]]
* [[algorithmic efficiency]]
* [[algorithmically solvable]]
* [[algorithm V]]
* [[all pairs shortest path]]
* [[alphabet (computer science)|alphabet]]
* [[Alpha Skip Search algorithm]]
* [[alternating path]]
* [[alternating Turing machine]]
* [[alternation]]
* [[American flag sort]]
* [[amortized cost]]
* [[ancestor]]
* [[logical conjunction|and]]
* [[American National Standards Institute|ANSI]]
* [[antichain]]
* [[antisymmetric relation]]
* [[Arithmetic progression|AP]]
* [[Apostolico-Crochemore]]
* [[Apostolico-Giancarlo algorithm]]
* [[approximate string matching]]
* [[approximation algorithm]]
* [[arborescence (graph theory)|arborescence]]
* [[Arc (programming language)|arc]]
* [[arithmetic coding]]
* [[Array data structure|array]]
* [[array index]]
* [[array merging]]
* [[array search]]
* [[articulation point]]
* [[assignment problem]]
* [[association list]]
* [[associative]]
* [[associative array]]
* [[asymptotically tight bound]]
* [[asymptotic bound]]
* [[asymptotic lower bound]]
* [[asymptotic space complexity]]
* [[asymptotic time complexity]]
* [[asymptotic upper bound]]
* [[augmenting path]]
* [[Automata theory|automaton]]
* [[average case]]
* [[average-case cost]]
* [[AVL tree]]
* [[axiomatic semantics]]

== B ==
* [[backtracking]]
* [[Multiset|bag]]
* [[balance]]
* [[balanced binary search tree]]
* [[balanced binary tree]]
* [[balanced k-way merge sort]]
* [[balanced merge sort]]
* [[balanced multiway merge]]
* [[balanced multiway tree]]
* [[balanced quicksort]]
* [[Self-balancing binary search tree|balanced tree]]
* [[balanced two-way merge sort]]
* [[BANG file]]
* [[Batcher sort]]
* [[Baum Welch algorithm]]
* [[BB alpha tree|BB α tree]]
* [[Binary decision diagram|BDD]]
* [[BD-tree]]
* [[Bellman-Ford algorithm]]
* [[Benford's law]]
* [[best case]]
* [[best-case cost]]
* [[best-first search]]
* [[biconnected component]]
* [[biconnected graph]]
* [[bidirectional bubble sort]]
* [[big-O notation]]
* [[binary function]]
* [[binary GCD algorithm]]
* [[binary heap]]
* [[binary insertion sort]]
* [[binary knapsack problem]]
* [[binary priority queue]]
* [[binary relation]]
* [[binary search]]
* [[binary search tree]]
* [[binary tree]]
* [[binary tree representation of trees]]
* [[bingo sort]]
* [[binomial heap]]
* [[binomial tree]]
* [[bin packing problem]]
* [[bin sort]]
* [[binary tree|bintree]]
* [[bipartite graph]]
* [[bipartite matching]]
* [[bisector]]
* [[bitonic sort]]
* [[bit vector]]
* [[Bk tree]]
* [[block]]
* [[block addressing index]]
* [[blocking flow]]
* [[block search]]
* [[Bloom filter]]
* [[Blossom (mathematics)|blossom]]
* [[bogosort]]
* [[boogol]]
* [[boolean datatype|boolean]]
* [[boolean expression]]
* [[boolean function]]
* [[bottleneck traveling salesman]]
* [[bottom-up tree automaton]]
* [[boundary-based representation]]
* [[bounded error probability in polynomial time]]
* [[bounded queue]]
* [[bounded stack]]
* [[Boyer-Moore]]
* [[Boyer-Moore-Horspool]]
* [[bozo sort]]
* [[B+ tree]]
* [[BPP]]
* [[Bradford's law]]
* [[Branch (computer science)|branch]] (as in control flow)
* [[Branching (software)|branch]] (as in revision control)
* [[branch and bound]]
* [[breadth-first search]]
* [[Bresenham's algorithm]]
* [[brick sort]]
* [[bridge (graph theory)|bridge]]
* [[British Museum algorithm]]
* [[brute force attack]]
* [[brute force search]]
* [[brute force string search]]
* [[brute force string search with mismatches]]
* [[BSP-tree]]
* [[B*-tree]]
* [[B-tree]]
* [[bubble sort]]
* [[bucket (computing)|bucket]]
* [[bucket array]]
* [[bucketing method]]
* [[bucket sort]]
* [[bucket trie]]
* [[Buddy memory allocation|buddy system]]
* [[buddy tree]]
* [[build-heap]]
* [[Burrows-Wheeler transform]]
* [[busy beaver]]
* [[BV-tree]]
* [[Burrows-Wheeler transform|BWT]]
* [[Byzantine generals]]

== C ==
* [[cactus stack]]
* [[Calculus of Communicating Systems]]
* [[calendar queue]]
* [[candidate consistency testing]]
* [[candidate verification]]
* [[canonical complexity class]]
* [[capacitated facility location]]
* [[capacity]]
* [[capacity constraint]]
* [[cartesian tree]]
* [[cascade merge sort]]
* [[caverphone]]
* [[Cayley-Purser]]
* [[Calculus of communicating systems|CCS]]
* [[C curve]]
* [[cell probe model]]
* [[cell tree]]
* [[cellular automaton]]
* [[centroid]]
* [[Public key certificate|certificate]]
* [[Chain (disambiguation)|chain]]
* [[chaining (algorithm)]]
* [[child node|child]]
* [[Chinese postman problem]]
* [[Chinese remainder theorem]]
* [[Christofides algorithm]]
* [[Christofides heuristic]]
* [[chromatic index]]
* [[chromatic number]]
* [[Church-Turing thesis]]
* [[digital circuit|circuit]]
* [[circuit complexity]]
* [[circuit value problem]]
* [[circular list]]
* [[circular queue]]
* [[Clique (graph theory)|clique]]
* [[clique problem]]
* [[clustering]]
* [[clustering free]]
* [[coalesced hashing]]
* [[coarsening]]
* [[cocktail shaker sort]]
* [[codeword]]
* [[coding tree]]
* [[collective recursion]]
* [[Hash collision|collision]]
* [[collision resolution scheme]]
* [[Colussi]]
* [[combination]]
* [[comb sort]]
* [[Communicating Sequential Processes]]
* [[commutative]]
* [[compact DAWG]]
* [[compact trie]]
* [[comparison sort]]
* [[competitive analysis]]
* [[competitive ratio]]
* [[complement]]
* [[complete binary tree]]
* [[complete graph]]
* [[completely connected graph]]
* [[complete tree]]
* [[complexity]]
* [[complexity class]]
* [[computable]]
* [[concave function]]
* [[concurrent flow]]
* [[concurrent read, concurrent write]]
* [[concurrent read, exclusive write]]
* [[computer configuration|configuration]]
* [[confluently persistent data structure]]
* [[Logical conjunction|conjunction]]
* [[connected component (graph theory)|connected components]]
* [[connected graph]]
* [[co-NP]]
* [[constant function]]
* [[continuous knapsack problem]]
* [[Cook reduction]]
* [[Cook's theorem]]
* [[counting sort]]
* [[covering]]
* [[Cyclic redundancy check|CRC]]
* [[CRCW]]
* [[Crew (algorithm)]]
* [[critical path problem]]
* [[Communicating sequential processes|CSP]] (communicating sequential processes)
* [[Constraint satisfaction problem|CSP]] (constraint satisfaction problem)
* [[Computational tree logic|CTL]]
* [[cuckoo hashing]]
* [[cut (graph theory)]]
* [[cut (logic programming)]]
* [[cutting plane]]
* [[cutting stock problem]]
* [[cutting theorem]]
* [[cut vertex]]
* [[cycle (mathematics)|cycle]]
* [[cyclic redundancy check]]

== D ==
* [[D-adjacent]]
* [[Directed acyclic graph|DAG]]
* [[DAG shortest paths]]
* [[Damerau–Levenshtein distance]]
* [[data structure]]
* [[DAWG]]
* [[decidable]]
* [[decidable language]]
* [[Decimation (signal processing)|decimation]]
* [[decision problem]]
* [[decision tree]]
* [[decomposable searching problem]]
* [[degree (mathematics)|degree]]
* [[dense graph]]
* [[depoissonization]]
* [[depth (disambiguation)|depth]]
* [[depth-first search]]
* [[deque]]
* [[derangement]]
* [[descendant]]
* [[Deterministic algorithm|deterministic]]
* [[deterministic algorithm]]
* [[deterministic finite automata string search]]
* [[deterministic finite automaton]]
* [[deterministic finite state machine]]
* [[deterministic finite tree automaton]]
* [[deterministic pushdown automaton]]
* [[deterministic tree automaton]]
* [[Deutsch-Jozsa algorithm]]
* [[Deterministic finite automaton|DFA]]
* [[Depth-first search|DFS]]
* [[DFS forest]]
* [[DFTA]]
* [[diagonalization]]
* [[diameter]]
* [[dichotomic search]]
* [[dictionary]]
* diet (see ''discrete interval encoding tree'' below)
* [[difference]]
* [[digital search tree]]
* [[digital tree]]
* [[Directed graph|digraph]]
* [[Dijkstra's algorithm]]
* [[diminishing increment sort]]
* [[dining philosophers]]
* [[direct chaining hashing]]
* [[directed acyclic graph]]
* [[directed acyclic word graph]]
* [[directed graph]]
* [[discrete interval encoding tree]]
* [[discrete p-center]]
* [[disjoint set]]
* [[disjunction]]
* [[distributional complexity]]
* [[distribution sort]]
* [[divide and conquer algorithm]]
* [[divide and marriage before conquest]]
* [[division method]]
* [[Data domain]]
* [[don't care]]
* [[Doomsday rule]]
* [[double-direction bubble sort]]
* [[double-ended priority queue]]
* [[double hashing]]
* [[double left rotation]]
* [[Double Metaphone]]
* [[double right rotation]]
* [[doubly-chained tree]]
* [[doubly-ended queue]]
* [[doubly linked list]]
* [[Deterministic pushdown automaton|DPDA]]
* [[Dragon curve]]
* [[dual]]
* [[dual linear program]]
* [[Dutch national flag]]
* [[dyadic tree]]
* [[Dynamic programming|dynamic]]
* [[dynamic array]]
* [[dynamic hashing]]
* [[dynamic programming]]
* [[dynamization transformation]]

== E ==
* [[Glossary of graph theory#Basics|edge]]
* [[edge coloring]]
* [[edge connectivity]]
* [[edge crossing]]
* [[edge-weighted graph]]
* [[edit distance]]
* [[edit operation]]
* [[edit script]]
* [[8 queens]]
* [[elastic-bucket trie]]
* [[element uniqueness]]
* [[end-of-string]]
* [[enfilade]]
* [[ERCW]]
* [[EREW]]
* [[Euclidean algorithm]]
* [[Euclidean distance]]
* [[Euclidean Steiner tree]]
* [[Euclidean traveling salesman problem]]
* [[Euclid's algorithm]]
* [[Euler cycle]]
* [[Eulerian graph]]
* [[Eulerian path]]
* [[exact string matching]]
* [[EXCELL]]
* [[exchange sort]]
* [[exclusive or]]
* [[exclusive read, concurrent write]]
* [[exclusive read, exclusive write]]
* [[exhaustive search]]
* [[existential state]]
* [[expandable hashing]]
* [[expander graph]]
* [[exponential]]
* [[extended binary tree]]
* [[Extended Euclidean algorithm|extended Euclid's algorithm]]
* [[extended k-d tree]]
* [[extendible hashing]]
* [[external index]]
* [[external memory algorithm]]
* [[external memory data structure]]
* [[external merge]]
* [[external merge sort]]
* [[external node]]
* [[external quicksort]]
* [[external radix sort]]
* [[external sort]]
* [[extrapolation search]]
* [[extremal]]
* [[extreme point]]

== F ==
* [[facility location]]
* [[factor]]
* [[factorial]]
* [[fast fourier transform]]
* [[fathoming]]
* [[feasible region]]
* [[feasible solution]]
* [[feedback edge set]]
* [[feedback vertex set]]
* [[Ferguson-Forcade algorithm]]
* [[Fast Fourier Transform|FFT]]
* [[Fibonacci number]]
* [[Fibonacci search]]
* [[Fibonacci tree]]
* [[Fibonacci heap]]
* [[FIFO (computing)FIFO]]
* [[filial-heir chain]]
* [[Find]]
* [[find kth least element]]
* [[finitary tree]]
* [[finite Fourier transform]]
* [[finite state automaton]]
* [[finite state machine]]
* [[finite state machine minimization]]
* [[finite state transducer]]
* [[first child-next sibling binary tree]]
* [[first come, first served]]
* [[first-in, first-out]]
* [[fixed-grid method]]
* [[flash sort]]
* [[flow]]
* [[flow conservation]]
* [[flow function]]
* [[flow network]]
* [[Floyd-Warshall algorithm]]
* [[Ford-Bellman]]
* [[Ford-Fulkerson method]]
* [[forest]]
* [[forest editing problem]]
* [[formal language]]
* [[formal methods]]
* [[formal verification]]
* [[forward index]]
* [[fractal]]
* [[fractional knapsack problem]]
* [[fractional solution]]
* [[free edge]]
* [[free list]]
* [[free tree]]
* [[free vertex]]
* [[frequency count heuristic]]
* [[full array]]
* [[full binary tree]]
* [[full inverted index]]
* [[fully dynamic graph problem]]
* [[fully persistent data structure]]
* [[fully polynomial approximation scheme]]
* [[function (programming)]]
* [[function (mathematics)]]
* [[functional data structure]]

== G ==
* [[Galil-Giancarlo]]
* [[Galil-Seiferas]]
* [[gamma function]]
* [[GBD-tree]]
* [[greatest common divisor|GCD]]
* [[geometric optimization problem]]
* [[global optimum]]
* [[gnome sort]]
* [[goobi]]
* [[Graph (data structure)|graph]]
* [[graph coloring]]
* [[graph concentration]]
* [[graph drawing]]
* [[graph isomorphism]]
* [[graph partition]]
* [[Gray code]]
* [[greatest common divisor]]
* [[greedy algorithm]]
* [[greedy heuristic]]
* [[grid drawing]]
* [[grid file]]
* [[Grover's algorithm]]

== H ==
* [[halting problem]]
* [[Hamiltonian cycle]]
* [[Hamiltonian path]]
* [[Hamming distance]]
* [[hash function]]
* [[hash heap]]
* [[hash table]]
* [[hash table delete]]
* [[Hausdorff distance]]
* [[hB-tree]]
* head
* [[heap (data structure)|heap]]
* [[heapify]]
* [[heap property]]
* [[heapsort]]
* [[heaviest common subsequence]]
* [[height]]
* [[height-balanced binary search tree]]
* [[height-balanced tree]]
* [[Herter-Heighway Dragon]]
* [[heuristic]]
* [[hidden Markov model]]
* [[highest common factor]]
* [[Hilbert curve]]
* [[histogram sort]]
* [[homeomorphic]]
* [[horizontal visibility map]]
* [[Horner's rule]]
* [[Horspool]]
* [[Huffman encoding]]
* [[Hungarian algorithm]]
* [[hybrid algorithm]]
* [[hyperedge]]
* [[hypergraph]]

== I ==
* [[ID]]
* [[ideal merge]]
* [[implication]]
* [[implies]]
* [[in-branching]]
* [[inclusion-exclusion principle]]
* [[inclusive or]]
* [[incompressible string]]
* [[incremental algorithm]]
* [[in-degree]]
* [[independent set]]
* [[index file]]
* [[information theoretic bound]]
* [[in-order traversal]]
* [[in-place sort]]
* [[insertion sort]]
* [[instantaneous description]]
* [[integer linear program]]
* [[integer multi-commodity flow]]
* [[integer polyhedron]]
* [[interactive proof system]]
* [[interior-based representation]]
* [[internal node]]
* [[internal sort]]
* [[interpolation search]]
* [[interpolation-sequential search]]
* [[interpolation sort]]
* [[intersection]]
* [[interval tree]]
* [[intractable]]
* [[introsort]]
* [[introspective sort]]
* [[inverse Ackermann function]]
* [[inverted file index]]
* [[inverted index]]
* [[irreflexive]]
* [[isomorphic]]
* [[iteration]]

== J ==
* [[Jaro-Winkler]]
* [[Johnson's algorithm]]
* [[Johnson&amp;ndash;Trotter]]
* [[J sort]]
* [[JSort]]
* [[jump list]]
* [[jump search]]

== K ==
* [[Karmarkar's algorithm]]
* [[Karnaugh map]]
* [[Karp-Rabin]]
* [[Karp reduction]]
* [[k-ary heap]]
* [[k-ary Huffman encoding]]
* [[k-ary tree]]
* [[k-clustering]]
* [[k-coloring]]
* [[k-connected graph]]
* [[k-d-B-tree]]
* [[k-dimensional]]
* [[K-dominant match]]
* [[k-d tree]]
* [[Computer keyboard keys#Keys on a computer keyboard|key]]
* [[Internet Security Association and Key Management Protocol|KMP]]
* [[KmpSkip Search]]
* [[knapsack problem]]
* [[knight's tour]]
* [[Knuth-Morris-Pratt algorithm]]
* [[Königsberg bridges problem]]
* [[Kolmogorov complexity]]
* [[Kraft's inequality]]
* [[Kripke structure]]
* [[Kruskal's algorithm]]
* [[kth order Fibonacci numbers]]
* [[kth shortest path]]
* [[kth smallest element]]
* [[KV diagram]]
* [[k-way merge]]
* [[k-way merge sort]]
* [[k-way tree]]

== L ==
* [[labeled graph]]
* [[language]]
* [[LIFO (computing)|last-in, first-out]]
* [[Las Vegas algorithm]]
* [[lattice]]
* [[layered graph]]
* [[least common multiple|LCM]]
* [[Laboratory for Computer Science|LCS]]
* [[leaf]]
* [[least common multiple]]
* [[leftist tree]]
* [[left rotation]]
* [[Lempel-Ziv-Welch]]
* [[level-order traversal]]
* [[Levenshtein distance]]
* [[lexicographical order]]
* [[LIFO (computing)|LIFO]]
* [[linear]]
* [[linear congruential generator]]
* [[linear hash]]
* [[linear insertion sort]]
* [[linear order]]
* [[linear probing]]
* [[linear probing sort]]
* [[linear product]]
* [[linear program]]
* [[linear quadtree]]
* [[linear search]]
* [[link]]
* [[linked list]]
* [[List (computing)|list]]
* [[list contraction]]
* [[little-o notation]]
* [[Lm distance]]
* [[load factor]]
* [[local alignment]]
* [[local optimum]]
* [[logarithm]], [[logarithmic scale]]
* [[longest common subsequence]]
* [[longest common substring]]
* [[Lotka's law]]
* [[lower bound]]
* [[lower triangular matrix]]
* [[lowest common ancestor]]
* [[l-reduction]]
* [[lucky sort]]
* [[LZW (algorithm)|LZW]]

== M ==
* [[Malhotra-Kumar-Maheshwari blocking flow]]
* [[Manhattan distance]]
* [[many-one reduction]]
* [[Markov chain]]
* Marlena
* [[marriage problem]]
* [[Master theorem]]
* [[matched edge]]
* [[matched vertex]]
* [[matching]]
* [[matrix (math)|matrix]]
* [[matrix-chain multiplication problem]]
* [[max-heap property]]
* [[maximal independent set]]
* [[maximally connected component]]
* [[Maximal Shift]]
* [[maximum bipartite matching]]
* [[maximum-flow problem]]
* [[MAX-SNP]]
* [[Minimum bounding box|MBB]]
* [[Mealy machine]]
* [[mean]]
* [[median]]
* [[meld]]
* [[memoization]]
* [[merge]]
* [[merge sort]]
* [[meromorphic function]]
* [[metaheuristic]]
* [[metaphone]]
* [[midrange]]
* [[Miller-Rabin]]
* [[min-heap property]]
* [[minimal perfect hashing]]
* [[minimum bounding box]]
* [[minimum cut]]
* [[minimum path cover]]
* [[minimum spanning tree]]
* [[minimum vertex cut]]
* [[mixed integer linear program]]
* [[Mode (statistics)|mode]]
* [[model checking]]
* [[model of computation]]
* [[moderately exponential]]
* [[MODIFIND]]
* [[monotone priority queue]]
* [[monotonically decreasing]]
* [[monotonically increasing]]
* [[Monte Carlo algorithm]]
* [[Moore machine]]
* [[Morris-Pratt]]
* [[move]]
* [[move-to-front heuristic]]
* [[move-to-root heuristic]]
* [[Mean square for treatments|MST]]
* [[multi-commodity flow]]
* [[multigraph]]
* [[multilayer grid file]]
* [[multiplication method]]
* [[multiprefix]]
* [[multiprocessor model]]
* [[multiset]]
* [[multi suffix tree]]
* [[multiway decision]]
* [[multiway merge]]
* [[multiway search tree]]
* [[multiway tree]]
* [[Munkres' assignment algorithm]]

== N ==
* [[naive string search]]
* [[nand]]
* [[n-ary function]]
* [[NC (complexity)|NC]]
* [[NC many-one reducibility]]
* [[nearest neighbor]]
* [[negation]]
* [[network flow]]
* [[network flow problem]]
* [[next state]]
* [[Nondeterministic finite state machine|NFA]]
* [[Nondeterministic finite tree automaton|NFTA]]
* [[NIST]]
* [[node (computer science)|node]]
* [[nonbalanced merge]]
* [[nonbalanced merge sort]]
* [[nondeterministic]]
* [[nondeterministic algorithm]]
* [[nondeterministic finite automaton]]
* [[nondeterministic finite state machine]]
* [[nondeterministic finite tree automaton]]
* [[nondeterministic polynomial time]]
* [[nondeterministic tree automaton]]
* [[nondeterministic Turing machine]]
* [[nonterminal node]]
* [[nor]]
* [[negation|not]]
* [[Not So Naive]]
* [[NP (complexity)|NP]]
* [[NP-complete]]
* [[NP-complete language]]
* [[NP-hard]]
* [[n queens]]
* [[nullary function]]
* [[null tree]]
* [[New York State Identification and Intelligence System|NYSIIS]]

== O ==
* [[OBDD]]
* [[objective function]]
* [[occurrence]]
* [[octree]]
* [[offline algorithm]]
* [[offset]]
* [[omega]]
* [[omicron]]
* [[one-based indexing]]
* [[one-dimensional]]
* [[online algorithm]]
* [[open addressing]]
* [[Optimization (mathematics)|optimal]]
* [[optimal cost]]
* [[optimal hashing]]
* [[optimal merge]]
* [[optimal mismatch]]
* [[optimal polygon triangulation problem]]
* [[optimal polyphase merge]]
* [[optimal polyphase merge sort]]
* [[optimal solution]]
* [[optimal triangulation problem]]
* [[optimal value]]
* [[Optimization (mathematics)|optimization problem]]
* [[logical disjunction|or]]
* [[oracle set]]
* [[oracle tape]]
* [[oracle Turing machine]]
* [[Orders of approximation]]
* [[ordered array]]
* [[ordered binary decision diagram]]
* [[ordered linked list]]
* [[ordered tree]]
* [[order preserving hash]]
* [[order preserving minimal perfect hashing]]
* [[oriented acyclic graph]]
* [[oriented graph]]
* [[oriented tree]]
* [[orthogonal drawing]]
* [[orthogonal lists]]
* [[orthogonally convex rectilinear polygon]]
* [[oscillating merge sort]]
* [[out-branching]]
* [[out-degree]]
* [[overlapping subproblems]]

== P ==
* [[packing]]
* [[padding argument]]
* [[pagoda]]
* [[pairing heap]]
* [[PAM]]
* [[parallel computation thesis]]
* [[parallel prefix computation]]
* [[parallel random-access machine]]
* [[parametric searching]]
* [[parent]]
* [[partial function]]
* [[partially decidable problem]]
* [[partially dynamic graph problem]]
* [[partially ordered set]]
* [[partially persistent data structure]]
* [[partial order]]
* [[partial recursive function]]
* [[partition]]
* [[passive data structure]]
* [[patience sorting]]
* [[path]]
* [[path cover]]
* [[path system problem]]
* [[Patricia tree]]
* [[pattern]]
* [[pattern element]]
* [[P-complete]]
* [[Phencyclidine|PCP]]
* [[Pushdown automaton|PDA]]
* [[Peano curve]]
* [[Pearson's hash]]
* [[perfect binary tree]]
* [[perfect hashing]]
* [[perfect k-ary tree]]
* [[perfect matching]]
* [[perfect shuffle]]
* [[performance guarantee]]
* [[performance ratio]]
* [[permutation]]
* [[persistent data structure]]
* [[phonetic coding]]
* [[pile (data structure)]]
* [[pipelined divide and conquer]]
* [[planar graph]]
* [[planarization]]
* [[planar straight-line graph]]
* [[PLOP-hashing]]
* [[point access method]]
* [[pointer jumping]]
* [[pointer machine]]
* [[poissonization]]
* [[polychotomy]]
* [[polyhedron]]
* [[polylogarithmic]]
* [[polynomial]]
* [[polynomial approximation scheme]]
* [[polynomial hierarchy]]
* [[polynomial time]]
* [[polynomial-time Church-Turing thesis]]
* [[polynomial-time reduction]]
* [[polyphase merge]]
* [[polyphase merge sort]]
* [[polytope]]
* [[poset]]
* [[postfix traversal]]
* [[Post machine]]
* [[postman's sort]]
* [[postorder traversal]]
* [[Post's correspondence problem]]
* [[potential function]]
* [[Parallel Random Access Machine|PRAM]]
* [[Predicate (computer programming)|predicate]]
* [[Prefix (computer science)|prefix]]
* [[prefix code]]
* [[prefix computation]]
* [[prefix sum]]
* [[prefix traversal]]
* [[preorder traversal]]
* [[primary clustering]]
* [[primitive recursive]]
* [[Prim's algorithm]]
* [[principle of optimality]]
* [[priority queue]]
* [[prisoner's dilemma]]
* [[PRNG]]
* [[probabilistic algorithm]]
* [[probabilistically checkable proof]]
* [[probabilistic Turing machine]]
* [[probe sequence]]
* [[procedure]]
* [[process algebra]]
* [[proper]]
* [[proper binary tree]]
* [[proper coloring]]
* [[proper subset]]
* [[property list]]
* [[prune and search]]
* [[pseudo-random number generator]]
* [[Polynomial-time approximation scheme|PTAS]]
* [[pth order Fibonacci numbers]]
* [[P-tree]]
* [[purely functional language]]
* [[pushdown automaton]]
* [[pushdown transducer]]
* [[p-way merge sort]]

== Q ==
* [[qm sort]]
* [[q sort]]
* [[quadratic probing]]
* [[quadtree]]
* [[quadtree complexity theorem]]
* [[quad trie]]
* [[quantum computation]]
* [[Queue (data structure)|queue]]
* [[quick search]]
* [[quicksort]]

== R ==
* [[Rabin-Karp]]
* [[radix quicksort]]
* [[radix sort]]
* [[ragged matrix]]
* [[Raita algorithm]]
* [[random access machine]]
* [[randomization]]
* [[randomized algorithm]]
* [[randomized binary search tree]]
* [[randomized complexity]]
* [[randomized polynomial time]]
* [[randomized rounding]]
* [[randomized search tree]]
* [[Randomized-Select]]
* [[random number generator]]
* [[random sampling]]
* [[range]]
* [[range sort]]
* [[Rank (mathematics)|rank]]
* [[Ratcliff/Obershelp pattern recognition]]
* [[reachable]]
* [[rebalance]]
* [[recognizer]]
* [[rectangular matrix]]
* [[rectilinear]]
* [[rectilinear Steiner tree]]
* [[recurrence equations]]
* [[recurrence relation]]
* [[recursion]]
* [[recursion termination]]
* [[recursion tree]]
* [[recursive]]
* [[recursive data structure]]
* [[recursive doubling]]
* [[recursive language]]
* [[recursively enumerable language]]
* [[recursively solvable]]
* [[red-black tree]]
* [[reduced basis]]
* [[reduced digraph]]
* [[reduced ordered binary decision diagram]]
* [[Reduction (complexity)|reduction]]
* [[reflexive relation]]
* [[regular decomposition]]
* [[rehashing]]
* [[relation]]
* [[relational structure]]
* [[relative performance guarantee]]
* [[Relaxation technique (mathematics)|relaxation]]
* [[relaxed balance]]
* [[rescalable]]
* [[restricted universe sort]]
* [[result cache]]
* [[Reverse Colussi]]
* [[Reverse Factor]]
* [[R-file]]
* [[Rice's method]]
* [[right rotation]]
* [[right-threaded tree]]
* [[RNG]]
* [[ROBDD]]
* [[root]]
* [[root balance]]
* [[rooted tree]]
* [[rotate left]]
* [[rotate right]]
* [[rotation]]
* [[rough graph]]
* [[RP (complexity)|RP]]
* [[R+ tree|R+-tree]]
* [[R* tree|R*-tree]]
* [[R-tree]]
* [[Run time (computing)|run time]]

== S ==
* [[saguaro stack]]
* [[saturated edge]]
* [[SBB tree]]
* [[scan]]
* [[scapegoat tree]]
* [[search]]
* [[search tree]]
* [[search tree property]]
* [[secant search]]
* [[secondary clustering]]
* [[memory segment]]
* [[Select]]
* [[select and partition]]
* [[selection problem]]
* [[selection sort]]
* [[select kth element]]
* [[select mode]]
* [[self-loop]]
* [[self-organizing heuristic]]
* [[self-organizing list]]
* [[self-organizing sequential search]]
* [[semidefinite programming]]
* [[separate chaining hashing]]
* [[separation theorem]]
* [[sequential search]]
* [[Set (computer science)]]
* [[set cover]]
* [[set packing]]
* [[shadow heap]]
* [[shadow merge]]
* [[shadow merge insert]]
* [[shaker sort]]
* [[Shannon-Fano coding]]
* [[shared memory]]
* [[Shell sort]]
* [[Shift-Or]]
* [[Shor's algorithm]]
* [[shortcutting]]
* [[shortest common supersequence]]
* [[shortest common superstring]]
* [[shortest path]]
* [[shortest spanning tree]]
* [[shuffle]]
* [[shuffle sort]]
* [[sibling]]
* [[Sierpinski curve]]
* [[Sierpinski triangle]]
* [[sieve of Eratosthenes]]
* [[sift up]]
* [[signature]]
* [[Simon's algorithm]]
* [[simple merge]]
* [[path (graph theory)|simple path]]
* [[simple uniform hashing]]
* [[simplex communication]]
* [[simulated annealing]]
* [[simulation theorem]]
* [[single-destination shortest-path problem]]
* [[single-pair shortest-path problem]]
* [[single program multiple data]]
* [[single-source shortest-path problem]]
* [[singly linked list]]
* [[singularity analysis]]
* [[sink]]
* [[sinking sort]]
* [[skd-tree]]
* [[skew symmetry]]
* [[skip list]]
* [[skip search]]
* [[slope selection]]
* [[Smith algorithm]]
* [[Smith-Waterman algorithm]]
* [[smoothsort]]
* [[solvable]]
* [[sort]]
* [[sorted array]]
* [[sorted list]]
* [[sort in place]]
* [[sort merge]]
* [[soundex]]
* [[space-constructible function]]
* [[spanning tree (mathematics)|spanning tree]]
* [[sparse graph]]
* [[sparse matrix]]
* [[sparsification]]
* [[sparsity]]
* [[spatial index|spatial access method]]
* [[spectral test]]
* [[splay tree]]
* [[SPMD]]
* [[square matrix]]
* [[square root]]
* [[SST]]
* [[Numerical stability|stable]]
* [[stack]] -- [[stack (data structure)]]
* [[stack tree]]
* [[star-shaped polygon]]
* [[start state]]
* [[state (computer science)|state]]
* [[state machine]]
* [[state transition]]
* [[static]]
* [[static Huffman encoding]]
* [[s-t cut]]
* [[st-digraph]]
* [[Steiner minimum tree]]
* [[Steiner point]]
* [[Steiner ratio]]
* [[Steiner tree]]
* [[Steiner vertex]]
* [[Steinhaus–Johnson–Trotter algorithm]]
* [[Stirling's approximation]]
* [[Stirling's formula]]
* [[stooge sort]]
* [[straight-line drawing]]
* [[strand sort]]
* [[strictly decreasing]]
* [[strictly increasing]]
* [[strictly lower triangular matrix]]
* [[strictly upper triangular matrix]]
* [[string (computer science)|string]]
* [[string editing problem]]
* [[string matching]]
* [[string matching on ordered alphabets]]
* [[string matching with errors]]
* [[string matching with mismatches]]
* [[string searching]]
* [[strip packing]]
* [[strongly connected component]]
* [[strongly connected graph]]
* [[strongly NP-hard]]
* [[subadditive ergodic theorem]]
* [[subgraph]]
* [[subgraph isomorphism]]
* [[sublinear time algorithm]]
* [[subsequence]]
* [[subset]]
* [[substring]]
* [[subtree]]
* [[Suffix (computer science)|suffix]]
* [[suffix array]]
* [[suffix automaton]]
* [[suffix tree]]
* [[superimposed code]]
* [[superset]]
* [[supersink]]
* [[supersource]]
* [[symmetric relation]]
* [[symmetrically linked list]]
* [[symmetric binary B-tree]]
* [[symmetric set difference]]
* [[symmetry breaking]]
* [[symmetric min max heap]]

== T ==
* [[tail]]
* [[tail recursion]]
* [[target (CS)|target]]
* [[temporal logic]]
* [[Leaf node|terminal]]
* [[terminal node]]
* [[ternary search]]
* [[ternary search tree]]
* [[text searching]]
* [[theta]]
* [[threaded binary tree]]
* [[threaded tree]]
* [[Three-dimensional space|three-dimensional]]
* [[three-way merge sort]]
* [[three-way radix quicksort]]
* [[time-constructible function]]
* [[time/space complexity]]
* [[top-down radix sort]]
* [[top-down tree automaton]]
* [[Top-nodes algorithm|top-node]]
* [[topological order]]
* [[topological sort]]
* [[topology tree]]
* [[total function]]
* [[totally decidable language]]
* [[totally decidable problem]]
* [[totally undecidable problem]]
* [[total order]]
* tour
* [[Tournament (graph theory)|tournament]]
* [[towers of Hanoi]]
* [[tractable]]
* [[transducer]]
* [[transition]]
* [[transition function]]
* [[transitive relation]]
* [[transitive closure]]
* [[transitive reduction]]
* [[transpose sequential search]]
* [[travelling salesman problem]]
* [[treap]]
* [[tree data structure|tree]]
* [[tree automaton]]
* [[tree contraction]]
* [[tree editing problem]]
* [[tree sort]]
* [[tree transducer]]
* [[tree traversal]]
* [[triangle inequality]]
* [[triconnected graph]]
* [[trie]]
* [[trinary function]]
* [[tripartition]]
* [[Travelling salesman problem|TSP]]
* [[Ternary search tree|TST]]
* [[Turbo-BM]]
* [[Turbo Reverse Factor]]
* [[Turing machine]]
* [[Turing reduction]]
* [[Turing transducer]]
* [[twin grid file]]
* [[two-dimensional]]
* [[two-level grid file]]
* [[2-3-4 tree]]
* [[2-3 tree]]
* [[Two Way algorithm]]
* [[two-way linked list]]
* [[two-way merge sort]]

== U ==
* [[Unbounded Knapsack Problem|UKP]]
* [[unary function]]
* [[unbounded knapsack problem]]
* [[uncomputable function]]
* [[uncomputable problem]]
* [[undecidable]]
* [[undecidable language]]
* [[undecidable problem]]
* [[undirected graph]]
* [[uniform circuit complexity]]
* [[uniform circuit family]]
* [[uniform hashing]]
* [[uniform matrix]]
* [[union (computer science)|union]]
* [[union of automata]]
* [[universal hashing]]
* [[universal state]]
* [[universal Turing machine]]
* [[universe]]
* [[UnShuffle sort]]
* [[unsolvable problem]]
* [[unsorted list]]
* [[upper triangular matrix]]

== V ==
* [[Van Emde Boas tree|Van Emde Boas priority queue]]
* [[vehicle routing problem]]
* [[Veitch diagram]]
* [[Venn diagram]]
* [[vertex (graph theory)|vertex]]
* [[vertex coloring]]
* [[vertex connectivity]]
* [[vertex cover]]
* [[vertical visibility map]]
* [[virtual hashing]]
* [[visibility map]]
* [[visible]]
* [[Viterbi algorithm]]
* [[Vp-tree]]
* [[VRP]]

== W ==
* [[Glossary of graph theory#walks|walk]]
* [[weak cluster]]
* [[weak-heap]]
* [[weak-heap sort]]
* [[weight-balanced tree]]
* [[weighted, directed graph]]
* [[weighted graph]]
* [[window]]
* [[witness]]
* [[work-depth model]]
* [[work-efficient]]
* [[work-preserving]]
* [[worst case]]
* [[worst-case cost]]
* [[worst-case minimum access]]

== X ==
* [[xor]]

== Y ==
* [[Yule distribution]]

== Z ==
* [[Zeller's congruence]]
* [[0-ary function]]
* [[0-based indexing]]
* [[0/1 knapsack problem]]
* [[Zhu-Takaoka]]
* [[Zipfian distribution]]
* [[Zipf's law]]
* [[zipper]]
* [[ZPP (complexity)|ZPP]]

{{DEFAULTSORT:Algorithms}}
[[Category:Lists of computer terms|Algorithms and data structures]]
[[Category:Indexes of articles|Algorithms and data structures]]
[[Category:Algorithms|*]]
[[Category:Data structures|*]]
[[Category:Glossaries on mathematics]]

[[ru:Список терминов, относящихся к алгоритмам и структурам данных]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Concurrent algorithms</title>
    <id>15913343</id>
    <revision>
      <id>193515263</id>
      <timestamp>2008-02-23T17:12:27Z</timestamp>
      <contributor>
        <username>Andreas Kaufmann</username>
        <id>72502</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '[[Algorithm]]s used for [[concurrent computing]].  [[Category:Concurrent computing|Algorithms]] [[Category:Algorithms]]'</comment>
      <text xml:space="preserve">[[Algorithm]]s used for [[concurrent computing]].

[[Category:Concurrent computing|Algorithms]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Digital signal processing</title>
    <id>707453</id>
    <revision>
      <id>316238786</id>
      <timestamp>2009-09-26T02:38:15Z</timestamp>
      <contributor>
        <username>Biasoli</username>
        <id>3826891</id>
      </contributor>
      <text xml:space="preserve">'''Digital signal processing''' ('''DSP''') is the study of [[signal processing|signal]]s in a [[digital]] representation and the processing methods of these signals. DSP and [[analog signal processing]] are subsets of [[signal processing]]. It has three major subfields: [[audio signal processing]], [[digital image processing]] and [[speech processing]].

{{catmore}}
{{Commons cat|Digital signal processing}}

[[Category:Signal processing]]
[[Category:Digital systems|Signal processing]]
[[Category:Algorithms]]

[[cs:Kategorie:Zpracování digitálního signálu]]
[[de:Kategorie:Digitale Signalverarbeitung]]
[[es:Categoría:Procesamiento digital de señales]]
[[eo:Kategorio:Cifereca signal-prilaborado]]
[[ko:분류:디지털 신호 처리]]
[[id:Kategori:Pengolahan sinyal digital]]
[[it:Categoria:Digital signal processing]]
[[mk:Категорија:Дигитална обработка на сигнали]]
[[ms:Kategori:Pemprosesan isyarat digital]]
[[no:Kategori:Digital signalbehandling]]
[[pl:Kategoria:Cyfrowe przetwarzanie sygnałów]]
[[ru:Категория:Цифровая обработка сигналов]]
[[th:หมวดหมู่:การประมวลผลสัญญาณดิจิทัล]]
[[tr:Kategori:Dijital sinyal işleme]]
[[uk:Категорія:Цифрова обробка сигналів]]
[[zh:Category:数字信号处理]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Machine learning</title>
    <id>706543</id>
    <revision>
      <id>314612322</id>
      <timestamp>2009-09-17T22:34:00Z</timestamp>
      <contributor>
        <username>VolkovBot</username>
        <id>3035831</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[tr:Kategori:Makine öğrenimi]]</comment>
      <text xml:space="preserve">'''Machine learning''' is a branch of [[statistics]] and [[computer science]], which studies algorithms and architectures that learn from observed facts.

{{catmore}}
{{Commons cat|Machine learning}}

[[Category:Artificial intelligence]]
[[Category:Computational statistics]]
[[Category:Algorithms]]
[[Category:Learning]]

[[ar:تصنيف:تعلم آلي]]
[[cs:Kategorie:Strojové učení]]
[[de:Kategorie:Maschinelles Lernen]]
[[es:Categoría:Aprendizaje automático]]
[[eu:Kategoria:Ikasketa automatikoa]]
[[fa:رده:یادگیری ماشینی]]
[[fr:Catégorie:Apprentissage automatique]]
[[ko:분류:기계 학습]]
[[it:Categoria:Apprendimento automatico]]
[[nl:Categorie:Automatisch leren]]
[[ja:Category:機械学習]]
[[ru:Категория:Машинное обучение]]
[[sk:Kategória:Strojové učenie]]
[[fi:Luokka:Koneoppiminen]]
[[th:หมวดหมู่:การเรียนรู้ของเครื่อง]]
[[tr:Kategori:Makine öğrenimi]]
[[zh:Category:机器学习]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Data mining</title>
    <id>5206601</id>
    <revision>
      <id>312443446</id>
      <timestamp>2009-09-07T19:26:18Z</timestamp>
      <contributor>
        <ip>87.65.58.254</ip>
      </contributor>
      <text xml:space="preserve">{{catmore}}
{{Commons cat|Data mining}}

Data mining facilities are included in some of the [[:Category:Data analysis software]] and [[:Category:Statistical software]] products.

[[Category:Data analysis|Mining]] &lt;!-- data mining software is statistical, with data analysis --&gt;
[[Category:Evaluation methods]]
[[Category:Computational statistics]]

[[Category:Information technology management]]
[[Category:Natural language processing]]
[[Category:Knowledge representation]]
[[Category:Business intelligence]]
[[Category:Knowledge discovery in databases]]
[[Category:Algorithms]]

[[ar:تصنيف:تنقيب البيانات]]
[[es:Categoría:Minería de datos]]
[[eu:Kategoria:Datu-meatzaritza]]
[[fa:رده:کاوش‌های ماشینی در داده‌ها]]
[[fr:Catégorie:Exploration de données]]
[[it:Categoria:Data mining]]
[[ko:분류:데이터 마이닝]]
[[pt:Categoria:Mineração de dados]]
[[tr:Kategori:Veri madenciliği]]
[[vi:Thể loại:Khai phá dữ liệu]]
[[zh:分类:数据挖掘]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Distributed algorithms</title>
    <id>15970362</id>
    <revision>
      <id>304964389</id>
      <timestamp>2009-07-29T22:07:47Z</timestamp>
      <contributor>
        <username>Miym</username>
        <id>8436643</id>
      </contributor>
      <minor/>
      <comment>see also</comment>
      <text xml:space="preserve">{{catmore}}

==See also==
* [[:Category:Distributed computing problems]]

[[Category:Algorithms]]
[[Category:Concurrent algorithms]]
[[Category:Distributed computing]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Memory management algorithms</title>
    <id>16143225</id>
    <revision>
      <id>255184085</id>
      <timestamp>2008-12-01T11:12:03Z</timestamp>
      <contributor>
        <username>VolkovBot</username>
        <id>3035831</id>
      </contributor>
      <minor/>
      <comment>robot  Adding: [[pt:Categoria:Algoritmos de gerenciamento de memória]]</comment>
      <text xml:space="preserve">[[Algorithm]]s used for [[memory management]].

[[Category:Memory management]]
[[Category:Algorithms]]

[[pt:Categoria:Algoritmos de gerenciamento de memória]]
[[tr:Kategori:Bellek yönetimi algoritmaları]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computational number theory</title>
    <id>1183044</id>
    <revision>
      <id>198048761</id>
      <timestamp>2008-03-13T21:12:52Z</timestamp>
      <contributor>
        <username>Andreas Kaufmann</username>
        <id>72502</id>
      </contributor>
      <minor/>
      <comment>[[Category:Algorithms|Number theory]]</comment>
      <text xml:space="preserve">{{catmore|Computational number theory}}

[[Category:Number theory]]
[[Category:Algorithms|Number theory]]
[[Category:Computational science|Number theory]]

[[eo:Kategorio:Komputa nombroteorio]]
[[ja:Category:計算数論]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computational group theory</title>
    <id>1426374</id>
    <revision>
      <id>302208892</id>
      <timestamp>2009-07-15T11:35:22Z</timestamp>
      <contributor>
        <username>Artem M. Pelenitsyn</username>
        <id>9255590</id>
      </contributor>
      <text xml:space="preserve">{{catmore}}

[[Category:Group theory]]
[[Category:Algorithms|Group theory]]

[[eo:Kategorio:Komputa grupa teorio]]
[[ru:Категория:Вычислительная теория групп]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computer graphics algorithms</title>
    <id>15847454</id>
    <revision>
      <id>231030713</id>
      <timestamp>2008-08-10T15:17:10Z</timestamp>
      <contributor>
        <username>Mhaitham.shammaa</username>
        <id>1608031</id>
      </contributor>
      <minor/>
      <comment>interwiki-ar</comment>
      <text xml:space="preserve">[[Algorithm]]s used in [[Computer graphics]].

[[Category:Computer graphics|Algorithms]]
[[Category:Algorithms|Graphics]]


[[ar:تصنيف:خوارزميات رسوميات حاسوبية]]
[[de:Kategorie:Algorithmus (Computergrafik)]]
[[pt:Categoria:Algoritmos de computação gráfica]]
[[tr:Kategori:Bilgisayar grafiği algoritmaları]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computer algebra</title>
    <id>984658</id>
    <revision>
      <id>326561388</id>
      <timestamp>2009-11-18T16:51:09Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor/>
      <comment>Adding [[Template:Commons_category|Commons category]] link to [[Commons:Category:Computer algebra]]</comment>
      <text xml:space="preserve">{{Portal|Algebra|Arithmetic symbols.svg}}
This is a category of topic relating to '''computer algebra''', which cover general information about [[algorithm]]s and techniques useful for computer algebra. Specific [[computer algebra system]]s are covered in the subcategory [[:Category:Computer algebra systems]].
{{Commons category|Computer algebra}}

[[Category:Algebra]]
[[Category:Computational science|Algebra]]
[[Category:Algorithms|Algebra]]

[[bs:Kategorija:Računarska algebra]]
[[eo:Kategorio:Komputila algebro]]
[[fr:Catégorie:Calcul formel]]
[[ru:Категория:Вычислительная алгебра]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Database algorithms</title>
    <id>16654862</id>
    <revision>
      <id>227436191</id>
      <timestamp>2008-07-23T16:06:54Z</timestamp>
      <contributor>
        <ip>72.43.188.243</ip>
      </contributor>
      <text xml:space="preserve">[[Algorithms]] used for implementation of [[database management systems]].

[[Category:Algorithms]]
[[Category:Databases]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Bioinformatics algorithms</title>
    <id>16659854</id>
    <revision>
      <id>201846374</id>
      <timestamp>2008-03-29T17:25:16Z</timestamp>
      <contributor>
        <username>Andreas Kaufmann</username>
        <id>72502</id>
      </contributor>
      <comment>[[WP:Automatic edit summaries|←]]Created page with '[[Algorithm]]s used in [[bioinformatics]].   [[Category:Bioinformatics]] [[Category:Algorithms]]'</comment>
      <text xml:space="preserve">[[Algorithm]]s used in [[bioinformatics]].


[[Category:Bioinformatics]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Calendar algorithms</title>
    <id>16660451</id>
    <revision>
      <id>218957908</id>
      <timestamp>2008-06-12T22:57:05Z</timestamp>
      <contributor>
        <username>Cgingold</username>
        <id>2013997</id>
      </contributor>
      <minor/>
      <text xml:space="preserve">[[Algorithm]]s for calculation of certain [[calendar date]]s.

[[Category:Algorithms]]
[[Category:Calendars|Algorithm]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Online algorithms</title>
    <id>16661272</id>
    <revision>
      <id>202136237</id>
      <timestamp>2008-03-30T20:45:11Z</timestamp>
      <contributor>
        <username>Andreas Kaufmann</username>
        <id>72502</id>
      </contributor>
      <comment>{{catmore|Online algorithm}}</comment>
      <text xml:space="preserve">{{catmore|Online algorithm}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computational physics</title>
    <id>792595</id>
    <revision>
      <id>312531023</id>
      <timestamp>2009-09-08T04:46:58Z</timestamp>
      <contributor>
        <username>Chobot</username>
        <id>259798</id>
      </contributor>
      <minor/>
      <comment>robot Adding: [[ko:분류:계산물리학]]</comment>
      <text xml:space="preserve">'''Computational physics''' is the study and implementation of numerical [[algorithm]]s in order to solve problems in [[physics]] for which a quantitative theory already exists.

{{catmore}}

[[Category:Physics]]
[[Category:Computational science]]
[[Category:Algorithms|Physics]]

[[ar:تصنيف:فيزياء حاسوبية]]
[[de:Kategorie:Computerphysik]]
[[es:Categoría:Física computacional]]
[[eo:Kategorio:Komputa fiziko]]
[[fa:رده:فیزیک محاسباتی]]
[[ko:분류:계산물리학]]
[[ja:Category:計算物理学]]
[[ru:Категория:Вычислительная физика]]
[[zh:Category:计算物理学]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computational statistics</title>
    <id>9272793</id>
    <revision>
      <id>323093033</id>
      <timestamp>2009-10-31T12:49:22Z</timestamp>
      <contributor>
        <username>Kolyma</username>
        <id>6237803</id>
      </contributor>
      <text xml:space="preserve">{{catmore}}

[[Category:Statistics]]
[[Category:Algorithms|Statistics]]

[[ko:분류:계산통계학]]
[[it:Categoria:Statistica computazionale]]
[[tr:Kategori:Berimsel istatistik]]
[[zh:Category:計算統計學]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Computer vision</title>
    <id>797088</id>
    <revision>
      <id>289611336</id>
      <timestamp>2009-05-13T03:57:03Z</timestamp>
      <contributor>
        <username>Ergosys</username>
        <id>8626533</id>
      </contributor>
      <minor/>
      <comment>spelling</comment>
      <text xml:space="preserve">{{catmore}}

'''[[Computer vision]]''' is an interdisciplinary field related to, e.g., [[artificial intelligence]], [[machine learning]], [[robotics]], [[signal processing]] and [[geometry]]. The purpose of computer vision is to program a [[computer]] to &quot;understand&quot; a scene or features in an image.

Computer vision shares many topics and methods with [[image processing]] and [[machine vision]].
Listed here are subjects relating to the field of computer vision, image processing and machine vision.

'''Notice:''' this category is intended for technical articles related to computer vision, not for showcasing reserach laboratories or companies that are active in area.

[[Category:Artificial intelligence]]
[[Category:Imaging]]
[[Category:Vision]]
[[Category:Image processing]]
[[Category:Algorithms|Vision]]

[[ar:تصنيف:رؤية حاسوبية]]
[[es:Categoría:Visión por computadora]]
[[eo:Kategorio:Komputila vizio]]
[[fa:رده:بینایی رایانه‌ای]]
[[fr:Catégorie:Vision artificielle]]
[[ja:Category:コンピュータビジョン]]
[[sv:Kategori:Datorseende]]
[[th:หมวดหมู่:คอมพิวเตอร์วิทัศน์]]
[[ur:زمرہ:شمارندی بصارت]]
[[zh:Category:计算机视觉]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Pattern matching</title>
    <id>2023695</id>
    <revision>
      <id>286821776</id>
      <timestamp>2009-04-29T09:49:03Z</timestamp>
      <contributor>
        <username>Pinar</username>
        <id>119849</id>
      </contributor>
      <comment>[[tr:Kategori:Örüntü eşleme]]</comment>
      <text xml:space="preserve">{{main|Pattern matching}}

[[Category:Formal languages]]
[[Category:Algorithms]]

[[ru:Категория:Сопоставление с образцом]]
[[sk:Kategória:Hľadanie vzorov]]
[[tr:Kategori:Örüntü eşleme]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Root-finding algorithms</title>
    <id>697463</id>
    <revision>
      <id>268886243</id>
      <timestamp>2009-02-06T09:40:27Z</timestamp>
      <contributor>
        <username>VolkovBot</username>
        <id>3035831</id>
      </contributor>
      <minor/>
      <comment>robot  Adding: [[ja:Category:求根アルゴリズム]]</comment>
      <text xml:space="preserve">A '''[[root-finding algorithm]]''' is a numerical method or [[algorithm]] for finding a value ''x'' such that ''f(x) = 0'', for a given [[function (mathematics)|function]] ''f''. Here, ''x'' is a single [[real number]]. Root-finding algorithms are studied in [[numerical analysis]].

[[Category:Algorithms]]
[[Category:Numerical analysis]]
[[Category:Zero]]

[[es:Categoría:Algoritmos de búsqueda de raíces]]
[[eo:Kategorio:Radiko-trovantaj algoritmoj]]
[[fr:Catégorie:Algorithme de recherche d'un zéro d'une fonction]]
[[he:קטגוריה:שיטות למציאת שורשים]]
[[ja:Category:求根アルゴリズム]]
[[tr:Kategori:Kök bulma algoritmaları]]
[[zh:Category:求根算法]]</text>
    </revision>
  </page>
  <page>
    <title>Super-recursive algorithm</title>
    <id>15641067</id>
    <revision>
      <id>324668731</id>
      <timestamp>2009-11-08T16:54:53Z</timestamp>
      <contributor>
        <ip>82.158.154.117</ip>
      </contributor>
      <text xml:space="preserve">In [[computability theory]], '''super-recursive algorithms''' are a generalization of ordinary [[algorithm]]s that are more powerful, that is, compute more than [[Turing machines]]. The term was introduced by Mark Burgin, whose book &quot;Super-recursive algorithms&quot; develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.

Burgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the [[Church-Turing thesis]], but this point of view has been criticized within the mathematical community and is not widely accepted.

== Definition ==

Burgin (2005: 13) uses the term '''recursive algorithms''' for [[algorithm]]s that can be implemented on Turing machines, and uses the word ''algorithm'' in a more general sense. Then a '''super-recursive class of algorithms''' is &quot;a class of algorithms in which it is possible to compute functions not computable by any [[Turing machine]]&quot; (Burgin 2005: 107).

Super-recursive algorithms are closely related to [[hypercomputation]] 
in a way similar to the relationship between ordinary computation and ordinary algorithms. Computation is a process, while an algorithm is a finite constructive description of such a process. Thus a super-recursive algorithm defines a &quot;computational process (including processes of input and output) that cannot be realized by recursive algorithms.&quot; (Burgin 2005: 108). A more restricted definition demands that  [[hypercomputation]] solves a [[supertask]] (see Copeland 2002; Hagar and Korolev 2007). 

Super-recursive algorithms are also related to '''algorithmic schemes''', which are more general than super-recursive algorithms. Burgin argues (2005: 115) that it is necessary to make a clear distinction between super-recursive algorithms and those algorithmic schemes that are not algorithms. Under this distinction, some types of hypercomputation are obtained by super-recursive algorithms, e.g., inductive Turing machines, while other types of hypercomputation are directed by algorithmic schemas, e.g., infinite time Turing machines. This explains how works on super-recursive algorithms are related to hypercomputation and vice versa. According to this argument, super-recursive algorithms are just one way of defining a hypercomputational process.

== Examples ==

Examples of super-recursive algorithms include (Burgin 2005: 132):
* '''limiting recursive functions''' and '''limiting partial recursive functions''' (E.M. Gold)
* '''trial and error predicates''' (Hilary Putnam) 
* '''inductive inference machines''' (Carl Smith)
* '''inductive Turing machines''', which perform computations similar to computations of [[Turing machines]] and produce their results after a finite number of steps (Mark Burgin)
* '''limit Turing machines''', which perform computations similar to computations of Turing machines but their final results are limits of their intermediate results (Mark Burgin)
*  '''trial-and-error machines''' (Ja. Hintikka and A. Mutanen)
* '''general Turing machines''' (J. Schmidhuber)
* '''Internet machines''' (van Leeuwen, J. and Wiedermann, J.)
* '''evolutionary computers''', which use DNA to produce the value of a function (Darko Roglic)
* '''fuzzy computation''' (Jirí Wiedermann)
* '''evolutionary Turing machines''' (Eugene Eberbach)

Examples of algorithmic schemes include:

* '''Turing machines with arbitrary oracles''' (Alan Turing)
* '''Transrecursive operators''' (Borodyanskii and Burgin)
* '''machines that compute with real numbers''' (L. Blum, F. Cucker, M. Shub, and S. Smale)
* '''neural networks based on real numbers''' (Hava Siegelmann)

For examples of practical '''super-recursive algorithms''', see the book of Burgin.

== Inductive Turing machines ==

'''Inductive Turing machines''' implement an important class of super-recursive algorithms. An inductive Turing machine is a definite list of well-defined instructions for completing a task, when given an initial state, will proceed through a well-defined series of successive states, eventually giving the final result. The difference between an inductive Turing machine and an ordinary [[Turing machine]] is that an ordinary Turing machine must stop when it has obtained its result, while in some cases an inductive Turing machine can continue to compute after obtaining the result, without stopping. Kleene called procedures that could run forever without stopping by the name ''calculation procedure or algorithm'' (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit &quot;some object&quot; (Kleene 1952:137). Burgin argues that this condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps. The reason that inductive Turing machines cannot be instructed to halt when their final output is produced is that in some cases inductive Turing machines may not be able to tell at which step the result has been obtained.

Simple inductive Turing machines are equivalent to other models of computation such as general Turing machines of Schmidhuber, trial and error predicates of Hilary Putnam, limiting partial recursive functions of Gold, and trial-and-error machines of Hintikka and Mutanen. More advanced inductive Turing machines are much more powerful. There are hierarchies of inductive Turing machines that can decide membership in arbitrary sets of the [[arithmetical hierarchy]] (Burgin 2005). In comparison with other equivalent models of computation, simple inductive Turing machines and general Turing machines give direct constructions of computing automata that are thoroughly grounded in physical machines. In contrast, trial-and-error predicates, limiting recursive functions, and limiting partial recursive functions present only syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial-and-error predicates as Turing machines are related to partial recursive functions and lambda calculus.

The non-halting computations of inductive Turing machines should not be confused with infinite-time computations (see, for example, Potgieter 2006). First, some computations of inductive Turing machines do halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not. Even it does not halt, an inductive Turing machine produces output from time to time. If this output stops changing, it is then considered the result of the computation. 

There are two main distinctions between ordinary Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine will always determine (by coming to a final state) when the result is obtained, while a simple inductive Turing machine, in some cases (such as when &quot;computing&quot; something that cannot be computed by an ordinary Turing machine), will not be able to make this determination.

== Schmidhuber's generalized Turing machines ==
A symbol sequence is '''computable in the limit''' if there is a finite, possibly non-halting program on a [[universal Turing machine]] that incrementally outputs every symbol of the sequence. This includes the dyadic expansion of &amp;pi; but still excludes most of the real numbers, because most cannot be described by a finite program. Traditional [[Turing machine]]s cannot edit their previous outputs; generalized [[Turing machines]], according to [[Jürgen Schmidhuber]], can. He defines the constructively describable symbol sequences as those that have a finite, non-halting program running on a generalized Turing machine, such that any output symbol eventually converges, that is, it does not change any more after some finite initial time interval. Due to limitations first exhibited by [[Kurt Gödel]] (1931), it may be impossible to predict the convergence time itself by a halting program, otherwise the [[halting problem]] could be solved. Schmidhuber (2000, 2002) uses this approach to define the set of formally describable or constructively computable universes or constructive [[theory of everything|theories of everything]]. Generalized Turing machines and simple inductive Turing machines are two classes of super-recursive algorithms that are the closest to recursive algorithms.

== Relation to the Church–Turing thesis ==

The Church–Turing thesis in recursion theory relies on a particular definition of the term ''algorithm''. Based on definitions that are more general than the one commonly used in recursion theory, Burgin argues that super-recursive algorithms, such as '''inductive Turing machines''' disprove the [[Church–Turing thesis]]. He proves furthermore that super-recursive algorithms could theoretically provide even greater efficiency gains than using [[quantum algorithms]].

Burgin's interpretation of super-recursive algorithms has encountered opposition in the mathematical community. One critic is logician [[Martin Davis]], who argues that Burgin's claims have been well understood &quot;for decades&quot;. Davis states, 
:&quot;The present criticism is not about the mathematical discussion of these matters but only about the misleading claims regarding physical systems of the present and future.&quot;(Davis 2006: 128)
Davis disputes Burgin's claims that sets at level &lt;math&gt;\Delta^0_2&lt;/math&gt; of the [[arithmetical hierarchy]] can be called computable, saying
:&quot;It is generally understood that for a computational result to be useful one must be able to at least recognize that it is indeed the result sought.&quot; (Davis 2006: 128)

==References==
* Akl, S.G., Three counterexamples to dispel the myth of the universal computer, ''Parallel Processing Letters'', Vol. 16, No. 3, September 2006, pp. 381 - 403.
* Akl, S.G., The myth of universal computation, in: Parallel Numerics, Trobec, R., Zinterhof, P., Vajtersic, M., and Uhl, A., Eds., Part 2, ''Systems and Simulation'', University of Salzburg, Salzburg, Austria and Jozef Stefan Institute, Ljubljana, Slovenia, 2005, pp. 211 - 236
* Angluin, D., and Smith, C. H. (1983) Inductive Inference: Theory and Methods, ''Comput. Surveys'', v. 15, no. 3, pp. 237—269
* Apsïtis, K, Arikawa, S, Freivalds, R., Hirowatari, E., and Smith, C. H. (1999) On the inductive inference of recursive real-valued functions, ''Theoret. Computer Science'', 219(1-2): 3—17
* Axt, P. (1959) On a Subrecursive Hierarchy and Primitive Recursive Degrees, ''Transactions of the American Mathematical Society'', v. 92, pp. 85-105
* Blum, L., and Blum, M. (1975) Toward a mathematical theory of inductive inference. ''Information and Control'' vol. 28, pp. 125-155
* Blum, L.,  F. Cucker, M. Shub, and S. Smale, ''Complexity and real computation'', Springer 1998
* Boddy, M, Dean, T.  1989.  &quot;Solving Time-Dependent Planning Problems&quot;. Technical Report: CS-89-03, Brown University
* Borodyanskii, Yu. M. and Burgin, M.S. (1994) Operations with Transrecursive Operators, Cybernetics and System Analysis, No. 4, pp. 3-11
* Burgin, Mark (2005), ''Super-recursive algorithms'', Monographs in computer science, Springer. ISBN 0387955690
**Review, José Félix Costa, [[MathSciNet]]. Review [http://www.ams.org/mathscinet/search/publdoc.html?pg1=IID&amp;s1=193826&amp;r=3&amp;mx-pid=2246430 MR2246430].
**Review, Harvey Cohn (2005), &quot;Computing Reviews&quot;, Review [http://www.computingreviews.net/browse/browse_topics4.cfm?ccs_id=2376 CR131542 (0606-0574)]
**Review, Martin Davis (2007), ''Bulletin of Symbolic Logic'', v. 13 n. 2. [http://www.math.ucla.edu/~asl/bsl/1302/1302-004.ps Online version]
**Review, Marc L. Smith (2006), &quot;The Computer Journal&quot;, Vol. 49 No. 6 [http://comjnl.oxfordjournals.org/cgi/reprint/49/6/762-a.pdf Online version]
**Review, Vilmar Trevisan (2005), [[Zentralblatt MATH]], Vol. 1070. Review [http://siba-sinmdb.unile.it/cgi-bin/zmen/ZMATH/en/quick.html?first=1&amp;maxdocs=3&amp;type=html&amp;an=1070.68038&amp;format=complete 1070.68038] 
* Burgin, M. How We Know What Technology Can Do, ''Communications of the ACM'', v. 44, No. 11, 2001, pp. 82-88
* Burgin M., Universal limit Turing machines, ''Notices of the Russian Academy of Sciences'', 325, No. 4, (1992), 654-658
* Burgin, M. and Klinger, A. Three Aspects of Super-recursive Algorithms and Hypercomputation or Finding Black Swans, ''Theoretical Computer Science'', v. 317, No. 1/3, 2004, pp. 1-11              
* Burgin, M. Algorithmic Complexity of Recursive and Inductive Algorithms, ''Theoretical Computer Science, v. 317, No. 1/3, 2004, pp. 31-60              
* Burgin, M. and Klinger, A. Experience, Generations, and Limits in Machine Learning, ''Theoretical Computer Science'', v. 317, No. 1/3, 2004, pp. 71-91    
* Campagnolo, M.L., Moore, C., and Costa, J.F.  (2000) An analog characterization of the subrecursive functions. In Proc. of the 4th Conference on Real Numbers and Computers, Odense University, pp. 91-109   
* Copeland, J. (2002) Hypercomputation, '' Minds and machines'', v. 12, pp. 461-502
* Davis, Martin (2006), &quot;[http://people.cs.uchicago.edu/~simon/TEACH/28000/DavisUniversal.pdf The Church–Turing Thesis: Consensus and opposition]&quot;. Proceedings, Computability in Europe 2006.  Lecture notes in computer science, 3988 pp. 125–132
* Eberbach, E. (2005) Toward a theory of evolutionary computation, ''BioSystems'' 82, 1-19
* Eberbach, E., and Wegner, P., Beyond Turing Machines, ''Bulletin of the European Association for Theoretical Computer Science'' (EATCS Bulletin), 81, Oct. 2003, 279-304
* [[Kurt Gödel]], 1931, &quot;Über formal unentscheidbare Sätze der ''[[Principia Mathematica]]'' und verwandter Systeme,&quot; ''Monatshefte für Mathematik und Physik 38'': 173-98.
* Gold, E.M. Limiting recursion. ''J. Symb. Logic'' 10 (1965), 28-48.
* Gold, E.M. (1967) Language Identification in the Limit, ''Information and Control'', v. 10, pp. 447-474 
* Hagar, A. and Korolev, A. (2007) Quantum Hypercomputation – Hype or Computation? http://philsci-archive.pitt.edu/archive/00003180/
* Hintikka, Ja. and Mutanen, A. An Alternative Concept of Computability, in “Language, Truth, and Logic in Mathematics”, Dordrecht, pp. 174-188, 1998
* E.J. Horvitz. Reasoning about inference tradeoffs in a world of bounded resources. Technical Report KSL-86-55, Medical Computer Science Group, Section on Medical Informatics, Stanford University, Stanford, CA, March 1986
* [[Juraj Hromkovič]], Design and Analysis of  Randomized Algorithms, Springer, 2005
* {{Citation | last1=Kleene | first1=Stephen C. | author1-link=Stephen C. Kleene| title=Introduction to Metamathematics | publisher=North-Holland | location=Amsterdam | year=First Edition 1952}}.
* Kosovsky, N. K. (1981) ''Elements of Mathematical Logic and its Application to the theory of Subrecursive Algorithms'', LSU Publ., Leningrad
* Peter Kugel,  It's time to think outside the computational box, ''Communications of the ACM'', Volume 48, Issue 11, November 2005 
* Petrus H.Potgieter, Zeno machines and hypercomputation, Theoretical Computer Science, Volume 358,  Issue 1  (July 2006) pp. 23 - 33   
* Hilary Putnam, Trial and Error Predicates and the Solution to a Problem of Mostowski. J. Symbolic Logic, Volume 30, Issue 1 (1965), 49-57
*Darko Roglic, &quot;[http://arxiv.org/abs/0708.2686 The universal evolutionary computer based on super-recursive algorithms of evolvability]&quot;
* [[Jürgen Schmidhuber|J. Schmidhuber]] (2000): Algorithmic Theories of Everything  http://arxiv.org/abs/quant-ph/0011122 
* [[Jürgen Schmidhuber|J. Schmidhuber]] (2002): Hierarchies of generalized [[Kolmogorov]] complexities and nonenumerable universal measures computable in the limit. International Journal of Foundations of Computer Science 13(4):587-612 http://www.idsia.ch/~juergen/kolmogorov.html
* Hava Siegelmann, ''Neural Networks and Analog Computation: Beyond the Turing Limit'', Birkhauser, 1999
* Turing, A. (1939) Systems of Logic Based on Ordinals, ''Proc. Lond. Math. Soc.'', Ser.2, v. 45: 161-228 
* van Leeuwen, J. and Wiedermann, J. (2000a) ''Breaking the Turing Barrier: The case of the Internet'', Techn. Report, Inst. of Computer Science, Academy of Sciences of the Czech. Rep., Prague
* Jiří Wiedermann, Characterizing the super-Turing computing power and efficiency of classical fuzzy Turing machines, ''Theoretical Computer Science'', Volume 317, Issue 1-3, June 2004
* Jiří Wiedermann and Jan van Leeuwen, The emergent computational potential of evolving artificial living systems, AI Communications, v. 15, No. 4, 2002
* Hector Zenil and Francisco Hernandez-Quiroz, On the possible computational power of the human mind, Worldviews, Science and Us, edited by Carlos Gershenson, Diederik Aerts and Bruce Edmonds, World Scientific, 2007, (arXiv:cs.NE/0605065v3)
* S. Zilberstein, Using Anytime Algorithms in Intelligent Systems, &quot;AI Magazine&quot;, 17(3):73-83, 1996

== External links ==
* [http://www.la-acm.org/Archives/laacm9912.html A New Paradigm for Computation]. Los Angeles ACM Chapter Meeting, December 1, 1999.
* ''[http://foldoc.org/?anytime+algorithm Anytime algorithm]'' from [[FOLDOC]]

[[Category:Theory of computation]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Divide and conquer algorithm</title>
    <id>201154</id>
    <revision>
      <id>327730073</id>
      <timestamp>2009-11-24T20:46:25Z</timestamp>
      <contributor>
        <ip>158.130.59.34</ip>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve">In [[computer science]], '''divide and conquer''' ('''D&amp;amp;C''') is an important [[algorithm design]] [[paradigm]] based on multi-branched [[recursion]]. A divide and conquer [[algorithm]] works by recursively breaking down a problem into two or more sub-problems of the same (or related) type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.

This technique is the basis of efficient algorithms for all kinds of problems, such as [[sorting algorithm|sorting]] (e.g., [[quicksort]], [[merge sort]]), [[multiplication algorithm|multiplying large numbers]] (e.g. [[Karatsuba algorithm|Karatsuba]]), [[syntactic analysis]] (e.g., [[top-down parser]]s), and computing the [[discrete Fourier transform]] ([[fast Fourier transform|FFT]]s).

On the other hand, the ability to understand and design D&amp;C algorithms is a skill that takes time to master.  As when [[proof (mathematics)|proving]] a [[theorem]] by induction, it is often necessary to replace the original problem by a more general or complicated problem in order to get the recursion going, and there is no systematic method for finding the proper generalization.

The name &quot;divide and conquer&quot; is sometimes applied also to algorithms that reduce each problem to only one subproblem, such as the [[binary search]] algorithm for finding a record in a sorted list (or its  analog in [[numerical algorithm|numerical computing]], the [[bisection algorithm]] for [[root-finding algorithm|root finding]]) &lt;ref name=CLR&gt;Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest, ''Introduction to Algorithms'' (MIT Press, 2000).&lt;/ref&gt;.  These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if  they use [[tail recursion]], they can be converted into simple [[loop (programming)|loops]].  Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a &quot;divide and conquer algorithm&quot;.  Therefore, some authors consider that the name &quot;divide and conquer&quot; should be used only when each problem may generate two or more subproblems.&lt;ref&gt;Brassard, G. and Bratley, P. Fundamental of Algorithmics, Prentice-Hall, 1996.&lt;/ref&gt; The name '''decrease and conquer''' has been proposed instead for the single-subproblem class.&lt;ref&gt;Anany V. Levitin, ''Introduction to the Design and Analysis of Algorithms'' (Addison Wesley, 2002).&lt;/ref&gt; 

The correctness of a divide and conquer algorithm is usually proved by [[mathematical induction]], and its computational cost is often determined by solving [[recurrence relation]]s.

== Early historical examples ==
Binary search, a divide and conquer algorithm in which the original problem is successively broken down into ''single'' subproblems of roughly half the original size, has a long history.  The idea of using a sorted list of items to facilitate searching dates back as far as Babylonia in 200BC,&lt;ref name=Knuth3/&gt; while a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly.&lt;ref name=Knuth3/&gt;  Another divide and conquer algorithm with a single subproblem is the [[Euclidean algorithm]] to compute the [[greatest common divisor]] of two numbers (by reducing the numbers to smaller and smaller equivalent subproblems), which dates to several centuries BC.

An early example of a divide-and-conquer algorithm with multiple subproblems is [[Carl Friedrich Gauss|Gauss]]'s 1805 description of what is now called the [[Cooley-Tukey FFT algorithm|Cooley-Tukey fast Fourier transform]] (FFT) algorithm,&lt;ref name=Heideman84&gt;Heideman, M. T., D. H. Johnson, and C. S. Burrus, &quot;Gauss and the history of the fast Fourier transform,&quot; IEEE ASSP Magazine, 1, (4), 14&amp;ndash;21 (1984)&lt;/ref&gt; although he did not analyze its [[algorithmic complexity|operation count]] quantitatively and FFTs did not become widespread until they were rediscovered over a century later.

An early two-subproblem D&amp;C algorithm that was specifically developed for computers and properly analyzed is the [[merge sort]] algorithm, invented by [[John von Neumann]] in 1945.&lt;ref&gt;{{ cite book | last=Knuth | first=Donald | authorlink=Donald Knuth | year=1998 | title=The Art of Computer Programming: Volume 3 Sorting and Searching | pages=159 | isbn=0-201-89685-0 }}&lt;/ref&gt;

Another notable example is the [[Karatsuba algorithm|algorithm]] invented by [[Anatolii Alexeevitch Karatsuba|A. Karatsuba]] in 1960 &lt;ref&gt;A. Karatsuba and Yu. Ofman, ''Multiplication of Many-Digital Numbers by Automatic Computers.'' Doklady Akad. Nauk SSSR, Vol. 145 (1962), pp. 293–294. Translation in Physics-Doklady, 7 (1963), pp. 595–596.&lt;/ref&gt; that could multiply two ''n''-digit numbers in &lt;math&gt;O(n^{\log_2 3})&lt;/math&gt; operations. This algorithm disproved [[Andrey Kolmogorov]]'s 1956 conjecture that &lt;math&gt;\Omega(n^2)&lt;/math&gt; operations would be required for that task.

As another example of a divide and conquer algorithm that did not originally involve computers, [[Donald Knuth|Knuth]] gives the method a [[post office]] typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered.&lt;ref name=Knuth3&gt;Donald E. Knuth, ''The Art of Computer Programming: Volume 3, Sorting and Searching'', second edition (Addison-Wesley, 1998).&lt;/ref&gt; This is related to a [[radix sort]], described for [[IBM 80 series Card Sorters|punch-card sorting]] machines as early as 1929.&lt;ref name=Knuth3/&gt;

== Advantages ==
=== Solving difficult problems ===
Divide and conquer is a powerful tool for solving conceptually difficult problems, such as the classic [[Tower of Hanoi]] puzzle: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem.

=== Algorithm efficiency ===
The divide-and-conquer paradigm often helps in the discovery of efficient algorithms.  It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the [[Strassen algorithm]] for matrix multiplication, and fast Fourier transforms. 

In all these examples, the D&amp;C approach led to an improvement in the [[asymptotic complexity|asymptotic cost]] of the solution.
For example, if the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size ''n'', and there are a bounded number ''p'' of subproblems of size ~ ''n''/''p'' at each stage, then the cost of the divide-and-conquer algorithm will be O(''n'' log ''n'').

=== Parallelism ===
Divide and conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.

=== Memory access ===
Divide-and-conquer algorithms naturally tend to make efficient use of memory [[cache]]s. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called ''[[cache-oblivious algorithm|cache oblivious]]'', because it does not contain the cache size(s) as an explicit parameter.&lt;ref name=&quot;cahob&quot;&gt;{{cite journal | author = M. Frigo | coauthors = C. E. Leiserson, H. Prokop | title = Cache-oblivious algorithms | journal = Proc. 40th Symp. On the Foundations of Computer Science | year = 1999}}&lt;/ref&gt;
Moreover, D&amp;C algorithms can be designed for many important algorithms, such as sorting, FFTs, and matrix multiplication, in such a way as to be ''optimal cache oblivious'' algorithms—they use the cache in a provably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is ''blocking'', where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine. 

The same advantage exists with regards to other hierarchical storage systems, such as [[Non-Uniform Memory Access|NUMA]] or [[virtual memory]], as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.

===Roundoff control===
In computations with rounded arithmetic, e.g. with [[floating point]] numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add ''N'' numbers either by a simple loop that adds each datum to a single variable, or by a D&amp;C algorithm  that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums.  While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.&lt;ref&gt;Nicholas J. Higham, &quot;The accuracy of floating point summation&quot;, ''SIAM J. Scientific Computing'' '''14''' (4), 783–799 (1993).&lt;/ref&gt;

== Implementation issues ==
=== Recursion ===
Divide-and-conquer algorithms are naturally implemented as [[subroutine|recursive procedures]]. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the [[call stack|procedure call stack]].

=== Explicit stack ===
Divide and conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a [[stack (data structure)|stack]], [[queue (data structure)|queue]], or [[priority queue]].  This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in [[breadth first recursion|breadth-first recursion]] and the [[branch and bound]] method for function optimization.  This approach is also the standard solution in programming languages that do not provide support for recursive procedures.

=== Stack size ===
In recursive implementations of D&amp;C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of [[stack overflow]].  Fortunately, D&amp;C algorithms that are time-efficient often have relatively small recursion depth.  For example, the quicksort algorithm can be implemented so that it never requires more than &lt;math&gt;\log_2 n&lt;/math&gt; nested recursive calls to  sort &lt;math&gt;n&lt;/math&gt; items.  

Stack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it.  Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure.  Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure, and/or by using an explicit stack structure.

=== Choosing the base cases === 
In any recursive algorithm, there is considerable freedom in the choice of the ''base cases'', the small subproblems that are solved directly in order to terminate the recursion.

Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve.  For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.

On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion.   Since a D&amp;C algorithm eventually reduces each problem or  sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low.  Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.

Thus, for example, many library implementations of quicksort will switch to a simple loop-based [[insertion sort]] (or similar) algorithm once the number of items to be sorted is sufficiently small.  Note that, if the empty list were the only base case, sorting a list with ''n'' entries would entain ''n''+1 quicksort calls that would do nothing but return immediately.  Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.

Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely [[loop unwinding|unrolled]] into code that has no recursion, loops, or [[Conditional (programming)|conditionals]] (related to the technique of [[partial evaluation]]).  For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.&lt;ref name=&quot;fftw&quot;&gt;{{cite journal | author = Frigo, M. | coauthors = Johnson, S. G. | url = http://www.fftw.org/fftw-paper-ieee.pdf | title = The design and implementation of FFTW3 | journal = Proceedings of the IEEE | volume = 93 | number = 2 | month = February | year = 2005 | pages = 216–231 | doi = 10.1109/JPROC.2004.840301}}&lt;/ref&gt;  The large number of separate base cases desirable to implement this strategy efficiently are sometimes produced by [[source code generation]] methods.&lt;ref name=&quot;fftw&quot;/&gt;

=== Sharing repeated subproblems ===
For some problems,  the branched recursion may end up evaluating the same sub-problem many times over.  In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as [[memoization]].  Followed to the limit, it leads to [[bottom-up design|bottom-up]] divide-and-conquer algorithms such as [[dynamic programming]] and [[chart parsing]].


== See also ==
{{portal|Computer Science|Internet map 1024.jpg}}
* [[Mathematical induction]]
* The [[Master theorem]]
* The [[Akra-Bazzi method]]
* [[Divide and rule]] (politics and sociology)
* [[Binary search algorithm]]
* [[Quicksort]]

== References ==
&lt;references/&gt; 

== External links ==
*[http://www.datastructures.info/the-divide-and-conquer-algorithmmethod/ Code example of Divide and Conquer, fast power calculation, in C++]

[[Category:Algorithms]]
[[Category:Operations research]]
[[Category:Optimization algorithms]]

[[cs:Rozděl a panuj (algoritmus)]]
[[de:Teile und herrsche (Informatik)]]
[[el:Διαίρει και βασίλευε (υπολογιστές)]]
[[es:Algoritmo divide y vencerás]]
[[fa:الگوریتم تقسیم و حل]]
[[fr:Diviser pour régner (informatique)]]
[[gl:Agoritmo divide e vencerás]]
[[ko:분할 정복 알고리즘]]
[[is:Deili- og drottnunarreiknirit]]
[[it:Divide et impera (informatica)]]
[[he:אלגוריתם הפרד ומשול]]
[[ja:分割統治法]]
[[pl:Dziel i zwyciężaj]]
[[pt:Divisão e conquista]]
[[ro:Divide et impera (informatică)]]
[[ru:Разделяй и властвуй (информатика)]]
[[sl:Deli in vladaj (računalništvo)]]
[[sr:Подели па владај (информатика)]]
[[zh:分治法]]</text>
    </revision>
  </page>
  <page>
    <title>Approximate counting algorithm</title>
    <id>20101191</id>
    <revision>
      <id>302774144</id>
      <timestamp>2009-07-18T14:13:37Z</timestamp>
      <contributor>
        <ip>98.28.161.10</ip>
      </contributor>
      <comment>/* Theory of operation */ fixed range of values</comment>
      <text xml:space="preserve">The '''approximate counting algorithm''' allows the counting of a large number of events using a small amount of memory.  Invented in 1977 by Robert Morris of [[Bell Labs]], it uses [[randomized algorithm|probabilistic techniques]] to increment the [[counter]].

== Theory of operation ==

Using Morris' algorithm, the counter represents an &quot;[[order of magnitude]] estimate&quot; of the actual count.  The approximation is mathematically [[Unbiased_estimator|unbiased]].

In order to increment the counter, a [[pseudo-random]] event is used, such that the incrementing is a probabilistic event.  In order to save space, only the exponent is kept.  For example, in base 2, the counter can estimate the count to be 1, 2, 4, 8, 16, 32, and all of the [[powers of two]].  The memory requirement is simply to hold the [[exponent]].

As an example, in order to increment from 4 to 8, a pseudo-random number would be generated such that a probability of .25 generates a positive change in the counter.  Otherwise, the counter remains at 4.

The table below illustrates some of the potential values of the counter:

{| class=&quot;wikitable&quot; border=&quot;1&quot;
|-
! Stored Binary Value of Counter
! Approximation
! Range of Possible Values for the Actual Count
|-
| 0
| 1
| 0, or initial value 
|-
| 1
| 2
| 1 or more
|-
| 10
| 4
| 2 or more
|-
| 11
| 8
| 3 or more
|-
| 100
| 16
| 4 or more
|-
| 101
| 32
| 5 or more
|}

If the counter holds the value of 101, which equates to an exponent of 5 (the decimal equivalent of 101), then the estimated count is 2^5, or 32.  There is a very low probability that the actual count of increment events was 5 (which would imply that an extremely rare event occurred with the pseudo-random number generator, the same probability as getting 10 consecutive heads in 10 coin flips). The actual count of increment events is likely to be around 32, but it could be infinitely high (with decreasing probabilities for actual counts above 32).

== Algorithm ==

When incrementing the counter, simply &quot;flip a coin&quot; the number of times of the counter's current value.  If it comes up &quot;Heads&quot; each time, then increment the counter.  Otherwise, do not increment it.

This can be done programmatically by generating &quot;c&quot; pseudo-random bits (where &quot;c&quot; is the current value of the counter), and using the logical AND function on all of those bits.  The result is a zero if any of those pseudo-random bits are zero, and a one if they are all ones.  Simply add the result to the counter.  This procedure should be executed each time the request is made to increment the counter.

== Applications ==
The algorithm is useful in examining large data streams for patterns.  This is particularly useful in applications of [[data compression]], sight and sound recognition, and other [[artificial intelligence]] applications.

== Sources ==

*Morris, R. Counting large numbers of events in small registers. Communications of the ACM 21, 10 (1977), 840–842
*Flajolet, P. Approximate Counting: A Detailed Analysis.  BIT 25, (1985), 113-134  [http://algo.inria.fr/flajolet/Publications/Flajolet85c.pdf]

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Monte Carlo algorithm</title>
    <id>624839</id>
    <revision>
      <id>312400450</id>
      <timestamp>2009-09-07T15:27:54Z</timestamp>
      <contributor>
        <username>Pohta ce-am pohtit</username>
        <id>7417813</id>
      </contributor>
      <minor/>
      <comment>{{algorithm-stub}}</comment>
      <text xml:space="preserve">In [[computing]], a '''Monte Carlo algorithm''' is one whose running time is deterministic, but whose output may be correct only with a certain [[probability]].  In practice, this probability of success is at least an inverse polynomial, which allows it to be made arbitrarily large via repetitions.

A Monte Carlo algorithm can be converted into a [[Las Vegas algorithm]] if there exists a procedure to verify that the output produced by the algorithm is indeed correct. If so, then the resulting Las Vegas algorithm is merely to run one iteration of the Monte Carlo algorithm and verify if the output is correct, repeating if not.

The [[complexity class]] [[BPP]] describes efficient Monte Carlo algorithms (with two-sided error). 

==See also==
* [[Randomness]]
* [[Monte Carlo method]]
* [[Las Vegas algorithm]]

{{algorithm-stub}}

[[Category:Randomness]]

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Principle of deferred decision</title>
    <id>20818154</id>
    <revision>
      <id>261541606</id>
      <timestamp>2009-01-02T22:38:56Z</timestamp>
      <contributor>
        <username>MatthewVanitas</username>
        <id>6524670</id>
      </contributor>
      <comment>Added category through [[WP:UNCAT]].  Please categorise articles whenever possible.</comment>
      <text xml:space="preserve">{{orphan|date=January 2009}}
{{unreferenced|date=January 2009}}
'''Principle of Deferred Decisions''' is a technique used in analysis of randomized algorithms.

== Definition ==
A [[randomized algorithm]] makes a set of random choices. This random choices may be intricately related making it difficult to analyze it. In many of these cases ''Principle of Deferred Decisions is used''. The idea behind the principle is that the entire set of random choices are not made in advance, but rather fixed only as they are revealed to the algorithm.

== Applications ==

[[Category:algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Pseudocode</title>
    <id>24185</id>
    <revision>
      <id>328137125</id>
      <timestamp>2009-11-27T02:10:28Z</timestamp>
      <contributor>
        <username>Ivan Akira</username>
        <id>6191043</id>
      </contributor>
      <comment>/* Syntax */ Add image.</comment>
      <text xml:space="preserve">{{Unreferenced|date=April 2009}}

'''Pseudocode''' is a compact and informal high-level description of a [[computer programming]] [[algorithm]] that uses the structural conventions of a [[programming language]], but is intended for human reading rather than machine reading. Pseudo-code typically omits details that are not essential for human understanding of the algorithm, such as [[variable declaration]]s, system-specific code and [[subroutines]]. The programming language is augmented with [[natural language]] descriptions of the details, where convenient, or with compact mathematical notation. The purpose of using pseudocode is that it is easier for humans to understand than conventional programming language code, and that it is a compact and environment-independent description of the key principles of an algorithm. It is commonly used in textbooks and scientific publications that are documenting various algorithms, and also in planning of computer program development, for sketching out the structure of the program before the actual coding takes place. 

No standard for pseudocode syntax exists, as a program in pseudocode is not an executable program. Pseudocode resembles, but should not be confused with, [[Skeleton (computer science)|skeleton programs]] including [[dummy code]], which can be [[Compiler|compiled]] without errors. [[Flowchart]]s can be thought of as a graphical alternative to pseudocode.

==Application==
Textbooks and [[scientific publication]]s related to [[computer science]] and [[numerical computation]] often use pseudocode in description of algorithms, so that all programmers can understand them, even if they do not all know the same programming languages. In textbooks, there is usually an accompanying introduction explaining the particular conventions in use. The level of detail of such languages may in some cases approach that of formalized general-purpose languages &amp;mdash; for example, [[Donald Knuth|Knuth]]'s seminal textbook ''[[The Art of Computer Programming]]'' describes algorithms in a fully-specified [[assembly language]] for a non-existent [[microprocessor]].

A [[programmer]] who needs to implement a specific algorithm, especially an unfamiliar one, will often start with a pseudocode description, and then simply &quot;translate&quot; that description into the target programming language and modify it to interact correctly with the rest of the program. Programmers may also start a project by sketching out the code in pseudocode on paper before writing it in its actual language, as a [[top-down]] structuring approach.

==Syntax==
[[File:Module in Pseudocode.png|thumb|upright=2.5|alt=Illustration showed the standard format for module in pseudocode.|This illustration showed the standard format for module (also known as method or function in programming language) in pseudocode. For information, there is no syntax coloring in any pseudocode, the coloring in the illustration just make viewer easy to understand the part of pseudocode.]]

As the name suggests, pseudocode generally does not actually obey the [[syntax]] rules of any particular language; there is no systematic standard form, although any particular writer will generally borrow style and syntax for example control structures from some conventional programming language. Popular syntax sources include [[Pascal programming language|Pascal]], [[BASIC]], [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]], [[Lisp programming language|Lisp]], and [[ALGOL]]. Variable declarations are typically omitted. Function calls and blocks of code, for example code contained within a loop, is often replaced by a one-line natural language sentence. 

Depending on the writer, pseudocode may therefore vary widely in style, from a near-exact imitation of a real programming language at one extreme, to a description approaching formatted prose at the other.

===Examples===
[[Pascal programming language|Pascal]] style pseudocode example:
&lt;pre&gt;
&lt;variable&gt; = &lt;expression&gt;

if &lt;condition&gt;
    do stuff;
else
    do other stuff;

while &lt;condition&gt;
    do stuff;

for &lt;variable&gt; from &lt;first value&gt; to &lt;last value&gt; by &lt;step&gt;
    do stuff with variable;

function &lt;function name&gt;(&lt;arguments&gt;)
    do stuff with arguments;
    return something;

&lt;function name&gt;(&lt;arguments&gt;)    // Function call
&lt;/pre&gt;

For more examples, see [[:category:articles with example pseudocode|articles with example pseudocode]].

==Mathematical style pseudocode==
In [[numerical computation]], pseudocode often consists of [[mathematical notation]], typically from [[set theory|set]] and [[matrix (mathematics)|matrix]] theory, mixed with the control structures of a conventional programming language, and perhaps also [[natural language]] descriptions. This is a compact and often informal notation that can be understood by a wide range of mathematically trained people, and is frequently used as a way to describe mathematical [[algorithm]]s. For example, the sum operator ([[capital-sigma notation]]) or the product operator ([[capital-pi notation]]) may represent a for loop and perhaps a selection structure in one expression: 
 &lt;code&gt;Return&lt;/code&gt; &lt;math&gt;\sum_{k\in S} x_k&lt;/math&gt; 

Normally non-[[ASCII]] [[typesetting]] is used for the mathematical equations, for example by means of [[TeX]] or [[MathML]] markup, or proprietary [[formula editor]]s.

Mathematical style pseudocode is sometimes referred to as [[pidgin code]], for example ''pidgin [[ALGOL]]'' (the origin of the concept), ''pidgin [[Fortran]]'', ''pidgin [[BASIC]]'', ''pidgin [[Pascal (programming language)|Pascal]]'', ''pidgin [[C (programming language)|C]]'', and ''pidgin [[Ada (programming language)|Ada]]''.


===Natural language grammar in programming languages===
Various attempts to bring elements of natural language grammar into computer programming have produced programming languages such as [[HyperTalk]], [[Lingo (programming language)|Lingo]], [[AppleScript]], [[SQL]] and [[Inform]]. In these languages, parentheses and other special characters are replaced by prepositions, resulting in quite talkative code. This may make it easier for a person without knowledge about the language to understand the code and perhaps also to learn the language. However, the similarity to natural language is usually more cosmetic than genuine. The syntax rules are just as strict and formal as in conventional programming, and do not necessarily make development of the programs easier.

===Mathematical programming languages===
An alternative to using mathematical pseudocode (involving set theory notation or matrix operations) for documentation of algorithms is to use a formal mathematical programming language that is a mix of non-ASCII mathematical notation and program control structures. Then the code can be parsed and interpreted by a machine. 

Several formal [[specification language]]s include set theory notation using special characters. Examples are:
* [[Z notation]] 
* [[Vienna Development Method]] Specification Language (VDM-SL).

Some [[array programming languages]] include vectorized expressions and matrix operations as non-ASCII formulas, mixed with conventional control structures. Examples are: 
* [[APL (programming language)|A programming language]] (APL), and its dialects [[APLX]] and [[A+ (programming language)|A+]].
* [[MathCAD]].

===Alternative forms of pseudocode===
Since the usual aim of pseudocode is present a simple form of some algorithm, you could use a language syntax closer to the problem domain. This would make the expression of ideas in the pseudocode simpler to convey in those domains.

==See also==
{{wiktionarypar}}
* [[Short Code]]
* [[Dummy code]]
* [[Pidgin code]]
* [[Program Design Language]] (PDL)
* [[Skeleton program]]
* [[Structured English]]
* [[Concept programming]]
* [[WALGOL]]

==External links==
*[http://www.csc.calpoly.edu/~jdalbey/SWE/pdl_std.html A pseudocode standard]
*[http://calgo.acm.org/ Collected Algorithms of the [[Association for Computing Machinery|ACM]]]
*[http://www.cs.cornell.edu/Courses/cs482/2003su/handouts/pseudocode.pdf Pseudocode Guidelines], PDF file.
*[http://www.coderookie.com/2006/tutorial/the-pseudocode-programming-process/ Pseudocode Programming Process] base on data from Code Complete book

[[Category:Programming language topics]]
[[Category:Articles with example pseudocode]]
[[Category:Algorithms]]
[[Category:Source code]]

[[ca:Pseudocodi]]
[[cs:Pseudokód]]
[[de:Pseudocode]]
[[el:Ψευδοκώδικας]]
[[es:Pseudocódigo]]
[[fa:شبه‌کد]]
[[fr:Pseudo-code]]
[[gl:Pseudocódigo]]
[[ko:의사코드]]
[[id:Kode palsu]]
[[it:Pseudocodice]]
[[he:פסאודו קוד]]
[[hu:Pszeudokód]]
[[nl:Pseudocode]]
[[ja:擬似コード]]
[[no:Pseudokode]]
[[pl:Pseudokod]]
[[pt:Pseudocódigo]]
[[ru:Псевдокод (язык описания алгоритмов)]]
[[sl:Psevdokoda]]
[[sr:Псеудокод]]
[[fi:Pseudokoodi]]
[[sv:Pseudokod]]
[[tr:Sözde kod]]
[[uk:Псевдокод]]
[[vi:Mã giả]]
[[zh:伪代码]]</text>
    </revision>
  </page>
  <page>
    <title>Best-case complexity</title>
    <id>21572972</id>
    <revision>
      <id>324632935</id>
      <timestamp>2009-11-08T12:06:06Z</timestamp>
      <contributor>
        <username>Arshad munir</username>
        <id>10967505</id>
      </contributor>
      <comment>/* Space complexity */</comment>
      <text xml:space="preserve">Often when dealing with computer algorithms it is not only useful to know [[worst-case complexity]], it is also important to know what is the best case for an algorithm.  This complexity can be split into two fields.  One is [[time complexity]] the other is [[space complexity]].  The definition using Ω(''g''(''n'')), read Big Omega of g, can be applied to both time or space and as such, only one definition of best case complexity is required.  Informally the best case complexity is the largest g(n) such that T(n) = Ω(''g''(''n'')) (''T''(''n'') is in Ω(''g''(''n''))).

: '''Best-case complexity ''b'' = {inf(''S'') | ''S'' = \-/ ''f'' s.t there exist ''a'' ''c'', ''B'' &gt; 0 s.t \-/ ''n'' ≥ ''B'' ''g''(''n'') ≥ ''cf''(''n'') } where ''g''(''n'') is your algorithm.'''

In plain English this means that the worst case complexity is the largest element in the set of all functions such that ''g'' is in Ω(''f''(''n'')) for all ''n'' greater than ''B'', which is the ''breaking point''.

Best case complexity is not all the best measure of well an algorithm performs.  More often computer scientists care more bout the worst case complexity and after that how well an algorithm will run on average.  An algorithm that has a best case of Ω(1) is pointless if its worst case is O(2&lt;sup&gt;''n''&lt;/sup&gt;).
==Time Complexity==

Best case time complexity is the least amount of time it would take for an algorithm to execute.  Small differences such as built in commands, and variable accesses are ignored, but loops and recursion have a much more profound effect on runtime.  
A very trivial example of this would be the code in java which prints the numbers 0 through n in order:
&lt;source lang=java&gt;
   public static void main(String[] args){
      for (int i = 0;i &lt; Integer.parseInt(args[1]) ; i++){
         System.out.println(&quot;&quot; + i);
      }
   }&lt;/source&gt;

The worst case complexity for this algorithm is the same as its best case.  This algorithm is in Ω(''n'').
In many cases however the best case does not equal the worst case.  In these instances it is quite easy to find a ''g''(''n'') such that ''T''(''n'') = Ω(''g''(''n'')), finding the tight bound however, ie. the inf(''S''), requires looking at possible sequences that yields the best results and proving there is no such sequence that yields a smaller ''g''(''n'').

Another less trivial example of this is linear search in java:

&lt;source lang=java&gt;
   public int search(A,x){
     boolean found = false;
     int c = 0;
     int index = -1;
     while ((c &lt; A.size()) &amp;&amp; (found == false)){
       if A[c] = x {
         found = true;
         index = b;
       }
     }return index;
   }&lt;/source&gt;

In this case clearly the worst case is O(''n'').  This happens when you have to traverse the entire array.  However with A[0] = x the algorithm only needs to check the first element.  This means the runtime of the algorithm at best no matter the size of the input is constant or O(1).  Clearly there is no input in which this algorithm could do better then O(1) so this is the best case complexity.

Ie. for a [[Bubble Sort]] modified to check for a sorted list the worst case is {{math|&lt;VAR &gt;n&lt;/VAR &gt;&lt;sup&gt;2&lt;/sup&gt;}} while the best case''(a list already in sorted order)'' yields a best case of Ω(n).  Even though it's a trivial case, it must still be considered.

==Space complexity==

Space complexity is the measure of the amount of space, excluding the original data passed in, that it takes an algorithm to execute on an input.  Any algorithm that must always store at least all of the data at a time runs in &amp;Omega:(''n'') of space complexity.  If however an algorithm only needed to store a counter of some kind its space complexity would be log(''n'') where ''n'' is the final value of the counter.  This is the reason as it only takes log&lt;sub&gt;2&lt;/sub&gt;(''arshad'') bits to store a number.

== See also ==
* [[Average-case complexity]]
* [[Worst-case complexity]]
* [[Big O notation]]
* [[Computational complexity theory]]

== References ==
{{No footnotes|date=February 2009}}
*Katoen,Joost-Pieter [http://webpages.ull.es/users/jriera/Docencia/lec1.pdf Introduction to Algorithm Analysis Lecture Notes],2002.

== Further reading ==
*Paul E. Black, &quot;Ω&quot;, in Dictionary of Algorithms and Data Structures [online], Paul E. Black, ed., U.S. National Institute of Standards and Technology. 14 August 2008. Retrieved Feb. 20/09. Available from: http://www.itl.nist.gov/div897/sqg/dads/HTML/omegaCapital.html .
*Michael Sipser (1997). Introduction to the Theory of Computation. PWS Publishing.
*Joachim Ziegler,[http://www.leda-tutorial.org/en/official/index.html The LEDA Tutorial] 2004. - section 2.2.3

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>List of algorithms</title>
    <id>18568</id>
    <revision>
      <id>324872451</id>
      <timestamp>2009-11-09T17:38:58Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* Digital signal processing */</comment>
      <text xml:space="preserve">The following is a '''list of algorithms''' described in Wikipedia.   This list is manually updated and additions of links to existing pages are welcome. 
See also the [[list of data structures]], [[list of algorithm general topics]] and [[list of terms relating to algorithms and data structures]].

If you intend to describe a new [[algorithm]], please read [[wikipedia:algorithms on Wikipedia|algorithms on Wikipedia]] first, then add a link to your article and a one-line description here.

==Combinatorial algorithms==
{{see| Combinatorics}}
===General combinatorial algorithms===

* [[Cycle detection#Brent's algorithm|Brent's algorithm]]: finds cycles in iterations using only two iterators
* [[Floyd's cycle-finding algorithm]]: finds cycles in iterations
* [[Gale-Shapley algorithm]]: solve the [[stable marriage problem]]
* [[Pseudorandom number generator]]s (uniformly distributed):
** [[Blum Blum Shub]]
** [[Mersenne twister]]
** [[Linear congruential generator]]
** [[Lagged Fibonacci generator]]

===Graph algorithms===
{{see|Graph theory|:Category:Graph algorithms}}
* [[Coloring algorithm]]: Graph coloring algorithm.
* [[Hopcroft–Karp algorithm]]: convert a bipartite graph to a [[maximum cardinality matching]] 
* [[Hungarian algorithm]]: algorithm for finding a [[perfect matching]]
* [[Prüfer sequence|Prüfer coding]]: conversion between a labeled tree and its [[Prüfer sequence]]
* [[Tarjan's off-line least common ancestors algorithm]]: compute [[lowest common ancestor|lowest common ancestors]] for pairs of nodes in a tree
* [[Topological sorting|Topological sort]]: finds linear order of nodes(e.g. jobs) based on their dependencies.

====Graph drawing====
{{see|Graph drawing}}
* [[Force-based algorithms]] (also known as force-directed algorithms)
* [[Spectral layout]]
* [[Spring based algorithm]]

====Network theory====
{{see|Network theory}}
* Network analysis
** Link analysis
*** [[Girvan–Newman algorithm]]: detect communities in complex systems
*** Web link analysis
**** [[Hyperlink-Induced Topic Search]] (HITS) (also known as [[Hubs and authorities]])
**** [[PageRank]]
**** [[TrustRank]]
* [[Flow network]]s
** [[Edmonds-Karp algorithm]]: implementation of Ford-Fulkerson
** [[Ford-Fulkerson algorithm]]: computes the [[maximum flow problem|maximum flow]] in a graph
** [[Karger's algorithm]]:  a Monte Carlo method to compute the [[minimum cut]] of a connected graph
** [[Push-relabel algorithm]]: computes a [[maximum flow problem|maximum flow]] in a graph

====Routing====
* [[Edmonds's algorithm]] (also known as Chu–Liu/Edmonds's algorithm): find maximum or minimum branchings
* [[Euclidean minimum spanning tree]]: algorithms for computing the minimum spanning tree of a set of points in the plane
* [[Minimum spanning tree]]
** [[Boruvka's algorithm]]
** [[Kruskal's algorithm]]
** [[Prim's algorithm]]
** [[Reverse-delete algorithm]]
* [[Nonblocking Minimal Spanning Switch]] say, for a [[telephone exchange]]
* [[Shortest path problem]]
** [[Bellman-Ford algorithm]]: computes [[shortest path problem|shortest paths]] in a weighted graph (where some of the edge weights may be negative)
** [[Dijkstra's algorithm]]: computes [[shortest path problem|shortest paths]] in a graph with non-negative edge weights
** [[Floyd-Warshall algorithm]]: solves the [[all pairs shortest path]] problem in a weighted, directed graph
** [[Johnson algorithm]]: All pairs shortest path algorithm in sparse weighted directed graph
** [[Perturbation methods]]: an algorithm that computes a locally [[shortest path problem|shortest paths]] in a graph
* [[Traveling salesman problem]]
** [[Christofides algorithm]]
** [[Nearest neighbour algorithm]]

====Search====
{{see|State space search|Graph search algorithm}}
* [[A-star search algorithm|A* tree search]]: special case of best-first search that uses heuristics to improve speed
* [[B-star search algorithm|B* search]]: a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)
* [[Backtracking]]: abandon partial solutions when they are found not to satisfy a complete solution
* [[Beam search]]: is a heuristic search algorithm that is an optimization of [[best-first search]] that reduces its memory requirement
* [[Beam stack search]]: integrates backtracking with [[beam search]]
* [[Best-first search]]: traverses a graph in the order of likely importance using a [[priority queue]]
* [[Bidirectional search]]: find the shortest path from an initial vertex to a goal vertex in a directed graph
* [[Breadth-first search]]: traverses a graph level by level
* [[D* search algorithm|D*]]: an [[incremental heuristic search]] algorithm
* [[Depth-first search]]: traverses a graph branch by branch
* [[Iterative deepening depth-first search]] (IDDFS): a state space search strategy
* [[Lexicographic breadth-first search]] (also known as Lex-BFS): a linear time algorithm for ordering the vertices of a graph
* [[Uniform-cost search]]: a [[Tree traversal|tree search]] that finds the lowest cost route where costs vary
* [[SSS*]]: state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm

====Subgraphs====
* [[Bron–Kerbosch algorithm]]: a technique for finding [[maximal clique]]s in an undirected graph
* [[Strongly connected components]]
** [[Gabow's algorithm|Cheriyan/Mehlhorn/Gabow algorithm]]
** [[Kosaraju's algorithm]]
** [[Tarjan's algorithm]]

===Sequence algorithms===
{{see|Sequences}}
====Sequence alignment====
* [[Hirschberg's algorithm]]: finds the least cost [[sequence alignment]] between two sequences, as measured by their [[Levenshtein distance]]
* [[Needleman-Wunsch algorithm]]: find global alignment between two sequences
* [[Smith-Waterman algorithm]]: find local sequence alignment
* [[Dynamic time warping]]: measure similarity between two sequences which may vary in time or speed

====Approximate matching====
* [[Bitap algorithm]]: fuzzy algorithm that determines if strings are approximately equal.
* [[String metrics]]: compute a similarity or dissimilarity (distance) score between two pairs of text strings
** [[Damerau–Levenshtein distance]] compute a distance measure between two strings, improves on [[Levenshtein distance]]
** [[Dice's coefficient]] (also known as the Dice coefficient): a similarity measure related to the [[Jaccard index]]
** [[Hamming distance]]: sum number of positions which are different
** [[Jaro-Winkler distance]]:  is a measure of similarity between two strings
** [[Levenshtein distance|Levenshtein edit distance]]: compute a metric for the amount of difference between two sequences 
* [[Phonetic algorithm]]s
** [[Daitch-Mokotoff Soundex]]: a [[Soundex]] refinement which allows matching of Slavic and Yiddish surnames
** [[Double Metaphone]]: an improvement on Metaphone
** [[Metaphone]]: an algorithm for indexing words by their sound, when pronounced in English
** [[New York State Identification and Intelligence System|NYSIIS]]: [[phonetic algorithm]], improves on [[Soundex]]
** [[Soundex]]: a phonetic algorithm for indexing names by sound, as pronounced in English
* [[Trigram search]]: search for text when the exact syntax or spelling of the target object is not precisely known

====Item search====
* [[Linear search]]: finds an item in an unsorted list
* [[Selection algorithm]]: finds the ''k''th largest item in a list
* [[Sorted list]]s
** [[Binary search algorithm]]: locates an item in a sorted list
** [[Fibonacci search technique]]: search a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of [[Fibonacci numbers]]
** [[Jump search]] (also called block search)
** [[Interpolation search|Predictive search]]: binary like search which factors in [[magnitude (mathematics)|magnitude]] of search term versus the high and low values in the search.  Sometimes called dictionary search or interpolated search.
** [[Uniform binary search]]: an optimization of the classic binary search algorithm
* [[Ternary search]]: a technique for finding the minimum or maximum of a function that is either strictly increasing and then strictly decreasing or vice versa

====Merging====
{{main|Merge algorithm}}
* Simple Merge algorithm
* k-way Merge algorithm

====Permutations====
{{see|Permutations}}
* [[Fisher–Yates shuffle]] (also known as the Knuth shuffle): randomly shuffle a finite set
* [[Robinson–Schensted algorithm]]: generates permutations from pairs of [[Young tableaux]]
* [[Steinhaus–Johnson–Trotter algorithm]] (also known as the Johnson–Trotter algorithm): generate permutations by transposing elements

====Sorting====
{{main|Sorting algorithms}}
* Exchange Sorts
** [[Bubble sort]]: for each pair of indices, swap the items if out of order
** [[Cocktail sort]]
** [[Comb sort]]
** [[Gnome sort]]
** [[Odd-even sort]]
** [[Quicksort]]: divide list into two, with all items on the first list coming before all items on the second list.; then sort the two lists. Often the method of choice
* Humorous or ineffective
** [[Bogosort]]
** [[Stooge sort]]
* Hybrid
** [[Flashsort]]
** [[Introsort]]: begin with quicksort and switch to heapsort when the recursion depth exceeds a certain level
* Insertion sorts
** [[Insertion sort]]: determine where the current item belongs in the list of sorted ones, and insert it there
** [[Library sort]]
** [[Patience sorting]]
** [[Shell sort]]: an attempt to improve insertion sort
** [[Tree sort]] (binary tree sort): build binary tree, then traverse it to create sorted list
* Merge sorts
** [[Merge sort]]: sort the first and second half of the list separately, then merge the sorted lists
** [[Strand sort]]
* Non-comparison sorts
** [[Bead sort]]
** [[Bucket sort]]
** [[Burstsort]]: build a compact, cache efficient [[burst trie]] and then traverse it to create sorted output
** [[Counting sort]]
** [[Pigeonhole sort]]
** [[Postman sort]]: variant of Bucket sort which takes advantage of hierarchical structure
** [[Radix sort]]: sorts strings letter by letter
* Selection sorts
** [[Heapsort]]: convert the list into a heap, keep removing the largest element from the heap and adding it to the end of the list
** [[Selection sort]]: pick the smallest of the remaining elements, add it to the end of the sorted list
** [[Smoothsort]]
* Other
** [[Bitonic sorter]]
** [[Pancake sorting]]
** [[Topological sorting|Topological sort]]
* Unknown class
** [[Samplesort]]

====Subsequences====
{{see|Subsequence}}
*[[Kadane's algorithm]]: finds maximum sub-array of any size
*[[Longest common subsequence problem]]: Find the longest subsequence common to all sequences in a set of sequences
*[[Longest increasing subsequence problem]]: find the longest increasing subsequence of a given sequence
*[[Shortest common supersequence]] problem: Find the shortest supersequence that contains two or more sequences as subsequences

====Substrings====
{{see|Substring}}
* [[Longest common substring problem]]:  find the longest string (or strings) that is a substring (or are substrings) of two or more strings
* [[Substring search]]
** [[Aho-Corasick algorithm]]: [[trie]] based algorithm for finding all substring matches to any of a finite set of strings
** [[Boyer-Moore string search algorithm]]: amortised linear ([[sublinear]] in most times) algorithm for substring search
** [[Boyer-Moore-Horspool algorithm]]: Simplification of Boyer-Moore
** [[Knuth-Morris-Pratt algorithm]]: substring search which bypasses reexamination of matched characters
** [[Rabin-Karp string search algorithm]]: searches multiple patterns efficiently
** [[Zhu-Takaoka]]: a variant of the [[Boyer-Moore string search algorithm]]
* [[Ukkonen's algorithm]]: a [[linear-time]], [[online algorithm]] for constructing [[suffix tree]]s

==Computational mathematics==
{{see|Computational mathematics}}
{{see also|List of algorithms#Combinatorial algorithms|l1=Combinatorial algorithms|List of algorithms#Computational science|l2=Computational science}}

===Abstract algebra===
{{see|Abstract Algebra}}
* [[Chien search]]: a recursive algorithm for determining roots of polynomials defined over a finite field
* [[Schreier-Sims algorithm]]: computing a base and [[strong generating set]] (BSGS) of a [[permutation group]]
* [[Todd-Coxeter algorithm]]: Procedure for generating [[coset]]s.

===Computer algebra===
{{see|Computer algebra}}
* [[Buchberger's algorithm]]: finds a [[Gröbner basis]]
* [[Cantor–Zassenhaus algorithm]]: factor polynomials over finite fields
* [[Faugère F4 algorithm]]: finds a Gröbner basis (also mentions the F5 algorithm)
* [[Gosper's algorithm]]: find sums of hypergeometric terms that are themselves hypergeometric terms
* [[Knuth-Bendix completion algorithm]]: for [[rewriting]] rule systems
* [[Multivariate division algorithm]]: for [[polynomial]]s in several indeterminates
* [[Pollard's kangaroo algorithm]] (also known as Pollard's lambda algorithm ): an algorithm for solving the discrete logarithm problem
* [[Polynomial long division]]: an algorithm for dividing a polynomial by another polynomial of the same or lower degree
* [[Risch algorithm]]: an algorithm for the calculus operation of indefinite integration (i.e. finding [[antiderivatives]])

===Geometry===
{{main|:Category:Geometric algorithms|l1=Geometric algorithms}}
{{see|Computational geometry}}
* [[Closest pair problem]]:  find the pair of points (from a set of points) with the smallest distance between them
* [[Collision detection]] algorithms: check for the collision or intersection of two given solids
* [[Cone algorithm]]: identify surface points
* [[Convex hull algorithms]]: determining the [[convex hull]] of a [[Set (mathematics)|set]] of points
** [[Chan's algorithm]]
** [[Gift wrapping algorithm]] or Jarvis march
** [[Graham scan]]
** [[Kirkpatrick–Seidel algorithm]]
* [[Euclidean distance map|Euclidean Distance Transform]] - Computes the distance between every point in a grid and a discrete collection of points.
* [[Geometric hashing]]: a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an [[affine transformation]]
* [[Gilbert-Johnson-Keerthi distance algorithm]]: determining the smallest distance between two [[convex set|convex]] shapes.
* [[Jump-and-Walk algorithm]]: an algorithm for point location in triangulations 
* [[Laplacian smoothing]]: an algorithm to smooth a polygonal mesh
* [[Line segment intersection]]: finding whether lines intersect, usually with a [[sweep line algorithm]]
** [[Bentley-Ottmann algorithm]]
** [[Shamos-Hoey algorithm]]
* [[Minimum bounding box algorithms]]: find the [[Minimum_bounding_box#Oriented_minimum_bounding_box|oriented minimum bounding box]] enclosing a set of points
* [[Nearest neighbor search]]:  find the nearest point or points to a query point
* [[Point in polygon]] algorithms: tests whether a given point lies within a given polygon
* [[Rotating calipers]]: determine all [[antipodal point|antipodal]] pairs of points and vertices on a [[convex polygon]] or [[convex hull]].
* [[Shoelace algorithm]]: determine the area of a polygon whose vertices are described by ordered pairs in the plane
* [[Triangulation (advanced geometry)|Triangulation]]
** [[Delaunay triangulation]]
*** [[Ruppert's algorithm]] (also known as Delaunay refinement): create quality Delaunay triangulations
** [[Marching triangles]]: reconstruct two-dimensional surface geometry from an unstructured point cloud
** [[Polygon triangulation]] algorithms: decompose a polygon into a set of triangles
** [[Voronoi diagram]]s, geometric [[duality (mathematics)|dual]] of [[Delaunay triangulation]]
*** [[Bowyer-Watson algorithm]]: create voronoi diagram in any number of dimensions
*** [[Fortune's Algorithm]]: create voronoi diagram

===Number theoretic algorithms===
{{see|Number theory}}
* [[Binary gcd algorithm]]: Efficient way of calculating gcd.
* [[Booth's multiplication algorithm]]
* [[Chakravala method]]: a cyclic algorithm to solve indeterminate quadratic equations, including [[Pell's equation]]
* [[Discrete logarithm]]:
** [[Baby-step giant-step]]
** [[Index calculus algorithm]]
** [[Pollard's rho algorithm for logarithms]]
** [[Pohlig&amp;ndash;Hellman algorithm]]
* [[Euclidean algorithm]]: computes the [[greatest common divisor]]
* [[Extended Euclidean algorithm]]: Also solves the equation ''ax''&amp;nbsp;+&amp;nbsp;''by''&amp;nbsp;=&amp;nbsp;''c''.
* [[Integer factorization]]: breaking an integer into its [[prime number|prime]] factors
** [[Congruence of squares]]
** [[Dixon's algorithm]]
** [[Fermat's factorization method]]
** [[General number field sieve]]  
** [[Lenstra elliptic curve factorization]]
** [[Pollard's p &amp;minus; 1 algorithm|Pollard's ''p''&amp;nbsp;−&amp;nbsp;1 algorithm]]
** [[Pollard's rho algorithm]]
** [[prime factorization algorithm]]
** [[Quadratic sieve]]
** [[Special number field sieve]]
** [[Trial division]]
* [[Multiplication algorithm]]s: fast multiplication of two numbers
* [[Odlyzko&amp;ndash;Schönhage algorithm]]: calculates nontrivial zeroes of the [[Riemann zeta function]]
* [[Primality test]]s: determining whether a given number is [[prime number|prime]]
** [[AKS primality test]]
** [[Miller&amp;ndash;Rabin primality test]]
** [[Sieve of Atkin]]
** [[Sieve of Eratosthenes]]

===Numerical algorithms===
{{see|Numerical analysis|List of numerical analysis topics}}

==== Elementary and special functions====
{{see|Special functions}}
* [[Computing π|Computation of π]]:
** [[Borwein's algorithm]]: an algorithm to calculate the value of 1/π
** [[Gauss–Legendre algorithm]]: computes the digits of [[pi]]
** [[Bailey–Borwein–Plouffe formula]]: (BBP formula) a spigot algorithm for the computation of the nth binary digit of π
* Hyperbolic and Trigonometric Functions:
** [[BKM algorithm]]:  compute [[Elementary function (differential algebra)|elementary functions]] using a table of logarithms
** [[CORDIC]]:  compute hyperbolic and trigonometric functions using a table of arctangents
* Exponentiation:
** [[Addition-chain exponentiation]] exponentiation by positive integer powers that requires a minimal number of multiplications
** [[Exponentiating by squaring]]: an algorithm used for the fast computation of [[Arbitrary-precision arithmetic|large integer]] powers of a number
* [[Montgomery reduction]]: an algorithm that allows [[modular arithmetic]] to be performed efficiently when the modulus is large
* [[Multiplication algorithm]]s: fast multiplication of two numbers
** [[Booth's multiplication algorithm]]: a multiplication algorithm that multiplies two signed binary numbers in two's complement notation
** [[Fürer's algorithm]]:  an integer multiplication algorithm for very large numbers possessing a very low [[Computational complexity theory|asymptotic complexity]]
** [[Karatsuba algorithm]]:  an efficient procedure for multiplying large numbers
** [[Schönhage-Strassen algorithm]]: an asymptotically fast multiplication algorithm for large integers
** [[Toom–Cook multiplication]]: (Toom3) a multiplication algorithm for large integers
* [[Rounding functions]]: the classic ways to round numbers
* [[Spigot algorithm]]: A way to compute the value of a [[mathematical constant]] without knowing preceding digits
* Square and Nth root of a number:
** [[Alpha max plus beta min algorithm]]: an approximation of the square-root of the sum of two squares
** [[Methods of computing square roots]]
** [[Nth root algorithm|''n''th root algorithm]]
** [[Shifting nth-root algorithm]]: digit by digit root extraction
* Summation:
** [[Binary splitting]]:  a [[divide and conquer]] technique which speeds up the numerical evaluation of many types of series with rational terms
** [[Kahan summation algorithm]]: a more accurate method of summing floating-point numbers

==== Geometric ====
* [[Level set method]] (LSM):  a numerical technique for tracking interfaces and shapes
* [[Radon transform#Filtered back-projection|Filtered back-projection]]: efficiently compute the inverse 2-dimensional [[Radon transform]].

==== Interpolation and extrapolation ====
{{see|Interpolation|Extrapolation}}
* [[Birkhoff interpolation]]: an extension of polynomial interpolation
* [[Cubic interpolation]]
* [[Hermite interpolation]]
* [[Linear interpolation]]: a method of curve fitting using linear polynomials
* [[Monotone cubic interpolation]]: a variant of cubic interpolation that preserves monotonicity of the data set being interpolated.
* [[Multivariate interpolation]]
** [[Bicubic interpolation]], a generalization of [[cubic interpolation]] to two dimensions
** [[Bilinear interpolation]]: an extension of [[linear interpolation]] for interpolating functions of two variables on a regular grid
** [[Lanczos resampling]] (&quot;Lanzosh&quot;): a multivariate interpolation method used to compute new values for any digitally sampled data
** [[Nearest-neighbor interpolation]]
** [[Tricubic interpolation]], a generalization of [[cubic interpolation]] to three dimensions
* [[Pareto interpolation]]: a method of estimating the median and other properties of a population that follows a [[Pareto distribution]].
* [[Polynomial interpolation]]
** [[Neville's algorithm]]
* [[Spline interpolation]]: Reduces error with [[Runge's phenomenon]].
** [[De Boor algorithm]]: [[B-spline]]s
** [[De Casteljau's algorithm]]: [[Bézier spline]]s
* [[Trigonometric interpolation]]

==== Numerical integration ====
{{see|Numerical integration}}
* [[MISER algorithm]]: Monte Carlo simulation, [[numerical integration]]
* [[Multigrid methods]]: (MG methods): a group of algorithms for solving differential equations using a hierarchy of discretizations
* [[Verlet integration]] (IPA: [veʁ'le]): integrate Newton's equations of motion

==== Linear algebra ====
{{see|Numerical linear algebra}}
* [[Eigenvalue algorithm]]s
** [[Arnoldi iteration]]
** [[Inverse iteration]]
** [[Jacobi eigenvalue algorithm|Jacobi method]]
** [[Lanczos iteration]]
** [[QR algorithm]]
** [[Rayleigh quotient iteration]]
* [[Gram-Schmidt process]]: orthogonalizes a set of vectors
* [[Matrix multiplication]]
** [[Cannon's algorithm]]: a [[distributed algorithm]] for [[matrix multiplication]] especially suitable for computers laid out in an N × N mesh
** [[Coppersmith–Winograd algorithm]]: square [[matrix multiplication]]
** [[Freivald's algorithm]]: a randomized algorithm used to verify matrix multiplication
** [[Strassen algorithm]]: faster [[matrix multiplication]]
* Solving [[system of linear equations|systems of linear equations]]
** [[Biconjugate gradient method]]: solves systems of linear equations
** [[Conjugate gradient]]: an algorithm for the numerical solution of particular systems of linear equations
** [[Gaussian elimination]]
** [[Gauss–Jordan elimination]]: solves systems of linear equations
** [[Gauss–Seidel method]]: solves systems of linear equations iteratively
** [[Levinson recursion]]: solves equation involving a [[Toeplitz matrix]]
** [[Stone's method]]: also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations
** [[Successive over-relaxation]] (SOR): method used to speed up convergence of the [[Gauss–Seidel method]]
** [[Tridiagonal matrix algorithm]] (Thomas algorithm): solves systems of tridiagonal equations
* [[Sparse matrix]] algorithms
** [[Cuthill–McKee algorithm]]: reduce the [[bandwidth (matrix theory)|bandwidth]] of [[Sparse matrix|sparse]] [[symmetric matrices]]
** [[Minimum degree algorithm]]: permute the rows and columns of a symmetric sparse matrix before applying the [[Cholesky decomposition]]
** [[Symbolic Cholesky decomposition]]: Efficient way of storing sparse matrix

==== Monte Carlo ====
{{see|Monte Carlo method}}
* [[Gibbs sampling]]: generate a sequence of samples from the joint probability distribution of two or more random variables
* [[Metropolis–Hastings algorithm]]: used to generate a sequence of samples from the [[probability distribution]] of one or more variables
* [[Wang and Landau algorithm]]: an extension of [[Metropolis–Hastings algorithm]] sampling

==== Root finding ====
{{main|Root-finding algorithm}}
* [[False position method]]: approximates roots of a function
* [[Newton's method]]: finds zeros of functions with [[calculus]]
* [[Secant method]]: approximates roots of a function

===Optimization algorithms===
{{main|Optimization (mathematics)}}
* [[Alpha-beta pruning]]: search to reduce number of nodes in minimax algorithm
* [[Branch and bound]]
* [[Chain matrix multiplication]]
* [[Combinatorial optimization]]: optimization problems where the set of feasible solutions is discrete
** [[Greedy randomized adaptive search procedure]] (GRASP): successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local search
** [[Hungarian method]]: a combinatorial optimization algorithm which solves the [[assignment problem]] in polynomial time
* [[Constraint satisfaction]]
** General algorithms for the constraint satisfaction
*** [[AC-3 algorithm]]
*** [[Difference map algorithm]]
*** [[Min conflicts algorithm]]
** [[Chaff algorithm]]: an algorithm for solving instances of the boolean satisfiability problem
** [[Davis–Putnam algorithm]]: check the validity of a first-order logic formula 
** [[DPLL algorithm|Davis-Putnam-Logemann-Loveland algorithm]] (DPLL): an algorithm for deciding the satisfiability of propositional logic formulae in conjunctive normal form, i.e. for solving the [[CNF-SAT]] problem
** [[Exact cover]] problem
*** [[Algorithm X]]: a [[nondeterministic algorithm]]
*** [[Dancing Links]]: an efficient implementation of Algorithm X
* [[Cross-entropy method]]: a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and [[importance sampling]]
* [[Differential evolution]]
* [[Dynamic Programming]]: problems exhibiting the properties of [[overlapping subproblem]]s and [[optimal substructure]]
* [[Ellipsoid method]]: is an algorithm for solving convex optimization problems
* [[Evolutionary computation]]: optimization inspired by biological mechanisms of evolution
** [[Evolution strategy]]
** [[Genetic algorithms]]
*** [[Fitness proportionate selection]] - also known as roulette-wheel selection
*** [[Stochastic universal sampling]]
*** [[Truncation selection]]
*** [[Tournament selection]]
** [[Memetic algorithm]]
** [[Swarm intelligence]]
*** [[Ant colony optimization]]
*** [[Bees algorithm]]: a search algorithm which mimics the food foraging behaviour of swarms of honey bees
*** [[Particle swarm optimization|Particle swarm]]
* [[Gradient descent]]
* [[Harmony search]] (HS): a [[metaheuristic]] algorithm mimicking the improvisation process of musicians
* [[Interior point method]]
* [[Linear programming]]
** [[Dantzig-Wolfe decomposition]]: an algorithm for solving linear programming problems with special structure
** [[Delayed column generation]]
** [[Integer linear programming]]: solve linear programming problems where some or all the unknowns are restricted to integer values
*** [[Branch and cut]]
*** [[Cutting-plane method]]
** [[Karmarkar's algorithm]]: The first reasonably efficient algorithm that solves the [[linear programming]] problem in [[polynomial time]].
** [[Simplex algorithm]]: An algorithm for solving the [[linear programming]] problem
* [[Line search]]
* [[Local search (optimization)|Local search]]: a metaheuristic for solving computationally hard optimization problems
** [[Random-restart hill climbing]]
** [[Tabu search]]
* [[Minimax#Minimax_algorithm_with_alternate_moves|Minimax]] used in game programming
* [[Nearest neighbor search]] (NNS): find closest points in a [[metric space]]
** [[Best Bin First]]: find an approximate solution to the [[Nearest neighbor search]] problem in very high dimensional spaces
* [[Newton's method in optimization]]
* [[Nonlinear optimization]]
** [[BFGS method]]: A [[nonlinear optimization]] algorithm
** [[Gauss–Newton algorithm]]: An algorithm for solving nonlinear [[least squares]] problems.
** [[Levenberg–Marquardt algorithm]]: An algorithm for solving nonlinear [[least squares]] problems.
** [[Nelder-Mead method]] (downhill simplex method): A [[nonlinear optimization]] algorithm
* [[Odds algorithm]] (Bruss algorithm) : Finds the optimal strategy to predict a last specific event in a random sequence event
* [[Simulated annealing]]
* [[Stochastic tunneling]]
* [[Subset sum problem|Subset sum]] algorithm

==Computational science==
{{see|Computational science}}
===Astronomy===
{{main|Astronomical algorithms}}
* [[Doomsday algorithm]]: day of the week
* [[Zeller's congruence]] is an algorithm to calculate the day of the week for any Julian or Gregorian calendar date

===Bioinformatics===
{{see|Bioinformatics}}
{{see also|List of algorithms#Sequence alignment|l1=Sequence alignment algorithms}}
* [[Basic Local Alignment Search Tool]] also known as BLAST: an algorithm for comparing primary biological sequence information
* [[Kabsch algorithm]]: calculate the optimal alignment of two sets of points in order to compute the [[RMSD|root mean squared deviation]] between two protein structures.
* [[Velvet (algorithm)|Velvet]]: a set of algorithms manipulating [[de Bruijn graph]]s for genomic [[sequence assembly]]

===Geoscience===
{{see|Geoscience}}
* [[Vincenty's formulae]]: a fast algorithm to calculate the distance between two latitude/longitude points on an ellipsoid

===Linguistics===
{{see|Computational linguistics|Natural language processing}}
* [[Lesk algorithm]]: word sense disambiguation
* [[Stemming|Stemming algorithm]]: a method of reducing words to their stem, base, or root form
* [[Sukhotins Algorithm]]: a statistical classification algorithm for classifying characters in a text as vowels or consonants

===Medicine===
{{main|Medical algorithms}}
* [[Texas Medication Algorithm Project]]

===Physics===
{{see|Computational physics}}
* [[Constraint algorithm]]: a class of algorithms for satisfying constraints for bodies that obey Newton's equations of motion
* [[Demon algorithm]]:  a [[Monte Carlo method]] for efficiently sampling members of a [[microcanonical ensemble]] with a given energy
* [[Featherstone's algorithm]]: compute the effects of forces applied to a structure of joints and links
* [[Fast multipole method]] (FMM): speed up the calculation of long-ranged forces in the [[n-body problem]]
* [[Rainflow-counting algorithm]]: Reduces a complex [[stress (physics)|stress]] history to a count of elementary stress-reversals for use in [[fatigue (material)|fatigue]] analysis
* [[Sweep and prune]]: a broad phase algorithm used during [[collision detection]] to limit the number of pairs of solids that need to be checked for collision
* [[VEGAS algorithm]]: a method for reducing error in [[Monte Carlo simulation]]s

===Statistics===
{{see|Computational statistics}}
* [[Algorithms for calculating variance]]: avoiding instability and numerical overflow
* [[Approximate counting algorithm]]: Allows counting large number of events in a small register
* [[Bayesian statistics]]
** [[Nested sampling algorithm]]: a computational approach to the problem of comparing models in Bayesian statistics
* [[Data clustering|Clustering Algorithms]]
** [[Canopy clustering algorithm]]:  an unsupervised clustering algorithm related to the K-means algorithm
** [[Fuzzy clustering]]: a class of clustering algorithms where each point has a degree of belonging to clusters
*** [[Cluster_analysis#Fuzzy_c-means_clustering|Fuzzy c-means]]
*** [[FLAME clustering]] (Fuzzy clustering by Local Approximation of MEmberships): define clusters in the dense parts of a dataset and perform cluster assignment solely based on the neighborhood relationships among objects
** [[k-means algorithm]]: cluster objects based on attributes into partitions
** [[k-medoids]]: similar to k-means, but chooses datapoints or [[medoid]]s as centers
** [[Linde-Buzo-Gray algorithm]]: a vector quantization algorithm to derive a good codebook
** [[Lloyd's algorithm]] (Voronoi iteration or relaxation): group data points into a given number of categories.
** [[Single-linkage clustering]]: a simple agglomerative clustering algorithm
** [[Cluster analysis#QT clustering algorithm|QT clustering]]: partitions without the number of clusters  a priori
* [[Estimation theory|Estimation Theory]]
** [[Expectation-maximization algorithm]] A class of related algorithms for finding maximum likelihood estimates of parameters in probabilistic models
*** [[Ordered subset expectation maximization]] (OSEM): used in [[medical imaging]] for [[positron emission tomography]], [[single photon emission computed tomography]] and [[X-ray]] computed tomography.
** [[Odds algorithm]] (Bruss algorithm) Optimal online search for distinguished value in sequential random input
** [[Kalman filter]]: estimate the state of a [[Dynamical system|dynamic system]] from a series of noisy measurements
* [[FNN algorithm|False nearest neighbor algorithm]] (FNN) estimates [[fractal dimension]]
* [[Hidden Markov Model]]
** [[Baum-Welch algorithm]]: compute maximum likelihood estimates and [[Maximum a posteriori|posterior mode]] estimates for the parameters of a [[hidden markov model]]
** [[Forward-backward algorithm]] a dynamic programming algorithm for computing the probability of a particular observation sequence
** [[Viterbi algorithm]]: find the most likely sequence of hidden states in a [[hidden markov model]]
* [[Partial least squares regression]]:  finds a linear model describing some predicted variables in terms of other observable variables
* [[Queuing theory]]
** [[Buzen's algorithm]]: an algorithm for calculating the normalization constant G(K) in the [[Gordon–Newell theorem]]
* [[RANSAC]] (an abbreviation for &quot;RANdom SAmple Consensus&quot;): an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers
* [[Scoring algorithm]]: is a form of [[Newton's method]] used to solve [[maximum likelihood]] equations numerically
* [[Yamartino method]]: calculate an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data
* [[Ziggurat algorithm]]: generate random numbers from a non-uniform distribution

==Computer science==
{{see|Computer science}}

===Computer architecture===
{{see|Computer architecture}}
* [[Tomasulo algorithm]]: allows sequential instructions that would normally be stalled due to certain dependencies to execute non-sequentially

===Computer graphics===
{{see|Computer graphics}}
* [[Clipping (computer graphics)|Clipping]]
** [[Line clipping]]
*** [[Cohen-Sutherland]]
*** [[Liang-Barsky]]
*** [[Line clipping#Fast-Clipping|Fast-Clipping]]
*** [[Line clipping#Cyrus-Beck|Cyrus-Beck]]
*** [[Nicholl-Lee-Nicholl]]
** Polygon Clipping
*** [[Sutherland-Hodgman]]
*** [[Weiler-Atherton]]
* [[Contour line]]s and [[Isosurface]]s
** [[Marching cubes]]: extract a polygonal mesh of an isosurface from a three-dimensional scalar field (sometimes called voxels)
** [[Marching squares]]: generate contour lines for a two-dimensional scalar field
** [[Marching tetrahedrons]]: an alternative to [[Marching cubes]]
* [[Flood fill]]: fills a connected region of a multi-dimensional array with a specified symbol
* [[Global illumination]] algorithms: Considers direct illumination and reflection from other objects.
** [[Radiosity]] 
** [[Ray tracing (graphics)|Ray tracing]]
** [[Beam tracing]]
** [[Cone tracing]]
** [[Path tracing]]
** [[Metropolis light transport]]
** [[Ambient occlusion]]
** [[Photon mapping]]
** [[Image based lighting]]
* [[Hidden surface determination|Hidden surface removal]] or [[Hidden surface determination|Visual surface determination]]
** [[Painter's algorithm]]: detects visible parts of a 3-dimensional scenery
** [[Newell's algorithm]]: eliminate polygon cycles in the depth sorting required in hidden surface removal
** [[Scanline rendering]]: constructs an image by moving an imaginary line over the image
** [[Warnock algorithm]]
* [[Line drawing algorithm|Line Drawing]]: graphical algorithm for approximating a line segment on discrete graphical media.
** [[Bresenham's line algorithm]]: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses decision variables)
** [[Digital Differential Analyzer (graphics algorithm)|DDA line algorithm]]: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses floating-point math)
** [[Xiaolin Wu's line algorithm]]: algorithm for line antialiasing.
* [[Midpoint circle algorithm]]: an algorithm used to determine the points needed for drawing a circle
* [[Ramer-Douglas-Peucker algorithm]]: Given a 'curve' composed of line segments to find a curve not too dissimilar but that has fewer points
* [[Shading]]
** [[Gouraud shading]]: an algorithm to simulate the differing effects of light and colour across the surface of an object in 3D computer graphics
** [[Phong shading]]: an illumination model and an interpolation method in 3D computer graphics
* [[Slerp]] (spherical linear interpolation): quaternion interpolation for the purpose of animating 3D rotation
* [[Summed Area Table]] (also known as an Integral Image): is an algorithm for computing sum of values in a rectangular subset of a grid in constant time

===Cryptography===
{{see|Cryptography|Topics in cryptography}}

* [[Asymmetric key algorithm|Asymmetric (public key) encryption]]:
** [[Digital Signature Algorithm|DSA]]
** [[ElGamal encryption|ElGamal]]
** [[NTRUEncrypt]]
** [[RSA]]
* Cryptographic [[Message digest]] functions (hashing functions):
** [[keyed-hash message authentication code|HMAC]]: keyed-hash message authentication
** [[MD5]] – Note that there is now a method of generating collisions for MD5
** [[RIPEMD-160]]
** [[SHA-1]]
** [[Tiger (hash)|Tiger]] (TTH), usually used in [[Hash tree|Tiger tree hashes]]
* [[Cryptographically secure pseudo-random number generator]]s
** [[Blum Blum Shub]] - based on the hardness of [[integer factorization|factorization]]
** [[Fortuna (PRNG)|Fortuna]], intended as an improvement on [[Yarrow algorithm]]
** [[Linear feedback shift register]]
** [[Yarrow algorithm]]
* Key exchange
** [[Diffie-Hellman key exchange]]
*[[Secret sharing]], Secret Splitting, Key Splitting, M of N algorithms
** Blakey's Scheme
** [[Shamir's Secret Sharing|Shamir's Scheme]]
* [[symmetric key algorithm|Symmetric (secret key) encryption]]:
** [[Advanced Encryption Standard]] (AES), winner of [[NIST]] competition, also known as [[Rijndael]]
** [[Blowfish (cipher)|Blowfish]]
** [[Data Encryption Standard]] (DES), sometimes DE Algorithm, winner of NBS selection competition, replaced by AES for most purposes
** [[International Data Encryption Algorithm|IDEA]]
** [[RC4 (cipher)]]
** [[Tiny Encryption Algorithm]]

===Digital logic===
* [[Boolean minimization]]
** [[Quine&amp;ndash;McCluskey algorithm]]: Also called as Q-M algorithm, programmable method for simplyfying the boolean equations.
** [[Petrick's method]]: Another algorithm for boolean simplification.
** [[Minilog|Espresso heuristic logic minimization]]: Fast algorithm for boolean function minimization.

===Machine learning and statistical classification ===
{{see|Machine Learning|Statistical classification}}
* [[ALOPEX]]: a correlation based [[Machine learning|machine learning algorithm]]
* [[Association rule learning]]: discover interesting relations between variables, used in [[Data mining]]
** [[Apriori algorithm]]
** [[Association rule learning#Eclat algorithm|Eclat algorithm]]
** [[Association rule learning#FP-Growth algorithm|FP-Growth algorithm]]
** [[One-attribute-rule]]
** [[Association rule learning#Zero-attribute-rule|Zero-attribute-rule]]
* [[Boosting]]: Use many weak learners to boost effectiveness
** [[AdaBoost]]: adaptive boosting
** [[BrownBoost]]:a boosting algorithm that may be robust to noisy datasets
** [[LogitBoost]]: [[logistic regression]] boosting
** [[LPBoost]]: [[linear programming]] boosting
* [[Bootstrap aggregating]] (bagging): technique to improve stability and classification accuracy
* [[Decision tree learning|Decision Trees]]
** [[C4.5 algorithm]]: an extension to ID3
** [[ID3 algorithm]] (Iterative Dichotomiser 3):  Use heuristic to generate small decision trees
* [[k-nearest neighbors]] (k-NN): a method for classifying objects based on closest training examples in the [[feature space]]
* [[Linde-Buzo-Gray algorithm]]:  a vector quantization algorithm used to derive a good codebook
* [[Locality Sensitive Hashing]] (LSH): a method of performing probabilistic dimension reduction of high-dimensional data.
* [[Artificial neural network|Neural Network]]
** [[Backpropagation]]: A [[supervised learning]] method which requires a teacher that knows, or can calculate, the desired output for any given input
** [[Hopfield net]]:  a [[Recurrent neural network]] in which all connections are symmetric
** [[Perceptron]]: the simplest kind of feedforward neural network: a [[linear classifier]].
** [[Pulse-coupled networks|Pulse-Coupled Neural Networks]] (PCNN): [[Neural network|neural models]] proposed by modeling a cat's [[visual cortex]] and developed for high-performance [[Bionics|biomimetic]] image processing.
** [[Radial basis function network]]: an artificial neural network that uses radial [[basis function]]s as activation functions
** [[Self-organizing map]]:  an unsupervised network that produces a low-dimensional representation of the input space of the training samples
* [[Random forest]]: classify using many decision trees
* [[Random multinomial logit]]: classify using repeated [[multinomial logit]] analyses
* [[Reinforcement Learning]]:
** [[Q-learning]]:  learn an action-value function that gives the expected utility of taking a given action in a given state and following a fixed policy thereafter
** [[SARSA]] (State-Action-Reward-State-Action): learn a [[Markov decision process]] policy
** [[Temporal difference learning]]
* [[Relevance Vector Machine]] (RVM): similar to SVM, but provides probabilistic classification
* [[Support Vector Machines]] (SVM): a set of methods which divide multidimensional data by finding a dividing hyperplane with the maximum margin between the two sets
** [[Structured SVM]]:  allows training of a classifier for general structured output labels.
* [[Winnow algorithm]]: related to the perceptron, but uses a multiplicative weight-update scheme

===Programming language theory===
{{see|Programming language theory}}
* [[C3 linearization]]: an algorithm used primarily to obtain a consistent linearization of a multiple inheritance hierarchy in object-oriented programming
* [[Chaitin's algorithm]]: a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric
* [[Type inference#Hindley–Milner type inference algorithm|Hindley–Milner type inference algorithm]]
* [[Rete algorithm]]: an efficient pattern matching algorithm for implementing production rule systems
* [[Sethi-Ullman algorithm]]: generate optimal code for arithmetic expressions

====Parsing====
{{see|Parsing}}
* [[CYK algorithm]]: An O(n&lt;sup&gt;3&lt;/sup&gt;) algorithm for parsing [[context-free grammar]]s in [[Chomsky normal form]]
* [[Earley parser]]: Another O(n&lt;sup&gt;3&lt;/sup&gt;) algorithm for parsing any [[context-free grammar]]
* [[GLR parser]]:An algorithm for parsing any [[context-free grammar]] by [[Masaru Tomita]]. It is tuned for deterministic grammars, on which it performs almost [[linear time]] and O(n&lt;sup&gt;3&lt;/sup&gt;) in worst case.
* [[Inside-outside algorithm]]: An O(n&lt;sup&gt;3&lt;/sup&gt;) algorithm for re-estimating production probabilities in [[probabilistic context-free grammar]]s
* [[LL parser]]: A relatively simple [[linear time]] parsing algorithm for a limited class of [[context-free grammar]]s
* [[LR parser]]: A more complex [[linear time]] parsing algorithm for a larger class of [[context-free grammar]]s.  Variants:
** [[Canonical LR parser]]
** [[Look-ahead LR parser|LALR (Look-ahead LR) parser]]
** [[Operator-precedence parser]]
** [[Simple LR parser|SLR (Simple LR) parser]]
* [[Packrat parser]]: A [[linear time]] parsing algorithm supporting some [[context-free grammar]]s and [[parsing expression grammar]]s
* [[Recursive descent parser]]: A [[top-down parsing|top-down parser]] suitable for LL(''k'') grammars
* [[Shunting yard algorithm]]: convert an infix-notation math expression to postfix

===Quantum algorithms===
{{see|Quantum algorithm}}
* [[Deutsch-Jozsa algorithm]]: criterion of balance for Boolean function
* [[Grover's algorithm]]: provides quadratic speedup for many search problems
* [[Shor's algorithm]]: provides [[exponential function|exponential]] speedup (relative to currently known non-quantum algorithms) for factoring a number
* [[Simon's algorithm]]: provides a provably [[exponential function|exponential]] speedup (relative to any non-quantum algorithm) for a black-box problem

===Theory of computation and automata===
{{see|Theory of computation}}
* [[Powerset construction]]: Algorithm to convert nondeterministic automaton to [[deterministic automaton]].
* [[Tarski–Kuratowski algorithm]]: a [[non-deterministic algorithm]] which provides an upper bound for the complexity of formulas in the [[arithmetical hierarchy]] and [[analytical hierarchy]]

==Information theory and signal processing==
{{main|Information theory|Signal processing}}
===Coding theory===
{{see|Coding theory}}

====Error detection and correction====
{{see|Error detection and correction}}
* [[BCH Code]]s
** [[Berlekamp–Massey algorithm]]
** [[BCH Code#Peterson Gorenstein Zierler algorithm|Peterson-Gorenstein-Zierler  algorithm]]
** [[Reed-Solomon error correction#Sketch of the error correction algorithm|Reed Solomon error correction]]
* [[BCJR algorithm]]: decoding of error correcting codes defined on trellises (principally convolutional codes)
* [[Hamming code]]s
** [[Hamming(7,4)]]:  a [[Hamming code]] that encodes 4 bits of data into 7 bits by adding 3 parity bits
** [[Hamming distance]]: sum number of positions which are different
** [[Hamming weight]] (population count): find the number of 1 bits in a binary word
* [[Redundancy check]]s
** [[Adler-32]]
** [[Cyclic redundancy check]]
** [[Fletcher's checksum]]
** [[Luhn algorithm]]: a method of validating identification numbers
** [[Luhn mod N algorithm]]: extension of Luhn to non-numeric characters
** [[Parity bit|Parity]]: simple/fast error detection technique
** [[Verhoeff algorithm]]
** [[Longitudinal redundancy check]] (LRC)

====Lossless compression algorithms====
{{main|:Category:Lossless compression algorithms|l1=Lossless compression algorithms}}
* [[Burrows-Wheeler transform]]: preprocessing useful for improving [[Lossless data compression|lossless compression]]
* [[Context tree weighting]]
* [[Delta encoding]]: aid to compression of data in which sequential data occurs frequently
* [[Dynamic Markov compression]]: Compression using predictive arithmetic coding
* [[Dictionary coder]]s
** [[Byte pair encoding]] (BPE)
** [[DEFLATE (algorithm)|DEFLATE]]
** [[Lempel-Ziv]]
*** [[LZ77 and LZ78]]
*** [[LZJB|Lempel-Ziv Jeff Bonwick]] (LZJB)
*** [[Lempel-Ziv-Markov chain-Algorithm]] (LZMA)
*** [[Lempel-Ziv-Oberhumer]] (LZO): speed oriented
*** [[Lempel-Ziv-Storer-Szymanski]] (LZSS)
*** [[Lempel–Ziv–Welch]] (LZW)
*** [[LZWL]]: syllable-based variant
*** [[LZX]]
*** [[LZRW|Lempel-Ziv Ross Williams]] (LZRW)
* [[Entropy encoding]]: coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols
** [[Arithmetic coding]]: advanced [[entropy]] coding
*** [[Range encoding]]: same as [[arithmetic coding]], but looked at in a slightly different way
** [[Huffman coding]]: simple lossless compression taking advantage of relative character frequencies
*** [[Adaptive Huffman coding]]: [[adaptive coding]] technique based on Huffman coding
*** [[Package-merge algorithm|Package-Merge]]: Optimizes Huffman coding subject to a length restriction on code strings
** [[Shannon-Fano coding]]
** [[Shannon-Fano-Elias coding]]: precursor to arithmetic encoding[http://www.ece.msstate.edu/~fowler/Classes/ECE8813/Handouts/shannon_fano_elias.pdf]
* [[Entropy encoding|Entropy coding with known entropy characteristics]]
** [[Golomb coding]]: form of entropy coding that is optimal for alphabets following geometric distributions
** [[Rice coding]]: form of entropy coding that is optimal for alphabets following geometric distributions
** [[Truncated binary encoding]]
** [[Unary coding]]: code that represents a number n with n ones followed by a zero
** [[Universal code (data compression)|Universal codes]]: encodes positive integers into binary code words
*** Elias [[Elias delta coding|delta]], [[Elias gamma coding|gamma]], and [[Elias omega coding|omega]] coding
*** [[Exponential-Golomb coding]]
*** [[Fibonacci coding]]
*** [[Levenshtein coding]]
* [[FELICS|Fast Efficient &amp; Lossless Image Compression System]] (FELICS): a lossless image compression algorithm
* [[Incremental encoding]]: delta encoding applied to sequences of strings
* [[PPM compression algorithm|Prediction by partial matching]] (PPM): an adaptive statistical data compression technique based on context modeling and prediction
* [[Run-length encoding]]: lossless data compression taking advantage of strings of repeated characters
* [[SEQUITUR algorithm]]: lossless compression by incremental grammar inference on a string

====Lossy compression algorithms====
{{main|:Category:Lossy compression algorithms|l1=Lossy compression algorithms}}
* [[3Dc]]: a lossy data compression algorithm for [[Normal mapping|normal maps]]
* [[Audio data compression|Audio]] and [[speech encoding|Speech]] compression
** [[A-law algorithm]]: standard companding algorithm
** [[Code excited linear prediction]] (CELP): low bit-rate speech compression
** [[Linear predictive coding]] (LPC): lossy compression by representing the [[spectral envelope]] of a digital signal of speech in compressed form
** [[Mu-law algorithm]]: standard analog signal compression or companding algorithm
** [[Warped Linear Predictive Coding]] (WLPC)
* [[Image Compression]]
** [[Block Truncation Coding]] (BTC): a type of lossy image compression technique for greyscale images
** [[Embedded Zerotree Wavelet]] (EZW)
** [[Fast_Cosine_Transform|Fast Cosine Transform algorithm]]s (FCT algorithms): compute Discrete Cosine Transform (DCT) efficiently
** [[Fractal compression]]: method used to compress images using fractals
** [[Set Partitioning in Hierarchical Trees]] (SPIHT)
** [[Wavelet compression]]: form of data compression well suited for [[image compression]] (sometimes also video compression and audio compression)
* [[Transform coding]]: type of data compression for &quot;natural&quot; data like audio signals or photographic images
* [[Vector quantization]]: technique often used in lossy data compression

===Digital signal processing===
{{see|Digital signal processing}}
* [[Adaptive-additive algorithm]] (AA algorithm): find the spatial frequency phase of an observed wave source
* [[Discrete Fourier transform]]: determines the frequencies contained in a (segment of a) signal
** [[Bluestein's FFT algorithm]]
** [[Bruun's FFT algorithm]]
** [[Cooley&amp;ndash;Tukey FFT algorithm]]
** [[Fast Fourier transform]]
** [[Prime-factor FFT algorithm]]
** [[Rader's FFT algorithm]]
* [[Fast Folding Algorithm]]: an efficient algorithm for the detection of approximately-periodic events within time series data
* [[Gerchberg–Saxton algorithm]]: Phase retrieval algorithm for optical planes
* [[Goertzel algorithm]]: identify a particular frequency component in a signal.  Can be used for [[DTMF]] digit decoding.
* [[Karplus-Strong string synthesis]]: physical modelling synthesis to simulate the sound of a hammered or plucked string or some types of percussion

====Image processing====
{{see|Image processing}}
* [[Connected component labeling]]: find and label disjoint regions
* [[Dithering]] and [[half-toning]]
** [[Error diffusion]]
** [[Floyd–Steinberg dithering]]
** [[Ordered dithering]]
** [[Riemersma dithering]]
* [[Elser Difference-Map Algorithm]]: a search algorithm for general constraint satisfaction problems.  Originally used for [[X-ray crystallography|X-Ray diffraction]] microscopy
* [[Feature detection]]
** [[Canny edge detector]]: detect a wide range of edges in images
** [[Generalised Hough Transform]]
** [[Hough transform]]
** [[Marr-Hildreth algorithm]]: an early [[edge detection]] algorithm
** [[Scale-invariant_feature_transform | SIFT]] (Scale-invariant feature transform): is an algorithm to detect and describe local features in images.
* [[GrowCut algorithm]]: an interactive segmentation algorithm
* [[Richardson–Lucy deconvolution]]: image de-blurring algorithm

==Software engineering==
{{see|Software engineering}}

* [[Unicode Collation Algorithm]]
* [[CHS conversion]]: converting between disk addressing systems
* [[Double dabble]]: Convert binary numbers to BCD
* [[Hash Function]]: convert a large, possibly variable-sized amount of data into a small datum, usually a single integer that may serve as an index into an array
** [[Fowler Noll Vo hash]]: fast with low collision rate
** [[Pearson hashing]]: computes 8 bit value only, optimized for 8 bit computers
** [[Zobrist hashing]]: used in the implementation of [[transposition table]]s
* [[Cache algorithms]]
* [[Xor swap algorithm]]: swaps the values of two variables without using a buffer

===Database algorithms===
{{see|Database}}
* [[Algorithms for Recovery and Isolation Exploiting Semantics]] (ARIES): [[transaction (database)|transaction]] recovery
* [[Join (SQL)|Join algorithms]]
** [[Block nested loop]]
** [[Hash join]]
** [[Nested loop join]]
** [[Sort-Merge Join]]

===Distributed systems algorithms===
{{see|Distributed systems}}
* [[Bully algorithm]]:  a method for dynamically selecting a coordinator
* [[Byzantine fault tolerance]]: good [[Fault-tolerant system|fault tolerance]].
* [[Clock synchronization]]
** [[Berkeley algorithm]]
** [[Cristian's algorithm]]
** [[Intersection algorithm]]
** [[Marzullo's algorithm]]
* [[Lamport ordering]]: a [[partial order]]ing of events based on the ''happened-before'' relation
* [[Mutual exclusion]]
** [[Lamport's Distributed Mutual Exclusion Algorithm]]
** [[Maekawa's Algorithm]]
** [[Raymond's Algorithm]]
** [[Ricart-Agrawala Algorithm]]
* [[Paxos algorithm]]: a family of protocols for solving consensus in a network of unreliable processors
* [[Snapshot algorithm]]: record a consistent global state for an asynchronous system
* Detection of Process Termination
** [[Dijkstra-Scholten algorithm]]
** [[Huang's algorithm]]
* [[Vector clocks]]: generate a [[partial ordering]] of events in a distributed system and detect [[causality]] violations

===Memory allocation and deallocation algorithms===
* [[Buddy memory allocation]]: Algorithm to allocate memory such that fragmentation is less.
* [[Garbage collection (computer science)|Garbage collectors]]
** [[Boehm garbage collector]]: Conservative garbage collector
** [[Cheney's algorithm]]: An improvement on the [[Semi-space collector]]
** [[garbage collection (computer science)|Generational garbage collector]]: Fast garbage collectors that segregate memory by age
** [[Mark-compact algorithm]]:  a combination of the [[Mark and sweep|mark-sweep algorithm]]  and [[Cheney's algorithm| Cheney's copying algorithm]]
** [[Mark and sweep]]
** [[Semi-space collector]]: An early copying collector
* [[Reference counting]]

===Operating systems algorithms===
{{see|Operating systems}}
* [[Banker's algorithm]]: Algorithm used for deadlock avoidance.
* [[Page replacement algorithms]]: Selecting the victim page under low memory conditions.
** [[Adaptive replacement cache]]: better performance than LRU
** [[Clock with Adaptive Replacement]] (CAR): is a page replacement algorithm that has performance comparable to [[Adaptive replacement cache]]

====Disk scheduling====
{{see|Disk scheduling}}
* [[Elevator algorithm]]: Disk scheduling algorithm that works like an elevator.
* [[Shortest seek first]]: Disk scheduling algorithm to reduce [[seek time]].

====Networking====
{{see|Computer Network}}
* [[Karn's Algorithm]]: addresses the problem of getting accurate estimates of the round-trip time for messages when using TCP
* [[Luleå algorithm]]: a technique for storing and searching internet routing tables efficiently
* [[Network congestion]]
** [[Exponential backoff]]
** [[Nagle's algorithm]]: improve the efficiency of TCP/IP networks by coalescing packets
** [[Truncated binary exponential backoff]]
* [[Traffic shaping]] and [[Rate limiting]] 
** [[Leaky bucket]]
** [[Token bucket]]

====Process synchronization====
{{see|Process synchronization}}
* [[Dekker's algorithm]]
* [[Lamport's Bakery algorithm]]
* [[Peterson's algorithm]]

====Scheduling====
{{see|Scheduling (computing)}}

* [[Earliest deadline first scheduling]]
* [[Fair-share scheduling]]
* [[Least slack time scheduling]]
* [[List scheduling]]
* [[Multi level feedback queue]]
* [[Rate-monotonic scheduling]]
* [[Top-nodes algorithm]]: resource calendar management
* [[Round-robin scheduling]]
* [[shortest job next]]
* [[shortest remaining time]]

==Other==
*[[Rutishauser's algorithm]]

==See also==
*[[Algorithm]]
*[[Heuristic]]

[[Category:Algorithms|*]]
[[Category:Mathematics-related lists|Algorithms]]

[[de:Liste von Algorithmen]]
[[et:Algoritmide loend]]
[[fr:Liste des algorithmes]]
[[hi:कलन-विधियों की सूची]]
[[id:Daftar algoritma]]
[[pt:Anexo:Lista de algoritmos]]
[[ru:Список алгоритмов]]
[[sr:Списак алгоритама]]
[[tg:Рӯихати алгоритмҳо]]
[[tr:Algoritma listesi]]
[[uk:Список алгоритмів]]</text>
    </revision>
  </page>
  <page>
    <title>Grafting (algorithm)</title>
    <id>21677830</id>
    <revision>
      <id>310332114</id>
      <timestamp>2009-08-27T09:16:37Z</timestamp>
      <contributor>
        <username>Mark Renier</username>
        <id>9139161</id>
      </contributor>
      <minor/>
      <comment>Redirect bypass from [[Ordered tree]] to [[Tree (graph theory)]] using [[:en:WP:POP|popups]]</comment>
      <text xml:space="preserve">In [[computer science]], '''grafting''' is a method used to manipulate trees. One such tree is an [[Tree (graph theory)|ordered tree]], which is where the subtrees for any node are ordered. Let root(''T''&lt;sub&gt;1&lt;/sub&gt;),&amp;nbsp;...,&amp;nbsp;root(''T''&lt;sub&gt;''n''&lt;/sub&gt;) be the children of root(''T'') and root(''T''&lt;sub&gt;''i''&lt;/sub&gt;) be the ''i''th child. A suitable representation of ordered trees is to make them a [[rooted binary tree]], where each node is stored in the same amount of memory &lt;ref&gt;http://acm.pku.edu.cn/JudgeOnline/problem?id=3437 Peking University&lt;/ref&gt;.

The conversion to a rooted binary tree root(''T'') is:
 1. For each child of root(''T''), remove all the edges from the child to the parent.
 2. For each node:
   a. Add an edge to the first child (if one exists) as the left child.
   b. Add an edge to the next sibling (if one exists) as the right child.

Grafting can identify regions where there are no occupancies and correct the poor class assignments to increase accuracy. The extension to graft multiple branches at each leaf reduces the number of errors. &lt;ref&gt;http://www.answers.com/topic/grafting-computer Grafting (computer)&lt;/ref&gt;

==See also==

*[[Pruning (algorithm)]]

==References==

&lt;References /&gt;

[[Category:Trees (graph theory)]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Streaming algorithm</title>
    <id>21759411</id>
    <revision>
      <id>317494337</id>
      <timestamp>2009-10-02T16:40:06Z</timestamp>
      <contributor>
        <ip>132.228.195.207</ip>
      </contributor>
      <text xml:space="preserve">In [[computer science]], '''streaming algorithms''' are algorithms for
processing data streams in which the input is presented as a sequence of
items and can be examined in only a few passes (typically just one). These
algorithms have limited memory available to them (much less than the input
size) and also limited processing time per item.

==History==

Though streaming algorithms had already been studied by Munro and
Paterson&lt;ref&gt;{{Harvtxt|Munro|Paterson|1980}}&lt;/ref&gt; as well as Flajolet and
Martin&lt;ref&gt;{{Harvtxt|Flajolet|Martin|1985}}&lt;/ref&gt;, the field of streaming
algorithms was first formalized and popularized in a paper by [[Noga Alon]],
[[Yossi Matias]], and [[Mario Szegedy]].&lt;ref&gt;{{Harvtxt|Alon|Matias|Szegedy|1996}}&lt;/ref&gt; 
For this paper, the authors later won the [[Gödel Prize]] in 2005 &quot;for their foundational
contribution to streaming algorithms.&quot; There has since been a large body
of work centered around data streaming algorithms that spans a diverse
spectrum of computer science fields such as theory, databases, networking,
and natural language processing.

==Models==

Much of the streaming literature is concerned with computing statistics on
frequency distributions that are too large to be stored. For this class of
problems, there is a vector &lt;math&gt;\mathbf{a} = (a_1, \dots, a_n)&lt;/math&gt;
(initialized to the zero vector &lt;math&gt;\mathbf{0}&lt;/math&gt;) that has updates
presented to it in a stream. The goal of these algorithms is to compute
functions of &lt;math&gt;\mathbf{a}&lt;/math&gt; using considerably less space than it
would take to represent &lt;math&gt;\mathbf{a}&lt;/math&gt; precisely. There are two
common models for updating such streams, called the &quot;cash register&quot; and
&quot;turnstile&quot; models.&lt;ref
name=&quot;turnstile&quot;&gt;{{harvtxt|Gilbert|Kotidis|Muthukrishnan|Strauss|2001}}&lt;/ref&gt;

In the cash register model each update is of the form &lt;math&gt;\langle i,
c\rangle&lt;/math&gt;, so that &lt;math&gt;a_i&lt;/math&gt; is incremented by some positive
integer &lt;math&gt;c&lt;/math&gt;. A notable special case is when &lt;math&gt;c = 1&lt;/math&gt;
(only unit insertions are permitted).

In the turnstile model each update is of the form &lt;math&gt;\langle i,
c\rangle&lt;/math&gt;, so that &lt;math&gt;a_i&lt;/math&gt; is incremented by some (possible
negative) integer &lt;math&gt;c&lt;/math&gt;. In the &quot;strict turnstile&quot; model, no
&lt;math&gt;a_i&lt;/math&gt; at any time may be less than zero.

Several papers also consider the &quot;sliding window&quot; model. In this model,
the function of interest is computing over a fixed-size window in the
stream. As the stream progresses, items from the end of the window are
removed from consideration while new items from the stream take their
place.

Besides the above frequency-based problems, some other types of problems
have also been studied. Many graph problems are solved in the setting
where the [[adjacency matrix]] or the [[adjacency list]] of the graph is streamed in
some unknown order. There are also some problems that are very dependent
on the order of the stream (i.e., asymmetric functions), such as counting
the number of inversions in a stream and finding the longest increasing
subsequence.

==Applications==

Streaming algorithms have several applications in [[Computer network|networking]] such as
monitoring network links for [[elephant flow]]s, counting the number of
distinct flows, estimating the distribution of flow sizes, and so
on.&lt;ref&gt;{{Harvtxt|Xu|2007}}&lt;/ref&gt; They also have applications in
databases, such estimating the size of a [[Join (SQL)|join]].
&lt;!--

===Flowsize distribution and subpopulation flowsize distribution===

===Traffic and flow matrix estimation===

===Super-spreaders===

--&gt;

==Some streaming problems==

===Frequency moments===

The &lt;math&gt;k&lt;/math&gt;th frequency moment of a set of frequencies
&lt;math&gt;\mathbf{a}&lt;/math&gt; is defined as &lt;math&gt;F_k(\mathbf{a}) = \sum_{i=1}^n
a_i^k&lt;/math&gt;.

The first moment &lt;math&gt;F_1&lt;/math&gt; is simply the sum of the frequencies
(i.e., the total count). The second moment &lt;math&gt;F_2&lt;/math&gt;is useful for
computing statistical properties of the data, such as the Gini coefficient
of variation. &lt;math&gt;F_{\infty}&lt;/math&gt; is defined as the frequency of the
most frequent item(s).

The seminal paper of Alon, Matias, and Szegedy paper dealt with the
problem of estimating the frequency moments.

===Heavy hitters===

Find all elements &lt;math&gt;i&lt;/math&gt; whose frequency &lt;math&gt;a_i &gt; T&lt;/math&gt;, say. Some
notable algorithms are:
* Karp-Papadimitriou-Shenker algorithm
* Count-min sketch
* Sticky sampling
* Lossy counting
* Sample and Hold
* Multi-stage Bloom filters
* Count-sketch
* Sketch-guided sampling

===Counting distinct elements===

Counting the number of distinct elements in a stream (sometimes called the
&lt;math&gt;F_0&lt;/math&gt; moment) is another problem that has been well studied.
The first algorithm for it was proposed by Flajolet and Martin, and the
currently best known  algorithm is due to Bar-Yossef et al.

===Entropy===

The (empirical) entropy of a set of frequencies &lt;math&gt;\mathbf{a}&lt;/math&gt; is
defined as &lt;math&gt;F_k(\mathbf{a}) = \sum_{i=1}^n
\frac{a_i}{m}\log{a_i/m}&lt;/math&gt;, where &lt;math&gt;m = \sum_{i=1}^n a_i&lt;/math&gt;.

Estimation of this quantity in a stream has been done by:
* McGregor et al.
* Do Ba et al.
* Lall et al.
* Chakraborty et al.
&lt;!--

===Wavelets===

===Counting inversions===

===Longest increasing subsequence==

===Graph problems===
Counting triangles

===Natural language summarization===

===Computational geometry===

streaming convex hulls - Hershberger Suri

===MUD model===

--&gt;

==Lower bounds==

Lower bounds have been computed for many of the data streaming problems
that have been studied. By far, the most common technique for computing
these lower bounds has been using [[communication complexity]].

==Notes==
&lt;references/&gt;

==References==
* {{citation
  | title = Surfing Wavelets on Streams: One-Pass Summaries for Approximate Aggregate Queries
  | last1=Gilbert|first1=A.C. 
  | last2=Kotidis|first2=Y. 
  | last3=Muthukrishnan|first3=S. 
  | last4=Strauss|first4=M.J.
  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.7062
  | journal = Proceedings of the International Conference on Very Large Data Bases
  | pages = 79&amp;ndash;88
  | year = 2001}}.

* {{citation
  | title = A Tutorial on Network Data Streaming
  | last1 = Xu | first1=Jun (Jim)
  | url = http://www.cc.gatech.edu/%7Ejx/reprints/talks/sigm07_tutorial.pdf
  | year = 2007}}. From [[SIGMETRICS]] 2007.

* {{citation
  | title = Data streaming algorithms for estimating entropy of network traffic
  | last1 = Lall, A., Sekar, V., Ogihara, M., Xu, J., and Zhang, H.
  | journal = Proc. ACM SIGMETRICS
  | year = 2006.}}

* {{citation
  | title = A Simple Algorithm for Finding Frequent Elements in Streams and Bags
  | last1=Karp|first1=R.M.| last2=Papadimitriou|first2=C.H. | last3=Shenker|first3=S.
  | year = 2003
  | journal = Transactions on Database Systems}}

==External links==

* [http://www.cs.princeton.edu/~moses/papers/stream-kmedian.ps Princeton Lecture Notes]
* [http://people.csail.mit.edu/indyk/GEOSTREAM/geostream-bib.ps Streaming Algorithms for Geometric Problems], by [[Piotr Indyk]], MIT
* [http://www.dagstuhl.de/de/program/calendar/semhp/?semnr=05291 Dagstuhl Workshop on Sublinear Algorithms]
* [http://www.cse.iitk.ac.in/users/sganguly/workshop.html IIT Kanpur Workshop on Data Streaming]
* [http://www.cs.umass.edu/~mcgregor/papers/07-openproblems.pdf List of open problems in streaming] (compiled by [[Andrew McGregor]]) from discussion at the IITK Workshop on Algorithms for Data Streams, 2006.

;Tutorials and surveys

* [http://www.cs.rutgers.edu/~muthu/stream-1-1.ps Data Stream Algorithms and Applications] by [[S. Muthu Muthukrishnan]]
* [http://ilpubs.stanford.edu:8090/535/1/2002-19.pdf Stanford STREAM project survey]
* [http://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf Network Applications of Bloom filters], by Broder and Mitzenmacher
* [http://www.cc.gatech.edu/%7Ejx/reprints/talks/sigm07_tutorial.pdf Xu's SIGMETRICS 2007 tutorial]
* [http://www.cs.mcgill.ca/~denis/notes09.pdf Lecture notes from Data Streams course at Barbados in 2009], by [[Andrew McGregor]] and [[S. Muthu Muthukrishnan]]

;Courses

* [http://stellar.mit.edu/S/course/6/fa07/6.895/ MIT]
* [http://dsp.rice.edu/courses/elec631 Rice]
* [http://www.cs.rutgers.edu/~muthu/str05.html Rutgers]
* [http://www.cse.buffalo.edu/~atri/courses/data-stream/ University at Buffalo]


{{Compu-stub}}
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:External memory algorithms</title>
    <id>21813091</id>
    <revision>
      <id>275037077</id>
      <timestamp>2009-03-04T23:12:09Z</timestamp>
      <contributor>
        <username>Itai</username>
        <id>17456</id>
      </contributor>
      <minor/>
      <text xml:space="preserve">'''External memory algorithms''' are algorithms that are efficient when accessing most of the data is very slow, such as, on disk.

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Algorithm</title>
    <id>775</id>
    <revision>
      <id>328057307</id>
      <timestamp>2009-11-26T16:05:55Z</timestamp>
      <contributor>
        <ip>129.102.65.58</ip>
      </contributor>
      <comment>/* Mechanical contrivances with discrete states */</comment>
      <text xml:space="preserve">[[Image:LampFlowchart.svg|thumb|right|This is an algorithm that tries to figure out why the lamp doesn't turn on and tries to fix it using the steps. [[Flowchart]]s are often used to graphically represent algorithms.]] 

In [[mathematics]], [[computing]], and related subjects, an '''algorithm''' is an [[effective method]] for solving a problem using a finite sequence of instructions. Algorithms are used for [[calculation]], [[data processing]], and many other fields.  

Each algorithm is a list of well-defined instructions for completing a task. Starting from an initial state, the instructions describe a computation that proceeds through a well-defined series of successive states, eventually terminating in a final ending state. The transition from one state to the next is not necessarily [[deterministic]]; some algorithms, known as [[randomized algorithms]], incorporate randomness.

A partial formalization of the concept began with attempts to solve the [[Entscheidungsproblem]] (the &quot;decision problem&quot;) posed by [[David Hilbert]] in 1928. Subsequent formalizations were framed as attempts to define &quot;[[effective calculability]]&quot;&lt;ref&gt;Kleene 1943 in Davis 1965:274&lt;/ref&gt; or &quot;effective method&quot;&lt;ref&gt;Rosser 1939 in Davis 1965:225&lt;/ref&gt;; those formalizations included the [[Kurt Gödel|Gödel]]-[[Jacques Herbrand|Herbrand]]-[[Stephen Cole Kleene|Kleene]] [[Recursion (computer science)|recursive function]]s of 1930, 1934 and 1935, [[Alonzo Church]]'s [[lambda calculus]] of 1936, [[Emil Post]]'s &quot;[[Formulation 1]]&quot; of 1936, and [[Alan Turing]]'s [[Turing machines]] of 1936–7 and 1939.

==Etymology==
[[Al-Khwārizmī]], [[Persian people|Persian]] [[astronomer]] and [[mathematician]], wrote a [[treatise]] in 825 AD, ''On Calculation with Hindu Numerals''. (See [[algorism]]). It was translated into [[Latin]] in the 12th century as ''Algoritmi de numero Indorum'' (al-Daffa 1977), whose title is supposedly likely intended to mean &quot;Algoritmi on the numbers of the Indians&quot;, where &quot;Algoritmi&quot; was the translator's rendition of the author's name; but people misunderstanding the title treated ''Algoritmi'' as a Latin plural and this led to the word &quot;algorithm&quot; (Latin ''algorismus'') coming to mean &quot;calculation method&quot;. The intrusive &quot;th&quot; is most likely due to a [[false cognate]] with the Greek {{lang|grc|ἀριθμός}} (''arithmos'') meaning &quot;numbers&quot;.

== Why algorithms are necessary: an informal definition ==
:''For a detailed presentation of the various points of view around the definition of &quot;algorithm&quot; see [[Algorithm characterizations]]. For examples of simple addition algorithms specified in the detailed manner described in [[Algorithm characterizations]], see [[Algorithm examples]].''
While there is no generally accepted ''formal'' definition of &quot;algorithm&quot;, an informal definition could be &quot;a process that performs some sequence of operations.&quot; For some people, a program is only an algorithm if it stops eventually. For others, a program is only an algorithm if it stops before a given number of calculation steps.

A prototypical example of an algorithm is [[Euclid's algorithm]] to determine the maximum common divisor of two integers.

We can derive clues to the issues involved and an informal meaning of the word from the following quotation from {{Harvtxt|Boolos|Jeffrey|1974, 1999}} (boldface added):

&lt;blockquote&gt;No human being can write fast enough, or long enough, or small enough† ( †&quot;smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons&quot;) to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give '''explicit instructions for determining the ''n''th member of the set''', for arbitrary finite ''n''. Such instructions are to be given quite explicitly, in a form in which '''they could be followed by a computing machine''', or by a '''human who is capable of carrying out only very elementary operations on symbols'''&lt;ref&gt;Boolos and Jeffrey 1974,1999:19&lt;/ref&gt;&lt;/blockquote&gt;

The words &quot;enumerably infinite&quot; mean &quot;countable using integers perhaps extending to infinity.&quot; Thus Boolos and Jeffrey are saying that an algorithm ''implies'' instructions for a process that &quot;creates&quot; output integers from an ''arbitrary'' &quot;input&quot; integer or integers that, in theory, can be chosen from 0 to infinity. Thus we might expect an algorithm to be an algebraic equation such as '''y = m + n''' — two arbitrary &quot;input variables&quot; '''m''' and '''n''' that produce an output '''y'''. As we see in [[Algorithm characterizations]] — the word algorithm implies much more than this, something on the order of (for our addition example):
:Precise instructions (in language understood by &quot;the computer&quot;) for a &quot;fast, efficient, good&quot; ''process'' that specifies the &quot;moves&quot; of &quot;the computer&quot; (machine or human, equipped with the necessary internally-contained information and capabilities) to find, decode, and then munch arbitrary input integers/symbols '''m''' and '''n''', symbols '''+''' and '''=''' ... and (reliably, correctly, &quot;effectively&quot;) produce, in a &quot;reasonable&quot; [[time]], output-integer '''y''' at a specified place and in a specified format.

The concept of ''algorithm'' is also used to define the notion of [[decidability (logic)|decidability]]. That notion is central for explaining how [[formal system]]s come into being starting from a small set of [[axiom]]s and rules. In [[logic]], the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of ''algorithm'' that suits both concrete (in some sense) and abstract usage of the term.

== Formalization ==
&lt;!-- If you change this heading's title, [[Computer program]] links here. --&gt;
Algorithms are essential to the way [[computer]]s process information.  Many [[computer program]]s contain algorithms that specify the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees’ paychecks or printing students’ report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a [[Turing completeness|Turing-complete]] system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):
&lt;blockquote&gt; Minsky: &quot;But we will also maintain, with Turing . . . that any procedure which could &quot;naturally&quot; be called effective, can in fact be realized by a (simple) machine. Although this may seem extreme, the arguments . . . in its favor are hard to refute&quot;&lt;ref name=&quot;Minsky 1967:105&quot;&gt;Minsky 1967:105&lt;/ref&gt;.&lt;/blockquote&gt;

 &lt;blockquote&gt;Gurevich: &quot;...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage [1987], an algorithm is a computational process defined by a Turing machine&quot;&lt;ref&gt;Gurevich 2000:1, 3&lt;/ref&gt;.&lt;/blockquote&gt;

Typically, when an algorithm is associated with processing information, data is read from an input source, written to an output device, and/or stored for further processing. Stored data is regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more [[data structure]]s.

For any such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation will always be critical to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting &quot;from the top&quot; and going &quot;down to the bottom&quot;, an idea that is described more formally by ''[[control flow|flow of control]]''.

So far, this discussion of the formalization of an algorithm has assumed the premises of [[imperative programming]]. This is the most common conception, and it attempts to describe a task in discrete, &quot;mechanical&quot; means. Unique to this conception of formalized algorithms is the [[assignment operation]], setting the value of a variable. It derives from the intuition of &quot;[[memory]]&quot; as a scratchpad. There is an example below of such an assignment.

For some alternate conceptions of what constitutes an algorithm see [[functional programming]] and [[logic programming]] .

=== Termination ===
Some writers restrict the definition of ''algorithm'' to procedures that eventually finish. In such a category Kleene places the &quot;''decision procedure'' or ''decision method'' or ''algorithm'' for the question&quot;&lt;ref&gt;Kleene 1952:136&lt;/ref&gt;. Others, including Kleene, include procedures that could run forever without stopping; such a procedure has been called a &quot;computational method&quot;&lt;ref&gt;Knuth 1997:5&lt;/ref&gt; or &quot;''calculation procedure'' or ''algorithm'' (and hence a ''calculation problem'') in relation to a general question which requires for an answer, not yes or no, but '''the exhibiting of some object'''&quot;&lt;ref&gt;Boldface added, Kleene 1952:137&lt;/ref&gt;.

Minsky makes the pertinent observation, in regards to determining whether an algorithm will eventually terminate (from a particular starting state):
&lt;blockquote&gt;But if the length of the process isn't known in advance, then &quot;trying&quot; it may not be decisive, because if the process does go on forever — then at no time will we ever be sure of the answer&lt;ref name=&quot;Minsky 1967:105&quot;/&gt;.&lt;/blockquote&gt;

As it happens, no other method can do any better, as was shown by [[Alan Turing]] with his celebrated result on the undecidability of the so-called [[halting problem]]. There is no algorithmic procedure for determining of arbitrary algorithms whether or not they terminate from given starting states. The analysis of algorithms for their likelihood of termination is called [[termination analysis]].

See the examples of (im-)&quot;proper&quot; subtraction at [[partial function]] for more about what can happen when an algorithm fails for certain of its input numbers — e.g., (i) non-termination, (ii) production of &quot;junk&quot; (output in the wrong format to be considered a number) or no number(s) at all (halt ends the computation with no output), (iii) wrong number(s), or (iv) a combination of these. Kleene proposed that the production of &quot;junk&quot; or failure to produce a number is solved by having the algorithm detect these instances and produce e.g., an error message (he suggested &quot;0&quot;), or preferably, force the algorithm into an endless loop&lt;ref&gt;Kleene 1952:325&lt;/ref&gt;. Davis (1958) does this to his subtraction algorithm — he fixes his algorithm in a second example so that it is proper subtraction and it terminates&lt;ref&gt;Davis 1958:12-15&lt;/ref&gt;. Along with the logical outcomes &quot;true&quot; and &quot;false&quot; Kleene (1952) also proposes the use of a third logical symbol &quot;u&quot; — undecided&lt;ref&gt;Kleene 1952:332&lt;/ref&gt; — thus an algorithm will always produce ''something'' when confronted with a &quot;proposition&quot;. The problem of wrong answers must be solved with an independent &quot;proof&quot; of the algorithm e.g., using induction:
&lt;blockquote&gt;We normally require auxiliary evidence for this [that the algorithm correctly defines a [[mu recursive function]]], e.g., in the form of an inductive proof that, for each argument value, the computation terminates with a unique value&lt;ref&gt;Minsky 1967:186&lt;/ref&gt;.&lt;/blockquote&gt;

=== Expressing algorithms ===
Algorithms can be expressed in many kinds of notation, including [[natural language]]s, [[pseudocode]], [[flowchart]]s, [[programming language]]s or [[control table]]s (processed by [[Interpreter (computing)|interpreters]]). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements, while remaining independent of a particular implementation language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a [[computer]], but are often used as a way to define or document algorithms.

There is a wide variety of representations possible and one can express a given [[Turing machine]] program as a sequence of machine tables (see more at [[finite state machine]] and [[state transition table]]), as flowcharts (see more at [[state diagram]]), or as a form of rudimentary [[machine code]] or [[assembly code]] called &quot;sets of quadruples&quot; (see more at [[Turing machine]]). 

Sometimes it is helpful in the description of an algorithm to supplement small &quot;flow charts&quot; (state diagrams) with natural-language and/or arithmetic expressions written inside &quot;[[block diagram]]s&quot; to summarize what the &quot;flow charts&quot; are accomplishing. 

Representations of algorithms are generally classed into three accepted levels of Turing machine description&lt;ref&gt;Sipser 2006:157&lt;/ref&gt;:
*'''1 High-level description''':
:: &quot;...prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head.&quot;
*'''2 Implementation description''':
:: &quot;...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level we do not give details of states or transition function.&quot;
*'''3 Formal description''':
:: Most detailed, &quot;lowest level&quot;, gives the Turing machine's &quot;state table&quot;.

:''For an example of the simple algorithm &quot;Add m+n&quot; described in all three levels see [[Algorithm examples]].''

=== Computer algorithms ===
In [[computer systems]], an algorithm is basically an instance of [[logic]] written in [[software]] by software developers to be effective for the intended &quot;target&quot; computer(s), in order for the [[software]] on the target machines to ''do something''. For instance, if a person is writing software that is supposed to print out a [[PDF]] document located at the operating system folder &quot;/My Documents&quot; at [[computer drive]] &quot;D:&quot; every [[Friday]] at 10PM, they will write an algorithm that specifies the following actions: &quot;If today's date (computer time) is 'Friday,' open the document at 'D:/My Documents' and call the 'print' function&quot;. While this simple algorithm does not look into whether the [[printer (computing)|printer]] has enough paper or whether the document has been moved into a different location, one can make this algorithm more robust and anticipate these problems by rewriting it as a formal [[Case statement|CASE statement]]&lt;ref&gt; Kleene 1952:229 shows that &quot;Definition by cases&quot; is [[primitive recursive]]. CASES requires that the list of testable instances within the CASE definition to be (i) [[mutually exclusive]] and (ii) [[collectively exhaustive]] i.e. it must include or &quot;cover&quot; all possibility. The CASE statement proceeds in numerical order and exits at the first successful test; see more at Boolos-Burgess-Jeffrey Fourth edition 2002:74&lt;/ref&gt; or as a (carefully crafted) sequence of IF-THEN-ELSE statements&lt;ref&gt; An IF-THEN-ELSE or &quot;logical test with branching&quot; is just a CASE instruction reduced to two outcomes: (i) test is successful, (ii) test is unsuccessful. The IF-THEN-ELSE is closely related to the AND-OR-INVERT logic function from which all 16 logical &quot;operators&quot; of one or two variables can be derived; see more at [[Propositional formula]]. Like definition by cases, a sequence of IF-THEN-ELSE logical tests must be mutually exclusive and collectively exhaustive over the variables tested.&lt;/ref&gt;. For example the CASE statement might appear as follows (there are other possibilities): 
:CASE 1: IF today's date is NOT Friday THEN ''exit this CASE instruction'' ELSE
:CASE 2: IF today's date is Friday AND the document is located at 'D:/My Documents' AND there is paper in the printer THEN print the document (and ''exit this CASE instruction'') ELSE
:CASE 3: IF today's date is Friday AND the document is NOT located at 'D:/My Documents' THEN display 'document not found' error message (and ''exit this CASE instruction'') ELSE
:CASE 4: IF today's date is Friday AND the document is located at 'D:/My Documents' AND there is NO paper in the printer THEN (i) display 'out of paper' error message and (ii) ''exit''.

Note that CASE 3 includes two possibilities: (i) the document is NOT located at 'D:/My Documents' AND there's paper in the printer OR (ii) the document is NOT located at 'D:/My Documents' AND there's NO paper in the printer. 

The sequence of IF-THEN-ELSE tests might look like this:
:TEST 1: IF today's date is NOT Friday THEN ''done'' ELSE TEST 2:
::TEST 2: IF the document is NOT located at 'D:/My Documents' THEN display 'document not found' error message ELSE TEST 3:
:::TEST 3: IF there is NO paper in the printer THEN display 'out of paper' error message ELSE print the document.

These examples' logic grants precedence to the instance of &quot;NO document at 'D:/My Documents' &quot;. Also observe that in a well-crafted CASE statement or sequence of IF-THEN-ELSE statements the number of distinct actions—4 in these examples: do nothing, print the document, display 'document not found', display 'out of paper' -- equals the number of cases.

Given unlimited memory, a computational machine with the ability to execute either a set of CASE statements or a sequence of IF-THEN-ELSE statements is [[Turing complete]]. Therefore, anything that is computable can be computed by this machine. This form of algorithm is fundamental to [[computer programming]] in all its forms (see more at [[McCarthy formalism]]).

=== Implementation ===
Most algorithms are intended to be implemented as [[computer programs]]. However, algorithms are also implemented by other means, such as in a biological [[neural network]] (for example, the [[human brain]] implementing [[arithmetic]] or an insect looking for food), in an [[electrical circuit]], or in a mechanical device.

== Example ==
{{further|[[Algorithm examples]]}}
[[File:Sorting quicksort anim.gif|thumb|right|An animation of the [[quicksort|quicksort algorithm]] sorting an array of randomized values. The red bars mark the pivot element; at the start of the animation, the element farthest to the right hand side is chosen as the pivot.]] 

One of the simplest algorithms is to find the largest number in an (unsorted) list of numbers. The solution necessarily requires looking at every number in the list, but only once at each. From this follows a simple algorithm, which can be stated in a high-level description English prose, as:

'''High-level description:'''
# Assume the first item is largest.
# Look at each of the remaining items in the list and if it is larger than the largest item so far, make a note of it.
# The last noted item is the largest in the list when the process is complete.

'''(Quasi-)formal description:'''
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in [[pseudocode]] or [[pidgin code]]:

{{algorithm-begin|name=LargestNumber}}
   Input: A non-empty list of numbers ''L''.
   Output: The ''largest'' number in the list ''L''.
 
   ''largest'' ← ''L''&lt;sub&gt;0&lt;/sub&gt;
   '''for each''' ''item'' '''in''' the list ''L&lt;sub&gt;≥1&lt;/sub&gt;'', '''do'''
     '''if''' the ''item'' &gt; ''largest'', '''then'''
       ''largest'' ← the ''item''
   '''return''' ''largest''
{{algorithm-end}}

For a more complex example of an algorithm, see [[Euclid's algorithm]] for the [[greatest common divisor]], one of the earliest algorithms known.

== Algorithmic analysis ==
{{Main|Analysis of algorithms}}
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the [[analysis of algorithms]] to obtain such quantitative answers (estimates); for example, the algorithm above has a time requirement of O(''n''), using the [[big O notation]] with ''n'' as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore it is said to have a space requirement of ''O(1)'', if the space required to store the input numbers is not counted, or O(''n'') if it is counted. 

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or '[[algorithmic efficiency|effort]]' than others. For example, a [[binary search]] algorithm will usually outperform a [[Brute-force search|brute force]] sequential search when used for [[lookup table|table lookup]]s on sorted lists.

=== Formal versus empirical ===
{{Main|Empirical algorithmics|Profiling (computer programming)|Program optimization}}
{{Expand section|date=September 2009}}
The [[analysis of algorithms|analysis and study of algorithms]] is a discipline of [[computer science]], and is often practiced abstractly without the use of a specific [[programming language]] or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually [[pseudocode]] is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their [[algorithmic efficiency]] is eventually put to the test using real code.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. For instance an algorithm that has no [[locality of reference]] may have much poorer performance than predicted because it 'thrashes the [[cache]]'. [[Benchmark (computing)|Benchmark]]s may be used to compare before/after potential improvements to an algorithm after [[program optimization]].

== Classification ==
There are various ways to classify algorithms, each with its own merits.

=== By implementation ===
One way to classify algorithms is by implementation means.{{Or|date=September 2009}}&lt;!-- see talk --&gt;

* '''Recursion''' or '''iteration''': A [[recursive algorithm]] is one that invokes (makes reference to) itself repeatedly until a certain condition matches, which is a method common to [[functional programming]]. [[Iteration|Iterative]] algorithms use repetitive constructs like [[Control flow#Loops|loops]] and sometimes additional data structures like [[Stack (data structure)|stacks]] to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, [[towers of Hanoi]] is well understood in recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
* '''Logical''': An algorithm may be viewed as controlled [[Deductive reasoning|logical deduction]]. This notion may be expressed as: '''Algorithm = logic + control'''&lt;ref&gt;Kowalski 1979&lt;/ref&gt;. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the [[logic programming]] paradigm. In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant [[Formal semantics of programming languages|semantics]]: a change in the axioms has a well defined change in the algorithm.
* '''Serial''' or '''parallel''' or '''distributed''': Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to [[parallel algorithm]]s or [[distributed algorithms]]. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a [[Computer Network|network]]. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms, and are called inherently serial problems.
* '''Deterministic''' or '''non-deterministic''': [[Deterministic algorithm]]s solve the problem with exact decision at every step of the algorithm whereas [[non-deterministic algorithm]]s solve problems via guessing although typical guesses are made more accurate through the use of [[heuristics]].
* '''Exact''' or '''approximate''': While many algorithms reach an exact solution, [[approximation algorithm]]s seek an approximation that is close to the true solution. Approximation may use either a deterministic or a random strategy. Such algorithms have practical value for many hard problems.

=== By design paradigm ===
Another{{Or|date=September 2009}}&lt;!-- see talk --&gt; way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories will include many different types of algorithms. Some commonly found paradigms include:

* '''[[Brute force search|Brute-force]]''' or '''exhaustive search'''.  This is the naïve method of trying every possible solution to see which is best.&lt;ref&gt;{{Cite web |title=Fundamental Concepts for the Software Quality Engineer |author=Sue Carroll, Taz Daughtrey |pages=282 et seq.|url=http://books.google.com/books?id=bz_cl3B05IcC&amp;pg=PA282}}&lt;/ref&gt;
* '''Divide and conquer'''. A [[divide and conquer algorithm]] repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually [[recursion|recursively]]) until the instances are small enough to solve easily. One such example of divide and conquer is [[mergesort|merge sorting]]. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a '''decrease and conquer algorithm''', that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage will be more complex than decrease and conquer algorithms. An example of decrease and conquer algorithm is the [[binary search algorithm]].
* '''[[Dynamic programming]]'''. When a problem shows [[optimal substructure]], meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems, and [[overlapping subproblems]], meaning the same subproblems are used to solve many different problem instances, a quicker approach called ''dynamic programming'' avoids recomputing solutions that have already been computed. For example, the shortest path to a goal from a vertex in a weighted [[graph (mathematics)|graph]] can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and [[memoization]] go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a [[Mathematical table|table]] of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
* '''The greedy method'''. A [[greedy algorithm]] is similar to a [[dynamic programming|dynamic programming algorithm]], but the difference is that solutions to the subproblems do not have to be known at each stage; instead a &quot;greedy&quot; choice can be made of what looks best for the moment. The greedy method extends the solution with the best possible decision (not all feasible decisions) at an algorithmic stage based on the current local optimum and the best decision (not all possible decisions) made in a previous stage. It is not exhaustive, and does not give accurate answer to many problems. But when it works, it will be the fastest method. The most popular greedy algorithm is finding the minimal spanning tree as given by [[kruskal's algorithm|Kruskal]].
* '''Linear programming'''. When solving a problem using [[linear programming]], specific [[inequality|inequalities]] involving the inputs are found and then an attempt is made to maximize (or minimize) some linear function of the inputs. Many problems (such as the [[Maximum flow problem|maximum flow]] for directed [[graph (mathematics)|graphs]]) can be stated in a linear programming way, and then be solved by a 'generic' algorithm such as the [[simplex algorithm]]. A more complex variant of linear programming is called integer programming, where the solution space is restricted to the [[integers]].
* '''[[Reduction (complexity)|Reduction]]'''. This technique involves solving a difficult problem by transforming it into a better known problem for which we have (hopefully) [[asymptotically optimal]] algorithms. The goal is to find a reducing algorithm whose [[Computational complexity theory|complexity]] is not dominated by the resulting reduced algorithm's. For example, one [[selection algorithm]] for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion).  This technique is also known as ''transform and conquer''.
* '''Search and enumeration'''. Many problems (such as playing [[chess]]) can be modeled as problems on [[graph theory|graphs]]. A [[graph exploration algorithm]] specifies rules for moving around a graph and is useful for such problems. This category also includes [[search algorithm]]s, [[branch and bound]] enumeration and [[backtracking]].
* '''The probabilistic and heuristic paradigm'''. Algorithms belonging to this class fit the definition of an algorithm more loosely.{{Dubious|date=September 2009}}{{Citation needed|date=September 2009}}
# [[Randomized algorithm]]s are those that make some choices randomly (or pseudo-randomly); for some problems, it can in fact be proven that the fastest solutions must involve some [[randomness]]. There are two large classes of such algorithms:
## [[Monte Carlo algorithm]]s return a correct answer with high-probability. E.g. [[RP (complexity)|RP]] is the subclass of these that run in [[polynomial time]])
## [[Las Vegas algorithm]]s always return the correct answer, but their running time is only bound in probaility, e.g. [[Zero-error Probabilistic Polynomial time|ZPP]].
# In [[optimization (mathematics)|optimization problems]], [[heuristic]] algorithms do not try to find an optimal solution, but an approximate solution where the time or resources are limited. They are not practical to find perfect solutions. An example of this would be [[local search (optimization)|local search]], [[tabu search]], or [[simulated annealing]] algorithms, a class of heuristic probabilistic algorithms that vary the solution of a problem by a random amount. The name &quot;[[simulated annealing]]&quot; alludes to the metallurgic term meaning the heating and cooling of metal to achieve freedom from defects. The purpose of the random variance is to find close to globally optimal solutions rather than simply locally optimal ones, the idea being that the random element will be decreased as the algorithm settles down to a solution. [[Approximation algorithms]] are those heuristic algorithms that additionally provide some bounds on the error. [[Genetic algorithm]]s attempt to find solutions to problems by mimicking biological [[evolution]]ary processes, with a cycle of random mutations yielding successive generations of &quot;solutions&quot;. Thus, they emulate reproduction and &quot;survival of the fittest&quot;. In [[genetic programming]], this approach is extended to algorithms, by regarding the algorithm itself as a &quot;solution&quot; to a problem.

=== By field of study ===
{{See also|List of algorithms}}
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are [[search algorithm]]s, [[sorting algorithm]]s, [[merge algorithm]]s, [[numerical analysis|numerical algorithms]], [[graph theory|graph algorithms]], [[string algorithms]], [[computational geometry|computational geometric algorithms]], [[combinatorial|combinatorial algorithms]], [[machine learning]], [[cryptography]], [[data compression]] algorithms and [[parsing|parsing techniques]].

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.

=== By complexity ===
{{See also|Complexity class| Parameterized complexity}}
Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.

=== By computing power ===
Another{{Dubious|date=September 2009}}&lt;!-- how is this different from the above? --&gt; way to classify algorithms is by computing power. This is typically done by considering some collection (class) of algorithms. A recursive class of algorithms is one that includes algorithms for all Turing computable functions. Looking at classes of algorithms allows for the possibility of restricting the available computational resources (time and memory) used in a computation. A subrecursive class of algorithms is one in which not all Turing computable functions can be obtained. For example, the algorithms that run in [[P (complexity)|polynomial time]] suffice for many important types of computation but do not exhaust all Turing computable functions. The class of algorithms implemented by [[primitive recursive function]]s is another subrecursive class. 

Burgin (2005, p.&amp;nbsp;24) uses a generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes a function must be determined after a finite number of steps.  He defines a super-recursive class of algorithms as &quot;a class of algorithms in which it is possible to compute functions not computable by any Turing machine&quot; (Burgin 2005, p.&amp;nbsp;107). This is closely related to the study of methods of [[hypercomputation]].

== Legal issues ==
:''See also: [[Software patents]] for a general overview of the patentability of software, including computer-implemented algorithms.''

Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute &quot;processes&quot; (USPTO 2006), and hence algorithms are not patentable (as in [[Gottschalk v. Benson]]). However, practical applications of algorithms are sometimes patentable. For example, in [[Diamond v. Diehr]], the application of a simple [[feedback]] algorithm to aid in the curing of [[synthetic rubber]] was deemed patentable. The [[Software patent debate|patenting of software]] is highly controversial, and there are highly criticized patents involving algorithms, especially [[data compression]] algorithms, such as [[Unisys]]' [[Graphics Interchange Format#Unisys and LZW patent enforcement|LZW patent]]. 

Additionally, some cryptographic algorithms have export restrictions (see [[export of cryptography]]).

== History: Development of the notion of &quot;algorithm&quot; ==
=== Origin of the word ===
The word ''algorithm'' comes from the name of the 9th century [[Iran|Persian]] mathematician [[Muhammad ibn Mūsā al-Khwārizmī]] whose works introduced Indian numerals and algebraic concepts. He worked in [[Baghdad]] at the time when it was the centre of scientific studies and trade. The word ''[[algorism]]'' originally referred only to the rules of performing [[arithmetic]] using [[Hindu-Arabic numeral system|Arabic numerals]] but evolved via European Latin translation of al-Khwarizmi's name into ''algorithm'' by the 18th century. The word evolved to include all definite procedures for solving problems or performing tasks.

=== Discrete and distinguishable symbols ===
'''Tally-marks''': To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually [[Roman numerals]] and the [[abacus]] evolved (Dilson, p.&amp;nbsp;16–41). Tally marks appear prominently in [[unary numeral system]] arithmetic used in [[Turing machine]] and [[Post-Turing machine]] computations.

=== Manipulation of symbols as &quot;place holders&quot; for numbers: algebra ===
The work of the ancient [[Greek mathematics|Greek geometers]], [[Islamic mathematics|Persian mathematician]] [[Muhammad ibn Mūsā al-Khwārizmī|Al-Khwarizmi]] (often considered the &quot;father of [[algebra]]&quot; and from whose name the terms &quot;[[algorism]]&quot; and &quot;algorithm&quot; are derived), and Western European mathematicians culminated in [[Leibniz]]'s notion of the [[calculus ratiocinator]] (ca 1680):
:&quot;A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers&quot;&lt;ref&gt;Davis 2000:18&lt;/ref&gt;.

=== Mechanical contrivances with discrete states ===
'''The clock''': Bolter credits the invention of the weight-driven [[clock]] as “The key invention [of Europe in the Middle Ages]&quot;, in particular the [[verge escapement]]&lt;ref&gt;Bolter 1984:24&lt;/ref&gt; that provides us with the tick and tock of a mechanical clock. “The accurate automatic machine”&lt;ref&gt;Bolter 1984:26&lt;/ref&gt; led immediately to &quot;mechanical [[automata theory|automata]]&quot; beginning in the thirteenth century and finally to “computational machines&quot; – the [[difference engine]] and [[analytical engine]]s of [[Charles Babbage]] and Countess [[Ada Lovelace]]&lt;ref&gt;Bolter 1984:33–34, 204–206)&lt;/ref&gt;.

'''Logical machines 1870 -- [[Stanley Jevons]]' &quot;logical abacus&quot; and &quot;logical machine&quot;''': The technical problem was to reduce [[Boolean equation]]s when presented in a form similar to what are now known as [[Karnaugh map]]s. Jevons (1880) describes first a simple &quot;abacus&quot; of &quot;slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a '''Logical Machine'''&quot; His machine came equipped with &quot;certain moveable wooden rods&quot; and &quot;at the foot are 21 keys like those of a piano [etc] . . .&quot;. With this machine he could analyze a &quot;[[syllogism]] or any other simple logical argument&quot;&lt;ref&gt;All quotes from W. Stanley Jevons 1880 ''Elementary Lessons in Logic: Deductive and Inductive'', Macmillan and Co., London and New York. Republished as a googlebook; cf Jevons 1880:199-201. Louis Couturat 1914 ''the Algebra of Logic'', The Open Court Publishing Company, Chicago and London. Republished as a googlebook; cf Couturat 1914:75-76 gives a few more details; interestingly he compares this to a typewriter as well as a piano. Jevons states that the account is to be found at Jan. 20, 1870 ''The Proceedings of the Royal Society''.&lt;/ref&gt;.

This machine he displayed in 1870 before the Fellows of the Royal Society&lt;ref&gt;Jevons 1880:199-200&lt;/ref&gt;. Another logician [[John Venn]], however, in his 1881 ''Symbolic Logic'', turned a jaundiced eye to this effort: &quot;I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines&quot;; see more at [[Algorithm characterizations]]. But not to be outdone he too presented &quot;a plan somewhat analogous, I apprehend, to Prof. Jevon's ''abacus'' ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine&quot;&lt;ref&gt;All quotes from John Venn 1881 ''Symbolic Logic'', Macmillan and Co., London. Republished as a googlebook. cf Venn 1881:120-125. The interested reader can find a deeper explanation in those pages.&lt;/ref&gt;.   

'''Jacquard loom, Hollerith punch cards, telegraphy and telephony — the electromechanical relay''': Bell and Newell (1971) indicate that the [[Jacquard loom]] (1801), precursor to [[Hollerith cards]] (punch cards, 1887), and “telephone switching technologies” were the roots of a tree leading to the development of the first computers&lt;ref&gt;Bell and Newell diagram 1971:39, cf. Davis 2000&lt;/ref&gt;. By the mid-1800s the [[telegraph]], the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as “dots and dashes” a common sound. By the late 1800s the [[ticker tape]] (ca 1870s) was in use, as was the use of [[Hollerith cards]] in the 1890 U.S. census. Then came the [[Teletype]] (ca. 1910) with its punched-paper use of [[Baudot code]] on tape.

'''Telephone-switching networks''' of electromechanical [[relay]]s (invented 1835) was behind the work of [[George Stibitz]] (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the “burdensome’ use of mechanical calculators with gears. &quot;He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device&quot;.&lt;ref&gt;* Melina Hill, Valley News Correspondent, ''A Tinkerer Gets a Place in History'', Valley News West Lebanon NH, Thursday March 31, 1983, page 13.&lt;/ref&gt;.

Davis (2000) observes the particular importance of the electromechanical relay (with its two &quot;binary states&quot; ''open'' and ''closed''):
: It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.&quot;&lt;ref&gt;Davis 2000:14&lt;/ref&gt;.

=== Mathematics during the 1800s up to the mid-1900s ===
'''Symbols and rules''': In rapid succession the mathematics of [[George Boole]] (1847, 1854), [[Gottlob Frege]] (1879), and [[Giuseppe Peano]] (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's ''The principles of arithmetic, presented by a new method'' (1888) was &quot;the first attempt at an axiomatization of mathematics in a symbolic language&quot;&lt;ref&gt;van Heijenoort 1967:81ff&lt;/ref&gt;.

But Heijenoort gives Frege (1879) this kudos: Frege’s is &quot;perhaps the most important single work ever written in logic. ... in which we see a &quot; 'formula language', that is a ''lingua characterica'', a language written with special symbols, &quot;for pure thought&quot;, that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules&quot;&lt;ref&gt;van Heijenoort's commentary on Frege's ''Begriffsschrift, a formula language, modeled upon that of arithmetic, for pure thought'' in van Heijenoort 1967:1&lt;/ref&gt;. The work of Frege was further simplified and amplified by [[Alfred North Whitehead]] and [[Bertrand Russell]] in their [[Principia Mathematica]] (1910–1913).

'''The paradoxes''': At the same time a number of disturbing paradoxes appeared in the literature, in particular the [[Burali-Forti paradox]] (1897), the [[Russell paradox]] (1902–03), and the [[Richard Paradox]]&lt;ref&gt;Dixon 1906, cf. Kleene 1952:36–40&lt;/ref&gt;. The resultant considerations led to [[Kurt Gödel]]’s paper (1931) — he specifically cites the paradox of the liar — that completely reduces rules of [[recursion]] to numbers. 

'''Effective calculability''': In an effort to solve the [[Entscheidungsproblem]] defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an &quot;effective method&quot; or &quot;effective calculation&quot; or &quot;effective calculability&quot; (i.e., a calculation that would succeed). In rapid succession the following appeared: [[Alonzo Church]], [[Stephen Kleene]] and [[J.B. Rosser]]'s [[λ-calculus]]&lt;ref&gt;cf. footnote in [[Alonzo Church]] 1936a in Davis 1965:90 and 1936b in Davis 1965:110&lt;/ref&gt; a finely-honed definition of &quot;general recursion&quot; from the work of Gödel acting on suggestions of [[Jacques Herbrand]] (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene&lt;ref&gt; Kleene 1935-6 in Davis 1965:237ff, Kleene 1943 in Davis 1965:255ff&lt;/ref&gt;. Church's proof&lt;ref&gt;Church 1936 in Davis 1965:88ff&lt;/ref&gt; that the [[Entscheidungsproblem]] was unsolvable, [[Emil Post]]'s definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction&lt;ref&gt;cf. &quot;Formulation I&quot;, Post 1936 in Davis 1965:289-290&lt;/ref&gt;. [[Alan Turing]]'s proof of that the Entscheidungsproblem was unsolvable by use of his &quot;a- [automatic-] machine&quot;&lt;ref&gt;Turing 1936-7 in Davis 1965:116ff&lt;/ref&gt; -- in effect almost identical to Post's &quot;formulation&quot;, [[J. Barkley Rosser]]'s definition of &quot;effective method&quot; in terms of &quot;a machine&quot;&lt;ref&gt;Rosser 1939 in Davis 1965:226&lt;/ref&gt;. [[S. C. Kleene]]'s proposal of a precursor to &quot;[[Church thesis]]&quot; that he called &quot;Thesis I&quot;&lt;ref&gt;Kleene 1943 in Davis 1965:273–274&lt;/ref&gt;, and a few years later Kleene's renaming his Thesis &quot;Church's Thesis&quot;&lt;ref&gt; Kleene 1952:300, 317&lt;/ref&gt; and proposing &quot;Turing's Thesis&quot;&lt;ref&gt;Kleene 1952:376&lt;/ref&gt;.

=== Emil Post (1936) and Alan Turing (1936-7, 1939)===
Here is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations — and they yield virtually identical definitions.

[[Emil Post]] (1936) described the actions of a &quot;computer&quot; (human being) as follows:
:&quot;...two concepts are involved: that of a ''symbol space'' in which the work leading from problem to answer is to be carried out, and a fixed unalterable ''set of directions''.

His symbol space would be
:&quot;a two way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.

:&quot;One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes....

:&quot;A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process will terminate only when it comes to the direction of type (C ) [i.e., STOP]&quot;&lt;ref&gt; Turing 1936-7 in Davis 1965:289–290&lt;/ref&gt;. See more at [[Post-Turing machine]]

[[Alan Turing]]’s work&lt;ref&gt; Turing 1936 in Davis 1965, Turing 1939 in Davis 1965:160&lt;/ref&gt; preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing’s biographer believed that Turing’s use of a typewriter-like model derived from a youthful interest: “Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'&quot;&lt;ref&gt; Hodges, p. 96&lt;/ref&gt;. Given the prevalence of Morse code and telegraphy, ticker tape machines, and Teletypes we might conjecture that all were influences.

Turing — his model of computation is now called a [[Turing machine]] — begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and &quot;states of mind&quot;. But he continues a step further and creates a machine as a model of computation of numbers&lt;ref&gt;Turing 1936-7:116)&lt;/ref&gt;.

:&quot;Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book....I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite....

:&quot;The behavior of the computer at any moment is determined by the symbols which he is observing, and his &quot;state of mind&quot; at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...

:&quot;Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided&quot;&lt;ref name=&quot;Turing 1936-7 in Davis 1965:136&quot;&gt;Turing 1936-7 in Davis 1965:136&lt;/ref&gt;.

Turing's reduction yields the following:
:&quot;The simple operations must therefore include:
::&quot;(a) Changes of the symbol on one of the observed squares
::&quot;(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.
&quot;It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:
::&quot;(A) A possible change (a) of symbol together with a possible change of state of mind.
::&quot;(B) A possible change (b) of observed squares, together with a possible change of state of mind&quot;

:&quot;We may now construct a machine to do the work of this computer&quot;&lt;ref name=&quot;Turing 1936-7 in Davis 1965:136&quot;/&gt;.

A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:
:&quot;A function is said to be &quot;effectively calculable&quot; if its values can be found by some purely mechanical process. Although it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition . . . [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing and Post] . . . We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability . . . .
::&quot;† We shall use the expression &quot;computable function&quot; to mean a function calculable by a machine, and we let &quot;effectively calculable&quot; refer to the intuitive idea without particular identification with any one of these definitions&quot;&lt;ref&gt;Turing 1939 in Davis 1965:160&lt;/ref&gt;.

=== J. B. Rosser (1939) and S. C. Kleene (1943) ===
'''[[J. Barkley Rosser]]''' boldly defined an ‘effective [mathematical] method’ in the following manner (boldface added):
:&quot;'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that '''an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer'''. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.&quot; (Rosser 1939:225–6)

Rosser's footnote #5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his ''An Unsolvable Problem of Elementary Number Theory'' (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper ''On Formally Undecidable Propositions of Principia Mathematica and Related Systems I'' (1931); and (3) Post (1936) and Turing (1936-7) in their mechanism-models of computation.

'''[[Stephen C. Kleene]]''' defined as his now-famous &quot;Thesis I&quot; known as the [[Church-Turing thesis]]. But he did this in the following context (boldface in original):
:&quot;12. '''Algorithmic theories'''... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, &quot;yes&quot; or &quot;no,&quot; to the question, &quot;is the predicate value true?”&quot; (Kleene 1943:273)

=== History after 1950 ===
A number of efforts have been directed toward further refinement of the definition of &quot;algorithm&quot;, and activity is on-going because of issues surrounding, in particular, [[foundations of mathematics]] (especially the [[Church-Turing Thesis]]) and [[philosophy of mind]] (especially arguments around [[artificial intelligence]]). For more, see [[Algorithm characterizations]].

== See also ==
{{wikibooks|Algorithms}}
{{WVD}}
&lt;div style=&quot;-moz-column-count:2; column-count:2;&quot;&gt;
* [[Abstract machine]]
* [[Algorithm characterizations]]
* [[Algorithm design]]
* [[Algorithmic efficiency]]
* [[Algorithm engineering]]
* [[Algorithm examples]]
* [[Algorithmic music]]
* [[Garbage In, Garbage Out]]
* [[High-level synthesis|Algorithmic synthesis]]
* [[Algorithmic trading]]
* [[Data structure]]
* [[Heuristics]]
* ''[[Introduction to Algorithms]]''
* [[List of important publications in computer science#Algorithms|Important algorithm-related publications]]
* [[List of algorithm general topics]]
* [[List of algorithms]]
* [[List of terms relating to algorithms and data structures]]
* [[Partial function]]
* [[Profiling (computer programming)]]
* [[Program optimization]]
* [[Theory of computation]]
** [[Computability]] (part of [[computability theory]])
** [[Computational complexity theory]]
* [[Randomized algorithm]] and [[quantum algorithm]]
&lt;/div&gt;

==Notes==
{{reflist|3}}

== References ==
* Axt, P. (1959) On a Subrecursive Hierarchy and Primitive Recursive Degrees, ''Transactions of the American Mathematical Society'' 92, pp.&amp;nbsp;85–105
*Bell, C. Gordon and Newell, Allen (1971), ''Computer Structures: Readings and Examples'', McGraw-Hill Book Company, New York. ISBN 0070043574}.
* {{Cite journal|author1-link=Andreas Blass|first1=Andreas|last1=Blass|author2-link=Yuri Gurevich|first2=Yuri|last2=Gurevich|year=2003|url=http://research.microsoft.com/~gurevich/Opera/164.pdf|title=Algorithms: A Quest for Absolute Definitions|journal= Bulletin of European Association for Theoretical Computer Science|volume= 81}} Includes an excellent bibliography of 56 references.
* {{Cite book|last1=Boolos|first1= George|last2=Jeffrey|first2= Richard|author1-link=George Boolos|author2-link=Richard Jeffrey|title=Computability and Logic|edition=4th|publisher=Cambridge University Press, London|year=1974, 1999|date=1974, 1980, 1989, 1999|isbn=0-521-20402-X|ref=harv|author=George Boolos, Richard Jeffrey.}}: cf. Chapter 3 ''Turing machines'' where they discuss &quot;certain enumerable sets not effectively (mechanically) enumerable&quot;.
* Burgin, M. ''Super-recursive algorithms'', Monographs in computer science, Springer, 2005. ISBN 0387955690 
* Campagnolo, M.L., Moore, C., and Costa, J.F. (2000) An analog characterization of the subrecursive functions. In ''Proc. of the 4th Conference on Real Numbers and Computers'', Odense University, pp.&amp;nbsp;91–109 
* {{cite journal|last=Church|first=Alonzo|authorlink=Alonzo Church|title=An Unsolvable Problem of Elementary Number Theory|journal=The American Journal of Mathematics|volume=58|pages= 345–363|year=1936a|doi=10.2307/2371045|issue=2}} Reprinted in ''The Undecidable'', p.&amp;nbsp;89ff. The first expression of &quot;Church's Thesis&quot;. See in particular page 100 (''The Undecidable'') where he defines the notion of &quot;effective calculability&quot; in terms of &quot;an algorithm&quot;, and he uses the word &quot;terminates&quot;, etc.
* {{cite journal|last=Church|first=Alonzo|authorlink=Alonzo Church|title=A Note on the Entscheidungsproblem|journal=The Journal of Symbolic Logic|volume=1|number=1|year=1936b|pages=40–41|id={{jstor|2269326}}|doi=10.2307/2269326|issue=1}} {{cite journal|last=Church|first=Alonzo|title=Correction to a Note on the Entscheidungsproblem|journal=The Journal of Symbolic Logic|volume=1|number=3|year=1936|pages=101–102|id={{jstor|2269030}}|doi=10.2307/2269030|issue=3}} Reprinted in ''The Undecidable'', p.&amp;nbsp;110ff. Church shows that the Entscheidungsproblem is unsolvable in about 3 pages of text and 3 pages of footnotes.
* {{cite book|last=Daffa'|first=Ali Abdullah al-|title=The Muslim contribution to mathematics|year=1977|publisher=Croom Helm|location=London|isbn=0-85664-464-1}}
* {{cite book|last=Davis|first=Martin|authorlink=Martin Davis|title=The Undecidable: Basic Papers On Undecidable Propositions, Unsolvable Problems and Computable Functions|publisher=Raven Press|location=New York|year=1965|isbn=0486432289}} Davis gives commentary before each article. Papers of [[Gödel]], [[Alonzo Church]], [[Alan Turing|Turing]], [[J. Barkley Rosser|Rosser]], [[Kleene]], and [[Emil Post]] are included; those cited in the article are listed here by author's name.
* {{cite book|last=Davis|first=Martin|authorlink=Martin Davis|title=Engines of Logic: Mathematicians and the Origin of the Computer|publisher=W. W. Nortion|location=New York|year=2000|isbn=0393322297}} Davis offers concise biographies of [[Leibniz]], [[Boole]], [[Frege]], [[Georg Cantor|Cantor]], [[Hilbert]], [[Gödel]] and [[Alan Turing|Turing]] with [[von Neumann]] as the show-stealing villain. Very brief bios of [[Joseph-Marie Jacquard]], [[Babbage]], [[Ada Lovelace]], [[Claude Shannon]], [[Howard Aiken]], etc.
* {{DADS|algorithm|algorithm}}
* {{cite book|last=Dennett|first=Daniel|authorlink=Daniel Dennett|title=Darwin's Dangerous Idea|publisher=Touchstone/Simon &amp; Schuster|location=New York|year=1995|isbn=0684802902}}
* [[Yuri Gurevich]], [http://research.microsoft.com/~gurevich/Opera/141.pdf ''Sequential Abstract State Machines Capture Sequential Algorithms''], ACM Transactions on Computational Logic, Vol 1, no 1 (July 2000), pages 77–111. Includes bibliography of 33 sources.
* {{cite journal|last=Kleene C.|first=Stephen|authorlink=Stephen Kleene |title=General Recursive Functions of Natural Numbers|journal=Mathematische Annalen|volume=112|number=5|pages=727–742|year=1936
 | doi = 10.1007/BF01565439}} Presented to the American Mathematical Society, September 1935. Reprinted in ''The Undecidable'', p.&amp;nbsp;237ff. Kleene's definition of &quot;general recursion&quot; (known now as mu-recursion) was used by Church in his 1935 paper ''An Unsolvable Problem of Elementary Number Theory'' that proved the &quot;decision problem&quot; to be &quot;undecidable&quot; (i.e., a negative result).
* {{cite journal|last=Kleene C.|first=Stephen|authorlink=Stephen Kleene |title= Recursive Predicates and Quantifiers|journal=American Mathematical Society Transactions|volume=54|number=1|pages=41–73|year=1943 |doi= 10.2307/1990131|issue=1}} Reprinted in ''The Undecidable'', p.&amp;nbsp;255ff. Kleene refined his definition of &quot;general recursion&quot; and proceeded in his chapter &quot;12. Algorithmic theories&quot; to posit &quot;Thesis I&quot; (p.&amp;nbsp;274); he would later repeat this thesis (in Kleene 1952:300) and name it &quot;Church's Thesis&quot;(Kleene 1952:317) (i.e., the [[Church thesis]]). 
* {{cite book|last=Kleene|first=Stephen C.|authorlink=Kleene|title=Introduction to Metamathematics|edition=Tenth Edition 1991|publisher=North-Holland Publishing Company|year=First Edition 1952|isbn=0720421039}} Excellent — accessible, readable — reference source for mathematical &quot;foundations&quot;.
* {{cite book|last=Knuth|first=Donald|authorlink=Donald Knuth|title=Fundamental Algorithms, Third Edition|publisher=Addison-Wesley|location=Reading, Massachusetts|year=1997|isbn=0201896834}}
* Kosovsky, N. K. ''Elements of Mathematical Logic and its Application to the theory of Subrecursive Algorithms'', LSU Publ., Leningrad, 1981 
* {{cite journal|last=Kowalski|first=Robert|authorlink=Robert Kowalski|title=Algorithm=Logic+Control|journal=[[Communications of the ACM]]|volume=22|issue=7|pages=424–436|year=1979|id=ISSN 0001-0782|doi=10.1145/359131.359136}}
* [[A. A. Markov]] (1954) ''Theory of algorithms''. [Translated by Jacques J. Schorr-Kon and PST staff] Imprint Moscow, Academy of Sciences of the USSR, 1954 [i.e., Jerusalem, Israel Program for Scientific Translations, 1961; available from the Office of Technical Services, U.S. Dept. of Commerce, Washington] Description 444 p.&amp;nbsp;28&amp;nbsp;cm. Added t.p. in Russian Translation of Works of the Mathematical Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algerifmov. [QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical Services, number OTS 60-51085.]
* {{cite book|last=Minsky|first=Marvin|authorlink=Marvin Minsky|title=Computation: Finite and Infinite Machines|edition=First|publisher=Prentice-Hall, Englewood Cliffs, NJ|year=1967|isbn=0131654497}} Minsky expands his &quot;...idea of an algorithm — an effective procedure...&quot; in chapter 5.1 ''Computability, Effective Procedures and Algorithms. Infinite machines.&quot;
* {{cite journal|last=Post|first=Emil|authorlink=Emil Post|title=Finite Combinatory Processes, Formulation I|journal=The Journal of Symbolic Logic|volume=1|year=1936|pages=103–105|doi=10.2307/2269031|issue=3}} Reprinted in ''The Undecidable'', p. 289ff. Post defines a simple algorithmic-like process of a man writing marks or erasing marks and going from box to box and eventually halting, as he follows a list of simple instructions. This is cited by Kleene as one source of his &quot;Thesis I&quot;, the so-called [[Church-Turing thesis]].
* {{cite journal|last=Rosser|first=J.B.|authorlink=J.B. Rosser|title=An Informal Exposition of Proofs of Godel's Theorem and Church's Theorem|journal=Journal of Symbolic Logic|volume= 4 |year=1939}} Reprinted in ''The Undecidable'', p.&amp;nbsp;223ff. Herein is Rosser's famous definition of &quot;effective method&quot;: &quot;...a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps... a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer&quot; (p.&amp;nbsp;225–226, ''The Undecidable'')
* {{cite book|last=Sipser|first=Michael|title=Introduction to the Theory of Computation|publisher=PWS Publishing Company|year=2006|isbn=053494728X}}
* {{cite book|last=Stone|first=Harold S.|title=Introduction to Computer Organization and Data Structures|edition=1972|publisher=McGraw-Hill, New York|isbn=0070617260|year=1972}} Cf. in particular the first chapter titled: ''Algorithms, Turing Machines, and Programs''. His succinct informal definition: &quot;...any sequence of instructions that can be obeyed by a robot, is called an ''algorithm''&quot; (p.&amp;nbsp;4).
* {{cite journal|last=Turing|first=Alan M.|authorlink=A. M. Turing|title=On Computable Numbers, With An Application to the Entscheidungsproblem|journal=[[Proceedings of the London Mathematical Society]], Series 2|volume=42|pages= 230–265 |year=1936-7|doi=10.1112/plms/s2-42.1.230 }}. Corrections, ibid, vol. 43(1937) pp.&amp;nbsp;544–546. Reprinted in ''The Undecidable'', p.&amp;nbsp;116ff. Turing's famous paper completed as a Master's dissertation while at King's College Cambridge UK. 
* {{cite journal|last=Turing|first=Alan M.|authorlink=A. M. Turing|title=Systems of Logic Based on Ordinals|journal=Proceedings of the London Mathematical Society, Series 2|volume=45|pages=161–228|year=1939|doi=10.1112/plms/s2-45.1.161}} Reprinted in ''The Undecidable'', p.&amp;nbsp;155ff. Turing's paper that defined &quot;the oracle&quot; was his PhD thesis while at Princeton USA. 
* [[United States Patent and Trademark Office]] (2006), [http://www.uspto.gov/web/offices/pac/mpep/documents/2100_2106_02.htm ''2106.02 **&gt;Mathematical Algorithms&lt; - 2100 Patentability''], Manual of Patent Examining Procedure (MPEP). Latest revision August 2006

=== Secondary references ===
* {{cite book|last=Bolter|first=David J.|authorlink=Bolter|title=Turing's Man: Western Culture in the Computer Age|edition=1984|publisher=The University of North Carolina Press, Chapel Hill NC|isbn=0807815640|year=1984}}, ISBN 0-8078-4108-0 pbk.
* {{cite book|last=Dilson|first=Jesse|authorlink=Dilson|title=The Abacus|edition=(1968,1994)|publisher=St. Martin's Press, NY|isbn=031210409X|year=2007}}, ISBN 0-312-10409-X (pbk.)
* {{cite book|last=van Heijenoort|first=Jean|authorlink=van Heijenoort|title=From Frege to Gödel, A Source Book in Mathematical Logic, 1879–1931|edition=(1967)|publisher=Harvard University Press, Cambridge, MA|isbn=0674324498|year=2001}}, 3rd edition 1976[?], ISBN 0-674-32449-8 (pbk.)
* {{cite book|last=Hodges|first=Andrew|authorlink=Hodges|title=Alan Turing: The Enigma|edition=(1983)|publisher=Simon and Schuster, New York|isbn=0671492071|year=1983}}, ISBN 0-671-49207-1. Cf. Chapter &quot;The Spirit of Truth&quot; for a history leading to, and a discussion of, his proof.

==Further reading==
* David Harel, Yishai A. Feldman, ''Algorithmics: the spirit of computing'', Edition 3, Pearson Education, 2004, ISBN 0321117840
* Jean-Luc Chabert, Évelyne Barbin, ''A history of algorithms: from the pebble to the microchip'', Springer, 1999, ISBN 3540633693

== External links ==
* [http://www.cs.sunysb.edu/~algorith/ The Stony Brook Algorithm Repository]
* {{MathWorld | urlname=Algorithm | title=Algorithm}}
* [http://everydaymath.uchicago.edu/educators/Algorithms_final.pdf Algorithms in Everyday Mathematics]
* {{dmoz|Computers/Algorithms/|Algorithms}}
* [http://sortieralgorithmen.de/ Sortier- und Suchalgorithmen (German)]
* [http://compgeom.cs.uiuc.edu/~jeffe//teaching/algorithms/ Jeff Erickson Algorithms course material]

[[Category:Algorithms|*]]
[[Category:Arabic words and phrases]]
[[Category:Discrete mathematics]]
[[Category:Mathematical logic]]
[[Category:Theoretical computer science]]
[[Category:Articles with example pseudocode]]

[[af:Algoritme]]
[[ar:خوارزمية]]
[[an:Algorismo]]
[[ast:Algoritmu]]
[[az:Alqoritm]]
[[bn:অ্যালগোরিদম]]
[[be:Алгарытм]]
[[be-x-old:Альгарытм]]
[[bs:Algoritam]]
[[bg:Алгоритъм]]
[[ca:Algorisme]]
[[cs:Algoritmus]]
[[da:Algoritme]]
[[de:Algorithmus]]
[[et:Algoritm]]
[[el:Αλγόριθμος]]
[[es:Algoritmo]]
[[eo:Algoritmo]]
[[eu:Algoritmo]]
[[fa:الگوریتم]]
[[fr:Algorithmique]]
[[gl:Algoritmo]]
[[ko:알고리즘]]
[[hi:अल्गोरिद्म]]
[[hr:Algoritam]]
[[io:Algoritmo]]
[[id:Algoritma]]
[[ia:Algorithmo]]
[[is:Reiknirit]]
[[it:Algoritmo]]
[[he:אלגוריתם]]
[[ka:ალგორითმი]]
[[ku:Algorîtma]]
[[la:Algorithmus]]
[[lv:Algoritms]]
[[lb:Algorithmus]]
[[lt:Algoritmas]]
[[hu:Algoritmus]]
[[mk:Алгоритам]]
[[ml:അല്‍ഗൊരിതം]]
[[mr:अल्गोरिदम]]
[[arz:الجوريتم]]
[[ms:Algoritma]]
[[mn:Алгоритм]]
[[nl:Algoritme]]
[[ja:アルゴリズム]]
[[no:Algoritme]]
[[nn:Algoritme]]
[[pl:Algorytm]]
[[pt:Algoritmo]]
[[kaa:Algoritm]]
[[ro:Algoritm]]
[[ru:Алгоритм]]
[[sah:Алгоритм]]
[[sq:Algoritmi]]
[[scn:Alguritmu]]
[[si:ඇල්ගොරිතම]]
[[simple:Algorithm]]
[[sd:الخوارزمي]]
[[sk:Algoritmus]]
[[sl:Algoritem]]
[[sr:Алгоритам]]
[[sh:Algoritam]]
[[su:Algoritma]]
[[fi:Algoritmi]]
[[sv:Algoritm]]
[[tl:Algoritmo]]
[[ta:படிமுறைத் தீர்வு]]
[[te:అల్గారిథం]]
[[th:ขั้นตอนวิธี]]
[[tg:Алгоритм]]
[[tr:Algoritma]]
[[uk:Алгоритм]]
[[ur:الخوارزم]]
[[vi:Thuật toán]]
[[wa:Algorisse]]
[[war:Algoritmo]]
[[yi:אלגאריטם]]
[[zh-yue:演算法]]
[[zh:算法]]</text>
    </revision>
  </page>
  <page>
    <title>Maze solving algorithm</title>
    <id>22074859</id>
    <revision>
      <id>323957840</id>
      <timestamp>2009-11-04T20:55:28Z</timestamp>
      <contributor>
        <ip>38.98.105.130</ip>
      </contributor>
      <comment>/* Random mouse algorithm */  Clarifying algorithm's behavior.</comment>
      <text xml:space="preserve">There are a number of different '''maze solving [[algorithm]]s''', that is, automated methods for the solving of [[maze]]s. A few important maze solving algorithms are explained below. The random mouse, wall follower, pledge, and Tremaux [[algorithms]] are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the [[dead-end]] filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.

Mazes containing no loops are known as &quot;standard&quot;, or &quot;perfect&quot; mazes, and are equivalent to a [[Tree (graph theory)| ''tree'']] in graph theory. Thus many maze solving algorithms are closely related to [[graph theory]]. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree &lt;ref&gt;http://www.youtube.com/watch?v=k1tSK5V1pds&lt;/ref&gt;.

== Random mouse algorithm ==
This is a trivial method that can be implemented by a very unintelligent [[robot]] or perhaps a mouse. It is simply to proceed in a straight line until a junction is reached, and then to make a random decision about the next direction to follow.

== Wall follower ==
[[Image:maze01-02.png|right|frame|Traversal using ''right-hand rule'']]

The wall follower, the best-known rule for traversing mazes, is also known as either the ''left-hand rule'' or the ''right-hand rule''. If the maze is [[Simply connected space|''simply connected'']], that is, all its walls are connected together or to the maze's outer boundary, then by keeping one hand in contact with one wall of the maze the player is guaranteed not to get lost and will reach a different exit if there is one; otherwise, he or she will return to the entrance. This strategy works best when implemented immediately upon entering the maze.

Another perspective into why wall following works is topological. If the walls are connected, then they may be deformed into a loop or circle &lt;ref&gt;http://www.youtube.com/watch?v=IIBwiGrUgzc&lt;/ref&gt;. Then wall following reduces to walking around a circle from start to finish.

If the maze is not simply connected (i.e. if the start or endpoints are in the center of the structure or the pathways cross over and under each other), this method will not be guaranteed to help the goal to be reached.

Wall-following can be done in 3D or higher dimensional mazes if its higher dimensional passages can be projected onto the 2D plane in a deterministic manner. For example, if in a 3D maze &quot;up&quot; passages can be assumed to lead northwest, and &quot;down&quot; passages can be assumed to lead southeast, then standard wall following rules can then be applied. However, unlike in 2D, this requires that the current orientation be known, to determine which direction is the first on the left or right.

== Pledge algorithm ==
[[Image:Cyclope robot.jpg|thumb|right|Robot in a wooden maze]]

Disjoint mazes can still be solved with the wall follower method, if the entrance and exit to the maze are on the outer walls of the maze. If however, the solver starts inside the maze, it might be on a section disjoint from the exit, and wall followers will continually go around their ring. The Pledge algorithm (named after [[Jon Pledge]] of [[Exeter]]) can solve this problem (see &quot;Turtle Geometry: the computer as a medium for exploring mathematics&quot;, Abelson &amp; diSessa, 1980).

The Pledge algorithm, designed to circumvent obstacles, requires an arbitrarily chosen direction to go toward. When an obstacle is met, one hand (say the right hand) is kept along the obstacle while the angles turned are counted. When the solver is facing the original direction again, and the angular sum of the turns made is 0, the solver leaves the obstacle and continues moving in its original direction.

Note that the use of &quot;total turning&quot; rather than just the &quot;current direction&quot; allows the algorithm to avoid traps shaped like an upper case &quot;G&quot;.  If one proceeds left into the trap, one gets turned around a full 360 [[degree (angle)|degree]]s by the walls.  A naive &quot;current direction&quot; algorithm gets into a limit cycle as it leaves the lower rightmost wall heading left and runs into the curved section on the left again.  The Pledge algorithm does not leave the rightmost wall due to the total turning not being zero at that point.  It follows the wall all the way around, finally leaving it heading left on the bottom outside.

This algorithm allows a person with a compass to find his way from any point inside to an outer exit of any finite and fair two-dimensional maze, regardless of the initial position of the solver. However, this algorithm will not work in doing the reverse, namely finding the way from an entrance on the outside of a maze to some end goal within it.

== Tremaux's algorithm ==
Tremaux's algorithm is an efficient method to find the way out of a maze that requires drawing a line on the floor to mark a path, and is guaranteed to work for all mazes that have well-defined passages. On arriving at a junction, pick a direction and mark it and the direction you came from.  When arriving at a marked junction pick an unmarked passage if possible. If it is not possible to pick an unmarked passage take a marked one, marking it again (you can pick the path you came from). Never pick a twice marked path, where you will never need to take any passage more than twice. If there is no exit, this method will take you back to the start. Tremaux's algorithm effectively implements a [[depth-first search]].&lt;ref&gt;http://riemannsurfaces.info/OtherTopics/Maze%20search,%20Tremaux%20generalized.html&lt;/ref&gt;

== Dead-end filling ==
Dead-end filling is an algorithm for solving mazes that looks at the entire maze at once. It can be used for solving mazes on paper or with a computer program, but it is not useful to a person inside an unknown maze. The method is to 1) find all of the dead-ends in the maze, and then 2) &quot;fill in&quot; the path from each dead-end until the first junction met. A video of dead-end filling in action can be seen here: [http://www.youtube.com/watch?v=yqZDYcpCGAI]. 

Dead-end filling cannot accidentally &quot;cut off&quot; the start from the finish since each step of the process preserves the topology of the maze. Furthermore, the process won't stop &quot;too soon&quot; since the end result cannot contain any dead-ends. Thus if dead-end filling is done on a perfect maze (maze with no loops), then only the solution will remain. If it is done on a partially braid maze (maze with some loops), then every possible solution will remain but nothing more. [http://www.astrolog.org/labyrnth/algrithm.htm]

== Shortest path algorithm ==
When a maze has multiple solutions, the solver may want to find the shortest path from start to finish. This algorithm find the shortest path by implementing a [[breadth-first search]]. The algorithm uses a [[queue (data structure)|queue]] to visit cells in increasing distance order from the start until the finish is reached. Each visited cell needs to keep track of its distance from the start or which adjacent cell nearer to the start caused it to be added to the queue. When the finish location is found, follow the path of cells backwards to the start, which is the shortest path.

== References ==
{{reflist}}

==See also==
* [[Mazes]]
* [[Maze generation algorithm]]

==External links==
* [http://www.astrolog.org/labyrnth/algrithm.htm#solve Think Labyrinth: Maze algorithms] (details on these and other maze solving algorithms)
* [http://mazeblog.com/howtosolvemazes.html MazeBlog: How to Solve Mazes]
* [http://www.mazeworks.com/mazegen/ Maze generation and solving Java applet]
* [http://chiesaclan.spaces.live.com/blog/cns!842434EBE9688900!632.entry Maze generator and solver, in C#] - print out mazes in various shapes on paper.

[[Category:Mazes]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Spreading activation</title>
    <id>7347241</id>
    <revision>
      <id>326985416</id>
      <timestamp>2009-11-20T20:11:53Z</timestamp>
      <contributor>
        <username>Sepreece</username>
        <id>40453</id>
      </contributor>
      <minor/>
      <comment>Attached references to appropriate point in text.</comment>
      <text xml:space="preserve">'''Spreading activation''' is a method for searching [[associative network]]s, [[neural network]]s, or [[semantic network]]s. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or &quot;activation&quot; and then iteratively propagating or &quot;spreading&quot; that activation out to other nodes linked to the source nodes.  Most often these &quot;weights&quot; are real values that decay as activation propagates through the network.  When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node.

Spreading activation models are used in [[cognitive psychology]]&lt;ref name=collins&gt;Collins, Allan M.; Loftus, Elizabeth F., &quot;A spreading-activation theory of semantic processing&quot;, Psychological Review. 1975 Nov Vol 82(6) 407-428 [http://psycnet.apa.org/index.cfm?fa=search.displayRecord&amp;uid=1976-03421-001]&lt;/ref&gt;&lt;ref name=anderson&gt;John R. Anderson. [http://act-r.psy.cmu.edu/publications/pubinfo.php?id=66 &quot;A spreading activation theory of memory.&quot;] ''Journal of Verbal Learning and Verbal Behavior'', 1983&lt;/ref&gt; to model the [[fan out effect]].

Spreading activation can also be applied in [[information retrieval]]&lt;ref name=preece&gt;S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.&lt;/ref&gt;&lt;ref name=crestani&gt;Fabio Crestani. &quot;Application of Spreading Activation Techniques in Information Retrieval&quot;. ''Artificial Intelligence Review'', 1997&lt;/ref&gt;, by means of a network of nodes representing documents and terms contained in those documents.

==Algorithm==
A directed graph is populated by Nodes[ 1...N ]  each having an associated activation value A [ i ] which is a real number in the range [ 0.0 ... 1.0].  A Link[ i, j ] connects source node[ i ] with target node[ j ].  Each link has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0]&lt;ref&gt;[http://www.public.asu.edu/~hdavulcu/SA-ACMWI05.pdf Boosting item keyword search with spreading activation] Aswath, D.; Ahmed, S.T.; Dapos;cunha, J.; Davulcu, H., Web Intelligence, 2005. Proceedings. The 2005 IEEE/WIC/ACM International Conference on
Volume , Issue , 19-22 Sept. 2005 Page(s): 704 - 707&lt;/ref&gt; . 

Parameters:
* Firing threshold F, a real number in the range [0.0 ... 1.0]
* Decay factor D, a real number in the range [0.0 ... 1.0]

Steps:
# Initialize the graph setting all activation values A [ i ] to zero.   Set one or more origin nodes to an initial activation value greater than the firing threshold F.  A typical initial value is 1.0.
# For each unfired node [ i ] in the graph having an activation value A [ i ] greater than the node firing threshold F: 
# For each Link [ i , j ] connecting the source node [ i ] with target node [ j ], adjust A [ j ] = A [ j ] + (A [ i ] * W [ i, j ] * D) where D is the decay factor.
# If a target node receives an adjustment to its activation value so that it would exceed 1.0, then set its new activation value to 1.0.  Likewise maintain 0.0 as a lower bound on the target node's activation value should it receive an adjustment to below 0.0.
# Once a node has fired it may not fire again, although variations of the basic algorithm permit repeated firings and loops through the graph.
# Nodes receiving a new activation value that exceeds the firing threshold F are marked for firing on the next spreading activation cycle.
# If activation originates from more than one node, a variation of the algorithm permits marker passing to distinguish the paths by which activation is spread over the graph
# The procedure terminates when either there are no more nodes to fire or in the case of marker passing from multiple origins, when a node is reached from more than one path. Variations of the algorithm that permit repeated node firings and activation loops in the graph, terminate after a steady activation state, with respect to some delta, is reached, or when a maximum number of iterations is exceeded.

==See also==
* [[Connectionism]]
==Examples==
[[Image:Spreading-activation-graph-1.png|thumb|center|400px|In this example, spreading activation originated at node 1 which has an initial activation value of 1.0 (100%).  Each link has the same weight value of 0.5.  The decay factor was 0.85.  Four cycles of spreading activation have occurred.  Color hue and saturation indicate different activation values.]]

==Notes==
&lt;references /&gt;

==References==
* Nils J. Nilsson. &quot;Artificial Intelligence: A New Synthesis&quot;. Morgan Kaufmann Publishers, Inc., San Francisco, California, 1998, pages 121-122
* Rodriguez, M.A., [http://arxiv.org/abs/0803.4355&quot; Grammar-Based Random Walkers in Semantic Networks&quot;], ''Knowledge-Based Systems'', 21(7), 727-739, doi:10.1016/j.knosys.2008.03.030, 2008.

==External links==
* [http://lead.asu.edu/home/jmapss JMaPSS] The Java Marker-Passing Search Service, a relevance search engine employing a family of marker-passing algorithms based on spreading activation theory.
* [http://texai.org Texai] An open source project to create artificial intelligence that provides a Java spreading activation class library.

[[Category:Semantics]]
[[Category:Psycholinguistics]]
[[Category:Memory]]
[[Category:Artificial intelligence]]
[[Category:Algorithms]]
[[Category:Search algorithms]]

[[de:Aktivierungsausbreitung]]</text>
    </revision>
  </page>
  <page>
    <title>Randomization function</title>
    <id>3578575</id>
    <revision>
      <id>283594297</id>
      <timestamp>2009-04-13T16:58:54Z</timestamp>
      <contributor>
        <username>Crashoffer12345</username>
        <id>9403702</id>
      </contributor>
      <text xml:space="preserve">In [[computer science]], a '''randomization function''' or '''randomizing function''' is an [[algorithm]] or [[procedure]] that implements a [[random]]ly chosen [[function (mathematics)|function]] between two specific [[set (mathematics)|set]]s, suitable for use in a [[randomized algorithm]].

Randomizing functions are related to [[random number generator]]s and [[hash function]]s, but have somewhat different requirements and uses, and often need specific algorithms.

==Uses==

Randomizing functions are used to turn algorithms that have good [[expected value|expected]] performance for ''random'' inputs, into algorithms that have the same performance for ''any'' input.

For example, consider a [[sorting algorithm]] like [[quicksort]], which has small expected running time when the input items are presented in random order, but is very slow when they are presented in certain unfavorable orders. A randomizing function from the integers 1 to ''n'' to the integers 1 to ''n'' can be used used to rerrange the ''n'' input items in &quot;random&quot; order, before calling that algorithm.  This modified (randomized) algorithm will have small expected running time, whatever the input order.

==Requirements==

===Randomness===
In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed.  The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time).  With such a &quot;pseudo-randomization&quot; function, one could in principle construct a sequence of calls such that the function would always yield a &quot;bad&quot; case for the underlying deterministic algorithm.  For that sequence of calls, the average cost would be closer to the worst-case cost, rather than the average cost for random inputs.

In practice, however, the main concern is that some &quot;bad&quot; cases for the deterministic algorithm may occur in practice much more often than it would be predicted by chance.  For example, in a naive variant of quicksort, the worst case is when the input items are already sorted — which is a very common occurrence in many applications.  For such algorithms, even a fixed pseudo-random permutation may be good enough.  Even though the resulting &quot;pseudo-randomized&quot; algorithm would still have as many &quot;bad&quot; cases as the original, they will be certain peculiar orders that would be quite unlikely to arise in real applications.  So, in practice one often uses randomization functions that are derived from [[pseudo-random number generator]]s, preferably [[random seed|seeded]] with external &quot;random&quot; data such as the program's startup time.

===Uniformity===
The uniformiy requirements for a randomizing function are usually much weaker than those of hash functions and pseudo-random generators.  The minimum requirement is that it maps any input of the deterministic algorithm into a &quot;good&quot; input with a sufficiently high probability.  (However, analysis is usually simpler if the randomizing function implements each possible mapping with uniform probability.)

==References==
{{unreferenced|date=April 2009}}

{{comp-sci-stub}}
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Simulation Algorithms for Atomic DEVS</title>
    <id>22509875</id>
    <revision>
      <id>328804032</id>
      <timestamp>2009-11-30T14:25:44Z</timestamp>
      <contributor>
        <ip>91.103.41.50</ip>
      </contributor>
      <comment>/* View 2: Total States = States * Lifespans * Elapsed Times */</comment>
      <text xml:space="preserve">Given an [[DEVS#Atomic DEVS|atomic DEVS]] model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see [[Behavior of DEVS]]). [[Simulation_Algorithms_for_Atomic_DEVS#References|[Zeigler84]]] originally introduced the algorithms that handle time variables related to ''lifespan'' &lt;math&gt;t_s \in [0,\infty]&lt;/math&gt; and ''elapsed time'' &lt;math&gt;t_e\in [0,\infty)&lt;/math&gt; by introducing two other time variables, ''last event time'', &lt;math&gt;t_l\in [0,\infty)&lt;/math&gt;, and ''next event time'' &lt;math&gt; t_n\in [0,\infty]&lt;/math&gt; with the following relations: 
&lt;center&gt;&lt;math&gt; t_e = t - t_l &lt;/math&gt;&lt;/center&gt;

and

&lt;center&gt;&lt;math&gt; t_s = t_n - t_l &lt;/math&gt;&lt;/center&gt;

where &lt;math&gt;t\in [0,\infty)&lt;/math&gt; denotes the ''current time''. And the ''remaining time'', &lt;math&gt;t_r=t_s-t_e&lt;/math&gt; is equivalently computed as
&lt;center&gt;&lt;math&gt; t_r = t_n - t&lt;/math&gt;&lt;/center&gt;, apparently &lt;math&gt; t_r \in [0,\infty]&lt;/math&gt;.


Since the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to [[Behavior of DEVS]]), the simulation algorithms are also introduced in two different views as below.
== Common Parts ==
Regardless of two different views of total states, algorithms for initialization and internal transition cases are commonly defined as below.

 DEVS-Simulator
   variables:
     parent // parent coordinator
     &lt;math&gt;t_l&lt;/math&gt;     // time of last event
     &lt;math&gt;t_n&lt;/math&gt;     // time of next event
     &lt;math&gt;A=(X,Y,S,ta, \delta_{ext}, \delta_{int}, \lambda) &lt;/math&gt;// the associated [[DEVS#Atomic DEVS|Atomic DEVS]] model 
   when receive init-message(Time &lt;math&gt;t&lt;/math&gt;)
      &lt;math&gt; t_l \leftarrow t;&lt;/math&gt;
      &lt;math&gt; t_n \leftarrow t_l + ta(s); &lt;/math&gt;
   when receive star-message(Time &lt;math&gt;t&lt;/math&gt;)
      if &lt;math&gt; t \ne t_n &lt;/math&gt; then
         error: bad synchronization;
      &lt;math&gt; y \leftarrow \lambda(s);&lt;/math&gt;
      send y-message(&lt;math&gt;y,t&lt;/math&gt;) to parent;
      &lt;math&gt; s \leftarrow \delta_{int}(s)&lt;/math&gt;
      &lt;math&gt; t_l \leftarrow t;&lt;/math&gt;
      &lt;math&gt; t_n \leftarrow t_l + ta(s); &lt;/math&gt;

== View 1: Total States = States * Elapsed Times ==
As addressed in [[Behavior of Atomic DEVS]], when DEVS receives an input event, right calling &lt;math&gt;\delta_{ext}&lt;/math&gt;, the last event time,&lt;math&gt;t_l&lt;/math&gt; is set by the current time,&lt;math&gt;t&lt;/math&gt; , thus the elapsed time&lt;math&gt;t_e&lt;/math&gt; becomes zero because &lt;math&gt;t_e = t - t_l&lt;/math&gt;.

   when receive x-message(&lt;math&gt;x \in X&lt;/math&gt;, Time &lt;math&gt;t&lt;/math&gt;)
      if &lt;math&gt;( t_l \le t &lt;/math&gt; and &lt;math&gt; t \le t_n )&lt;/math&gt; == false then
         error: bad synchronization;
      &lt;math&gt; s \leftarrow \delta_{ext}(s,t-t_l, x)&lt;/math&gt;
      &lt;math&gt; t_l \leftarrow t;&lt;/math&gt;
      &lt;math&gt; t_n \leftarrow t_l + ta(s); &lt;/math&gt;

== View 2: Total States = States * Lifespans * Elapsed Times  ==
Notice that as addressed in [[Behavior of Atomic DEVS]], depending on the value of &lt;math&gt;b&lt;/math&gt; return by &lt;math&gt; \delta_{ext}&lt;/math&gt;, last event time,&lt;math&gt;t_l&lt;/math&gt;, and next event time,&lt;math&gt;t_n&lt;/math&gt;,consequently, elapsed time, &lt;math&gt;t_e&lt;/math&gt;, and lifespan&lt;math&gt;t_n&lt;/math&gt;, are updated (if &lt;math&gt;b=1&lt;/math&gt;) or preserved (if &lt;math&gt;b=0&lt;/math&gt;).

   when receive x-message(&lt;math&gt;x \in X&lt;/math&gt;, Time &lt;math&gt;t&lt;/math&gt;)
      if &lt;math&gt;( t_l \le t &lt;/math&gt; and &lt;math&gt; t \le t_n )&lt;/math&gt; == false then
         error: bad synchronization;
      &lt;math&gt; (s,b) \leftarrow \delta_{ext}(s, t-t_l, x)&lt;/math&gt;
      if &lt;math&gt; b = 1 &lt;/math&gt; then 
         &lt;math&gt; t_l \leftarrow t;&lt;/math&gt;
         &lt;math&gt; t_n \leftarrow t_l + ta(s); &lt;/math&gt;

==See also==
* [[DEVS#Atomic DEVS|Atomic DEVS]] 
* [[Behavior of Atomic DEVS]]
* [[Simulation Algorithms for Coupled DEVS]]

== References ==
* [Zeigler84] {{cite book|author = Bernard Zeigler | year = 1984| title = Multifacetted Modeling and Discrete Event Simulation | publisher = Academic Press, London; Orlando | id = ISBN 978-0127784502  }}
* [ZKP00] {{cite book|author = Bernard Zeigler, Tag Gon Kim, Herbert Praehofer| year = 2000| title = Theory of Modeling and Simulation| publisher = Academic Press, New York  | id= ISBN 978-0127784557 |edition=second}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Simulation Algorithms for Coupled DEVS</title>
    <id>22513037</id>
    <revision>
      <id>302783367</id>
      <timestamp>2009-07-18T15:25:08Z</timestamp>
      <contributor>
        <username>Mhhwang2002</username>
        <id>5725715</id>
      </contributor>
      <text xml:space="preserve">Given a coupled DEVS model, simulation algorithms are methods to generate the model's ''legal'' behaviors, which are a set of trajectories not to reach illegal states. (see [[Behavior of Coupled DEVS|behavior of a Coupled DEVS]] model.)  [[Simulation_Algorithms_for_Coupled_DEVS#References|[Zeigler84]]] originally introduced the algorithms that handle time variables related to ''lifespan'' &lt;math&gt;t_s \in [0,\infty]&lt;/math&gt; and ''elapsed time'' &lt;math&gt;t_e\in [0,\infty)&lt;/math&gt; by introducing two other time variables, ''last event time'', &lt;math&gt;t_l\in [0,\infty)&lt;/math&gt;, and ''next event time'' &lt;math&gt; t_n\in [0,\infty]&lt;/math&gt; with the following relations: 
&lt;center&gt;&lt;math&gt; t_e = t - t_l &lt;/math&gt;&lt;/center&gt;

and

&lt;center&gt;&lt;math&gt; t_s = t_n - t_l &lt;/math&gt;&lt;/center&gt;

where &lt;math&gt;t\in [0,\infty)&lt;/math&gt; denotes the ''current time''. And the ''remaining time'', &lt;math&gt;t_r=t_s-t_e&lt;/math&gt; is equivalently computed as
&lt;center&gt;&lt;math&gt; t_r = t_n - t&lt;/math&gt;&lt;/center&gt;, apparently &lt;math&gt; t_r \in [0,\infty]&lt;/math&gt;.

Based on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.

== Algorithms ==

 DEVS-coordinator
   Variables:
      parent // parent coordinator
      &lt;math&gt;t_l&lt;/math&gt;: // time of last event
      &lt;math&gt;t_n&lt;/math&gt;: // time of next event
      &lt;math&gt;N=(X, Y, D, \{M_i\}, C_{xx}, C_{yx}, C_{yy},Select)&lt;/math&gt;// the associated [[DEVS#Coupled DEVS|Coupled DEVS]] model
   when receive init-message(Time ''t'')
      for each &lt;math&gt; i \in D &lt;/math&gt; do
         send init-message(''t'') to child &lt;math&gt;i&lt;/math&gt;
      &lt;math&gt;t_l \leftarrow \max\{t_{li}: i \in D\}&lt;/math&gt;;
      &lt;math&gt;t_n \leftarrow \min\{t_{ni}: i \in D\}&lt;/math&gt;;
   when receive star-message(Time ''t'')
      if &lt;math&gt;t \ne t_n &lt;/math&gt; then
         error: bad synchronization;
      &lt;math&gt;i^* \leftarrow Select(\{i \in D: t_{ni} = t_n\});&lt;/math&gt;
      send star-message(''t'')to &lt;math&gt;i^*&lt;/math&gt;
      &lt;math&gt;t_l \leftarrow \max\{t_{li}: i \in D\}&lt;/math&gt;;
      &lt;math&gt;t_n \leftarrow \min\{t_{ni}: i \in D\}&lt;/math&gt;;
   when receive x-message(&lt;math&gt;x \in X&lt;/math&gt;, Time ''t'')
      if &lt;math&gt;( t_l \le t &lt;/math&gt; and &lt;math&gt; t \le t_n )&lt;/math&gt; == false then
         error: bad synchronization;
      for each &lt;math&gt; (x,x_i) \in C_{xx} &lt;/math&gt; do
         send x-message(&lt;math&gt;x_i&lt;/math&gt;,''t'') to child &lt;math&gt;i&lt;/math&gt;
      &lt;math&gt;t_l \leftarrow \max\{t_{li}: i \in D\}&lt;/math&gt;;
      &lt;math&gt;t_n \leftarrow \min\{t_{ni}: i \in D\}&lt;/math&gt;;
   when receive y-message(&lt;math&gt;y_i \in Y_i&lt;/math&gt;, Time ''t'')
      for each &lt;math&gt; (y_i,x_i) \in C_{yx} &lt;/math&gt; do
         send x-message(&lt;math&gt;x_i&lt;/math&gt;,''t'') to child &lt;math&gt;i&lt;/math&gt;
      if &lt;math&gt;C_{yy}(y_i)\ne \phi&lt;/math&gt; then
         send y-message(&lt;math&gt;C_{yy}(y_i)&lt;/math&gt;, ''t'') to parent;
      &lt;math&gt;t_l \leftarrow \max\{t_{li}: i \in D\}&lt;/math&gt;;
      &lt;math&gt;t_n \leftarrow \min\{t_{ni}: i \in D\}&lt;/math&gt;;

==See also==
* [[DEVS#Coupled DEVS|Coupled DEVS]] 
* [[Behavior of Coupled DEVS]]
* [[Simulation Algorithms for Atomic DEVS]]

== References ==
* [Zeigler84] {{cite book|author = Bernard Zeigler | year = 1984| title = Multifacetted Modeling and Discrete Event Simulation | publisher = Academic Press, London; Orlando | id = ISBN 978-0127784502  }}
* [ZKP00] {{cite book|author = Bernard Zeigler, Tag Gon Kim, Herbert Praehofer| year = 2000| title = Theory of Modeling and Simulation| publisher = Academic Press, New York  | id= ISBN 978-0127784557 |edition=second}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Algorithmic Lovász local lemma</title>
    <id>22474664</id>
    <revision>
      <id>316050384</id>
      <timestamp>2009-09-25T02:49:47Z</timestamp>
      <contributor>
        <username>RobinK</username>
        <id>10041416</id>
      </contributor>
      <minor/>
      <text xml:space="preserve">In [[theoretical computer science]], the '''algorithmic Lovász local lemma''' gives an algorithmic way of constructing objects that obey a system of constraints with limited dependence.

Given a finite set of ''bad'' events &lt;math&gt;\{ A_1, \ldots, A_n \}&lt;/math&gt; in a probability space with limited dependence amongst the &lt;math&gt; A_i &lt;/math&gt;'s and with specific bounds on their respective probabilities, the [[Lovász local lemma]] proves that with non-zero probability all of these events can be avoided. However, the lemma is non-constructive in that it does not provide any insight on ''how'' to avoid the bad events.

If the events &lt;math&gt;\{ A_1, \ldots, A_n \}&lt;/math&gt; are determined by a finite collection of mutually independent random variables, a simple [[Las Vegas algorithm]] with [[ZPP (complexity)|expected polynomial runtime]] proposed by [[Robin Moser]] and [[Gábor Tardos]]&lt;ref name=&quot;moser:arxiv09&quot;&gt;{{citation|first1=Robin A.|last1=Moser|first2=Gabor|last2=Tardos|id={{arxiv|0903.0544}}|title=A constructive proof of the general Lovász Local Lemma|year=2009}}.&lt;/ref&gt; can compute an assignment to the random variables such that all events are avoided.

==Review of Lovász local lemma==
{{main|Lovász local lemma}}

The Lovász Local Lemma is a powerful tool commonly used in the [[probabilistic method]] to prove the existence of certain complex mathematical objects with a set of prescribed features. A typical proof proceeds by operating on the complex object in a random manner and uses the Lovász Local Lemma to bound the probability that any of the features is missing. The absence of a feature is considered a ''bad event'' and if it can be shown that all such bad events can be avoided simultaneously with non-zero probability, the existence follows. The lemma itself reads as follows:

Let &lt;math&gt; \mathcal{A} = \{ A_1, \ldots, A_n \}&lt;/math&gt; be a finite set of events in the probability space &lt;math&gt; \Omega &lt;/math&gt;. For &lt;math&gt; A \in \mathcal{A} &lt;/math&gt; let &lt;math&gt; \Gamma(A)&lt;/math&gt; denote a subset of &lt;math&gt; \mathcal{A} &lt;/math&gt; such that &lt;math&gt; A &lt;/math&gt; is independent from the collection of events &lt;math&gt; \mathcal{A} \setminus (\{A \} \cup \Gamma(A)) &lt;/math&gt;. If there exists an assignment of reals &lt;math&gt; x : \mathcal{A} \rightarrow (0,1) &lt;/math&gt; to the events such that

:&lt;math&gt; \forall A \in \mathcal{A} : \Pr[A] \leq x(A) \prod_{B \in \Gamma(A)} (1-x(B)) &lt;/math&gt;

then the probability of avoiding all events in &lt;math&gt; \mathcal{A} &lt;/math&gt; is positive, in particular

:&lt;math&gt; \Pr\left[\,\overline{A_1} \wedge \ldots \wedge \overline{A_n}\,\right] \geq \prod_{A \in \mathcal{A}} (1-x(A)). &lt;/math&gt;

==Algorithmic version of the Lovász local lemma==
The Lovász Local Lemma is non-constructive because it only allows us to conclude the existence of structural properties or complex objects but does not indicate how these can be found or constructed efficiently in practice. Note that random sampling from the probability space &lt;math&gt; \Omega &lt;/math&gt; is likely to be inefficient, since the probability of the event of interest &lt;math&gt; Pr\left[\overline{A_1} \wedge \ldots \wedge \overline{A_n}\right]&lt;/math&gt; is only bounded by a product of small numbers &lt;math&gt; \prod_{A \in \mathcal{A}} (1-x(A)) &lt;/math&gt; and therefore likely to be very small.

Under the assumption that all of the events in &lt;math&gt; \mathcal{A} &lt;/math&gt; are determined by a finite collection of mutually independent [[Random Variable|random variables]] &lt;math&gt; \mathcal{P} &lt;/math&gt; in &lt;math&gt; \Omega &lt;/math&gt;, [[Robin Moser]] and [[Gábor Tardos]] proposed an efficient randomized algorithm that computes an assignment to the random variables in &lt;math&gt; \mathcal{P} &lt;/math&gt; such that all events in &lt;math&gt; \mathcal{A} &lt;/math&gt; are avoided.

Hence, this algorithm can be used to efficiently construct witnesses of complex objects with prescribed features for most problems to which the Lovász Local Lemma applies.

===History===
Prior to the recent work of Moser and Tardos, earlier work had also made progress in developing algorithmic versions of the Lovász Local Lemma.  [[József_Beck|József Beck]] in 1991 first gave proof that an algorithmic version was possible.&lt;ref name=beck:rsa91&gt;{{citation|first=József|last=Beck|authorlink=József Beck|title=An algorithmic approach to the Lovász Local Lemma. I|journal=Random Structures and Algorithms|volume=2|issue=4|pages=343–366|year=1991}}.&lt;/ref&gt; In this breakthrough result, a stricter requirement was imposed upon the problem formulation than in the original non-constructive definition.  Beck's approach required that for each &lt;math&gt;A \in \mathcal{A}&lt;/math&gt;, the number of dependencies of ''A'' was bounded above with &lt;math&gt;|\Gamma(A)| &lt; 2^{\frac{k}{48}}&lt;/math&gt; (approximately).  The existential version of the Local Lemma permits a larger upper bound on dependencies:

:&lt;math&gt;|\Gamma(A)| &lt; \frac{2^{k}}{e},&lt;/math&gt;

This bound is known to be tight.  Since the initial algorithm, work has been done to push algorithmic versions of the Local Lemma closer to this tight value.  Moser and Tardos's recent work are the most recent in this chain, and provide an algorithm that achieves this tight bound.

===Algorithm===
Let us first introduce some concepts that are used in the algorithm.

For any random variable &lt;math&gt; P \in \mathcal{P},\; v_P &lt;/math&gt; denotes the current assignment (evaluation) of &lt;math&gt; P &lt;/math&gt;. An assignment (evaluation) to all random variables is denoted &lt;math&gt; (v_P)_{\mathcal{P}}&lt;/math&gt;.&lt;br /&gt;
The unique minimal subset of random variables in &lt;math&gt; \mathcal{P} &lt;/math&gt; that determine the event &lt;math&gt; A &lt;/math&gt; is denoted by &lt;math&gt; \text{vbl}(A) &lt;/math&gt;.&lt;br /&gt;
If the event &lt;math&gt; A &lt;/math&gt;  is true under an evaluation &lt;math&gt; (v_P)_{\mathcal{P}}&lt;/math&gt;, we say that &lt;math&gt; (v_P)_{\mathcal{P}}&lt;/math&gt; '''satisfies''' &lt;math&gt; A &lt;/math&gt;, otherwise it '''avoids''' &lt;math&gt; A &lt;/math&gt;.

Given a set of bad events &lt;math&gt; \mathcal{A} &lt;/math&gt; we wish to avoid that is determined by a collection of mutually independent random variables &lt;math&gt; \mathcal{P} &lt;/math&gt;, the algorithm proceeds as follows:

# &lt;math&gt; \forall P \in \mathcal{P} &lt;/math&gt;: &lt;math&gt; v_p \leftarrow &lt;/math&gt; a random evaluation of P
# '''while''' &lt;math&gt; \exists A \in \mathcal{A}&lt;/math&gt; such that A is satisfied by &lt;math&gt; (v_P)_{\mathcal{P}}&lt;/math&gt;
#* pick an arbitrary satisfied event &lt;math&gt; A \in \mathcal{A}&lt;/math&gt;
#* &lt;math&gt; \forall P \in \text{vbl}(A) &lt;/math&gt;: &lt;math&gt; v_p \leftarrow &lt;/math&gt; a new random evaluation of P
# '''return''' &lt;math&gt; (v_P)_{\mathcal{P}}&lt;/math&gt;

In the first step, the algorithm randomly initializes the current assignment &lt;math&gt; v_P &lt;/math&gt; for each random variable &lt;math&gt; P \in \mathcal{P}&lt;/math&gt;. This means that an assignment &lt;math&gt; v_P &lt;/math&gt; is sampled randomly and independently according to the distribution of the random variable &lt;math&gt; P &lt;/math&gt;.&lt;br /&gt;
The algorithm then enters the main loop which is executed until all events in &lt;math&gt; \mathcal{A} &lt;/math&gt; are avoided and which point the algorithm returns the current assignment. At each iteration of the main loop, the algorithm picks an arbitrary satisfied event &lt;math&gt; A &lt;/math&gt; (either randomly or deterministically) and resamples all the random variables that determine &lt;math&gt; A &lt;/math&gt;.

===Main theorem===
Let &lt;math&gt; \mathcal{P} &lt;/math&gt; be a finite set of mutually independent random variables in the probability space &lt;math&gt; \Omega &lt;/math&gt;. Let &lt;math&gt; \mathcal{A} &lt;/math&gt; be a finite set of events determined by these variables. If there exists an assignment of reals &lt;math&gt; x : \mathcal{A} \rightarrow (0,1) &lt;/math&gt; to the events such that

:&lt;math&gt; \forall A \in \mathcal{A} : Pr[A] \leq x(A) \prod_{B \in \Gamma(A)} (1-x(B)) &lt;/math&gt;

then there exists an assignment of values to the variables &lt;math&gt;\mathcal{P}&lt;/math&gt; avoiding all of the events in &lt;math&gt; \mathcal{A} &lt;/math&gt;.&lt;br /&gt;
Moreover, the randomized algorithm described above resamples an event &lt;math&gt; A \in \mathcal{A} &lt;/math&gt; at most an expected &lt;math&gt; \frac{x(A)}{1-x(A)}&lt;/math&gt; times before it finds such an evaluation. Thus the expected total number of resampling steps and therefore the expected runtime of the algorithm is at most &lt;math&gt; \sum_{A \in \mathcal{A}} \frac{x(A)}{1-x(A)}&lt;/math&gt;.

The proof of this theorem can be found in the paper by Moser and Tardos &lt;ref name=&quot;moser:arxiv09&quot;/&gt;

===Symmetric version===
The requirement of an assignment function &lt;math&gt; x &lt;/math&gt; satisfying a set of inequalities in the theorem above is complex and not intuitive. But this requirement can be replaced by three simple conditions:
* &lt;math&gt; \forall A \in \mathcal{A}: \Gamma(A) \leq D &lt;/math&gt;, i.e. each event &lt;math&gt; A &lt;/math&gt; depends on at most &lt;math&gt; D &lt;/math&gt; other events,
* &lt;math&gt; \forall A \in \mathcal{A}: \Pr[A] \leq  p &lt;/math&gt;, i.e. the probability of each event &lt;math&gt; A &lt;/math&gt; is at most &lt;math&gt; p &lt;/math&gt;,
* &lt;math&gt; \text{ep}(D+1) \leq 1 &lt;/math&gt;, where &lt;math&gt; e &lt;/math&gt; is the [[e (mathematical constant)|base of the natural logarithm]].

The version of the Lovász Local Lemma with these three conditions instead of the assignment function ''x'' is called the ''Symmetric Lovász Local Lemma''. Correspondingly, we can state the ''Symmetric Algorithmic Lovász Local Lemma'':

Let &lt;math&gt; \mathcal{P} &lt;/math&gt; be a finite set of mutually independent random variables and &lt;math&gt; \mathcal{A} &lt;/math&gt; be a finite set of events determined by these variables as before. If the above three conditions hold then there exists an assignment of values to the variables &lt;math&gt;\mathcal{P}&lt;/math&gt; avoiding all of the events in &lt;math&gt; \mathcal{A} &lt;/math&gt;.

Moreover, the randomized algorithm described above resamples an event &lt;math&gt; A \in \mathcal{A} &lt;/math&gt; at most an expected &lt;math&gt; \frac{1}{D}&lt;/math&gt; times before it finds such an evaluation. Thus the expected total number of resampling steps and therefore the expected runtime of the algorithm is at most &lt;math&gt; \frac{n}{D}&lt;/math&gt;.

==Example==
The following example illustrates how the algorithmic version of the Lovász Local Lemma can be applied to a simple problem.

Let &lt;math&gt; \Phi &lt;/math&gt; be a [[Conjunctive_normal_form|CNF]] formula over variables &lt;math&gt; X_1, \ldots, X_n &lt;/math&gt;, containing &lt;math&gt; n &lt;/math&gt; clauses, and with at least &lt;math&gt; k &lt;/math&gt; [[Literal_(mathematical_logic)|literals]] in each clause, and with each variable &lt;math&gt; X_i &lt;/math&gt; appearing in at most &lt;math&gt; \frac{2^k}{ke} &lt;/math&gt; clauses. Then, &lt;math&gt; \Phi &lt;/math&gt; is satisfiable.

This statement can be proven easily using the symmetric version of the Algorithmic Lovász Local Lemma. Let &lt;math&gt; X_1, \ldots, X_n &lt;/math&gt; be the set of mutually independent random variables &lt;math&gt; \mathcal{P} &lt;/math&gt; which are sampled [[Uniform_distribution_(discrete)|uniformly at random]].&lt;br /&gt;
Firstly, we truncate each clause in &lt;math&gt; \Phi &lt;/math&gt; to contain exactly &lt;math&gt; k &lt;/math&gt; literals. Since each clause is a disjunction, this does not harm satisfiability, for if we can find a satisfying assignment for the truncated formula, it can easily be extended to a satisfying assignment for the original formula by reinserting the truncated literals.

Now, define a bad event &lt;math&gt; A_j &lt;/math&gt; for each clause in &lt;math&gt; \Phi &lt;/math&gt;, where &lt;math&gt; A_j &lt;/math&gt; is the event that clause &lt;math&gt; j &lt;/math&gt; in &lt;math&gt; \Phi &lt;/math&gt; is unsatisfied by the current assignment. Since each clause contains &lt;math&gt; k &lt;/math&gt; literals (and therefore &lt;math&gt; k &lt;/math&gt; variables) and since all variables are sampled uniformly at random, we can bound the probability of each bad event by &lt;math&gt; Pr[A_j] = p = 2^{-k} &lt;/math&gt;. Since each variable can appear in at most &lt;math&gt; \frac{2^k}{ke} &lt;/math&gt; clauses and there are &lt;math&gt; k &lt;/math&gt; variables in each clause, each bad event &lt;math&gt; A_j &lt;/math&gt; can depend on at most &lt;math&gt; D = k\left(\frac{2^k}{ke}-1\right) \leq \frac{2^k}{e} -1 &lt;/math&gt; other events.

Finally, since  &lt;math&gt; D \leq \frac{2^k}{e} -1 \Rightarrow D+1 \leq \frac{2^k}{e}&lt;/math&gt; and therefore &lt;math&gt; ep(D+1) \leq e 2^{-k} \frac{2^k}{e} = 1 &lt;/math&gt; it follows by the symmetric Lovász Local Lemma that the probability of a random assignment to &lt;math&gt; X_1, \ldots, X_n &lt;/math&gt; satisfying all clauses in &lt;math&gt; \Phi &lt;/math&gt; is non-zero and hence such an assignment must exist.

Now, the Algorithmic Lovász Local Lemma actually allows us to efficiently compute such an assignment by applying the algorithm described above. The algorithm proceeds as follows:&lt;br /&gt;
It starts with a random truth value assignment to the variables &lt;math&gt; X_1, \ldots, X_n &lt;/math&gt; sampled uniformly at random. While there exists a clause in &lt;math&gt; \Phi &lt;/math&gt; that is unsatisfied, it randomly picks an unsatisfied clause &lt;math&gt; C &lt;/math&gt; in &lt;math&gt; \Phi &lt;/math&gt; and assigns a new truth value to all variables that appear in &lt;math&gt; C &lt;/math&gt; chosen uniformly at random. Once all clauses in &lt;math&gt; \Phi &lt;/math&gt; are satisfied, the algorithm returns the current assignment.

This algorithm is in fact identical to [[WalkSAT]] which is used to solve general [[Boolean_satisfiability_problem|boolean satisfiability problems]]. Hence, the Algorithmic Lovász Local Lemma proves that [[WalkSAT]] has an expected runtime of at most &lt;math&gt; \frac{n}{\frac{2^k}{e}-k} &lt;/math&gt; steps on CNF formulas that satisfy the two conditions above.

A stronger version of the above statement is proven by Moser.&lt;ref&gt;{{citation|title=A constructive proof of the Lovász Local Lemma|first=Robin A.|last=Moser|year=2008|id={{arxiv|0810.4812}}}}.&lt;/ref&gt;

==Applications==
As mentioned before, the Algorithmic Version of the Lovász Local Lemma applies to most problems for which the general Lovász Local Lemma is used as a proof technique. Some of these problems are discussed in the following articles:

* [[Probabilistic proofs of non-probabilistic theorems]]
* [[Random graph]]

==Parallel version==
The algorithm described above lends itself well to parallelization, since resampling two independent events &lt;math&gt; A,B \in \mathcal{A}&lt;/math&gt;, i.e. &lt;math&gt; \text{vbl}(A) \cap \text{vbl}(B) = \emptyset &lt;/math&gt;, in parallel is equivalent to resampling &lt;math&gt; A,B &lt;/math&gt; sequentially. Hence, at each iteration of the main loop one can determine the maximal set of independent and satisfied events &lt;math&gt; S &lt;/math&gt; and resample all events in &lt;math&gt; S &lt;/math&gt; in parallel.

Under the assumption that the assignment function &lt;math&gt; x &lt;/math&gt; satisfies the slightly stronger conditions:&lt;br /&gt;

:&lt;math&gt; \forall A \in \mathcal{A} : \Pr[A] \leq (1 - \epsilon) x(A) \prod_{B \in \Gamma(A)} (1-x(B)) &lt;/math&gt;&lt;br /&gt;

for some &lt;math&gt; \epsilon &gt; 0 &lt;/math&gt; Moser and Tardos proved that the parallel algorithm achieves a better runtime complexity. In this case, the parallel version of the algorithm takes an expected 

:&lt;math&gt; O\left(\frac{1}{\epsilon} \log \sum_{A \in \mathcal{A}} \frac{x(A)}{1-x(A)}\right) &lt;/math&gt;

steps before it terminates.  The parallel version of the algorithm can be seen as a special case of the sequential algorithm shown above, and so this result also holds for the sequential case.

==References== 
{{reflist}}

[[Category:Probability theorems]]
[[Category:Combinatorics]]
[[Category:Lemmas]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>FEE method</title>
    <id>21674054</id>
    <revision>
      <id>302917362</id>
      <timestamp>2009-07-19T09:21:15Z</timestamp>
      <contributor>
        <username>Jimp</username>
        <id>275655</id>
      </contributor>
      <comment>/* FEE-computation of classical constants */</comment>
      <text xml:space="preserve">In mathematics, the '''method FEE''' is the method of fast summation of series of a special form.  It was constructed in 1990 by E.A. Karatsuba&lt;ref&gt;E.A. Karatsuba,
Fast evaluations of transcendental functions. Probl. Peredachi
Informat., Vol. 27, N 4,  (1991)&lt;/ref&gt;&lt;ref&gt;D.W. Lozier and F.W.J. Olver,
Numerical Evaluation of Special Functions. Mathematics of
Computation 1943-1993: A Half -Century of Computational Mathematics,
W.Gautschi,eds., Proc. Sympos. Applied Mathematics, AMS, Vol.48 (1994).&lt;/ref&gt; 
and was called '''FEE''' --- '''Fast E-function Evaluation'''
--- for the reason that it makes it possible to compute fast the
Siegel '''&lt;math&gt;E&lt;/math&gt; -functions''', and in particular, '''&lt;math&gt;e^x.&lt;/math&gt;''' 

A class of functions, which are 'similar to the exponential function' was
given the name 'E-functions' by [[Carl Ludwig Siegel|Siegel]] &lt;ref&gt;C.L. Siegel,
Transcendental numbers. Princeton University Press,
Princeton (1949).&lt;/ref&gt;. Among these
functions are such higher transcendental functions as the
[[Hypergeometric series|hypergeometric function]],[[Bessel function|cylindrical]],[[Spherical harmonics|spherical]] functions and so
on. 

Using the FEE, it is possible to prove the following theorem 

'''Theorem.''' Let &lt;math&gt;y=f(x)&lt;/math&gt; be an elementary transcendental
function, that is the [[exponential function]], or a 
[[Trigonometric functions|trigonometric function]], or an [[Algebraic function|elementary algebraic function]], or their
superposition, or their inverse, or a superposition of the
inverses. Then

: &lt;math&gt;
s_f(n) = O(M(n)\log^2n). \,
&lt;/math&gt;

Here &lt;math&gt; s_f(n) &lt;/math&gt; is the [[complexity of computation (bit)]] 
of the function &lt;math&gt; f(x)&lt;/math&gt; with accuracy up to &lt;math&gt;n&lt;/math&gt; digits, &lt;math&gt;M(n)&lt;/math&gt;
is the complexity of multiplication of two &lt;math&gt;n&lt;/math&gt;-digit integers.

The algorithms based on the method FEE include the algorithms for fast calculation of 
any elementary
transcendental function for any value of the argument, the
classical constants &lt;math&gt;e,&lt;/math&gt;  [[Pi|&lt;math&gt;\pi,&lt;/math&gt;]] 
the [[Euler–Mascheroni constant|Euler constant]] &lt;math&gt;\gamma,&lt;/math&gt; the
[[Catalan's constant|Catalan]] and the [[Apéry's constant|Apéry constants]]&lt;ref&gt;Karatsuba E.A.,
Fast evaluation of &lt;math&gt;\zeta(3)&lt;/math&gt;, Probl. Peredachi Informat., Vol. 29,
N 1 (1993)&lt;/ref&gt; , such higher transcendental
functions as the Euler gamma function and it's derivatives, the
hypergeometric &lt;ref&gt;Ekatharine A. Karatsuba,
Fast evaluation of hypergeometric function by FEE. Computational
Methods and Function Theory (CMFT'97), N.Papamichael, St.Ruscheweyh
and E.B.Saff, eds., World Sc.Pub. (1999)&lt;/ref&gt;, spherical, cylindrical&lt;ref&gt;Catherine A. Karatsuba,
Fast evaluation of Bessel functions. Integral Transforms and Special
Functions, Vol.1, N4 (1993)&lt;/ref&gt; and other functions for
algebraic values of the argument and parameters, the 
[[Riemann zeta function]] for 
integer values of the argument &lt;ref&gt;E.A. Karatsuba,
Fast Evaluation of Riemann zeta-function &lt;math&gt;\zeta(s)&lt;/math&gt; for integer
values of argument &lt;math&gt;s&lt;/math&gt;. Probl. Peredachi Informat., Vol. 31, N 4 (1995).&lt;/ref&gt; 
&lt;ref&gt;J.M. Borwein, D.M. Bradley and R.E. Crandall,
Computational strategies for the Riemann zeta function. J. of
Comput. Appl. Math., Vol. 121 ,N 1-2 (2000).&lt;/ref&gt;
and the [[Hurwitz zeta function]] for integer argument and algebraic values of the
parameter &lt;ref&gt;E.A. Karatsuba,
Fast evaluation of Hurwitz zeta function and Dirichlet &lt;math&gt;L&lt;/math&gt;-series,
Problem. Peredachi Informat., Vol. 34, N 4, pp. 342--353, (1998).&lt;/ref&gt;, and also such special integrals as the integral of
probability, the [[Fresnel integral]]s, the integral exponential
function, the integral sine and cosine, and some other integrals &lt;ref&gt;E.A. Karatsuba,
Fast computation of some special integrals of mathematical physics.
Scientific Computing, Validated Numerics, Interval Methods,
W.Kramer,J.W.von Gudenberg,eds.(2001).&lt;/ref&gt;
for algebraic values of the argument with the complexity bound
which is close to the optimal one, namely

:&lt;math&gt;
s_{f}(n)=
O(M(n)\log^2 n). \,
&lt;/math&gt;

At present, only the FEE makes it possible to calculate fast the
values of the functions from the class of higher transcendental
functions &lt;ref&gt;E. Bach,
The complexity of number-theoretic constants. Info. Proc. Letters, N
62 (1997).&lt;/ref&gt;, certain special integrals of mathematical physics and
such classical constants as Euler's, Catalan's &lt;ref&gt;E.A. Karatsuba,
Fast computation of $\zeta(3)$ and of some special integrals ,using
the polylogarithms, the Ramanujan formula and it's generalization.
J. of Numerical Mathematics BIT, Vol. 41, N 4 (2001).&lt;/ref&gt; and Apery's
constants. An additional advantage of the method FEE is the
possibility of parallelizing the algorithms based on the FEE.

== FEE-computation of classical constants ==

For fast evaluation of the
constant &lt;math&gt;\pi,&lt;/math&gt; one can use the Euler formula
&lt;math&gt;\frac{\pi}{4} = \arctan \frac12 + \arctan \frac13,&lt;/math&gt;
and apply the FEE to sum the Taylor series for

: &lt;math&gt;
\arctan \frac12 = \frac{1}{1\cdot 2} - \frac{1}{3\cdot 2^3}+ \cdots +
\frac{(-1)^{r-1}}{(2r-1)2^{2r-1}} + R_1 ,
&lt;/math&gt;

: &lt;math&gt;
\arctan \frac13 = \frac{1}{1\cdot 3} - \frac{1}{3\cdot 3^3}+ \cdots +
\frac{(-1)^{r-1}}{(2r-1)3^{2r-1}} + R_2 ,
&lt;/math&gt;

with the remainder terms &lt;math&gt;R_1,&lt;/math&gt;  &lt;math&gt;R_2,&lt;/math&gt; which satisfy the bounds

: &lt;math&gt;|R_1| \leq \frac45\frac{1}{2r+1}\frac{1}{2^{2r+1}}; &lt;/math&gt; 

: &lt;math&gt;|R_2| \leq \frac{9}{10}\frac{1}{2r+1}\frac{1}{3^{2r+1}};&lt;/math&gt; 

and for

: &lt;math&gt;r = n,\,&lt;/math&gt; 

: &lt;math&gt;4(|R_1|+|R_2|) \ &lt; \ 2^{-n}.&lt;/math&gt; 

To calculate &lt;math&gt;\pi&lt;/math&gt;  by the
FEE it is possible to use also other approximations  &lt;ref&gt;D.H. Bailey, P.B. Borwein and S. Plouffe,
On the rapid computation of various polylogarithmic constants. Math.
Comp.,Vol. 66 (1997).&lt;/ref&gt; In all cases the complexity is

: &lt;math&gt;s_\pi = O(M(n)\log^2 n). \,&lt;/math&gt;

To compute the Euler constant gamma with accuracy up to &lt;math&gt;n&lt;/math&gt;
digits, it is necessary to sum by the FEE two series. Namely, for

: &lt;math&gt;m=6n, \quad k = n, \quad k \geq 1, \, &lt;/math&gt;

: &lt;math&gt;
\gamma = -
\log n \sum_{r=0}^{12n}
\frac{(-1)^rn^{r+1}}{(r+1)!} +
\sum_{r=0}^{12n}
\frac{(-1)^rn^{r+1}}{(r+1)!(r+1)} +
O(2^{-n}) .
&lt;/math&gt;

The complexity is

: &lt;math&gt;s_\gamma = O(M(n)\log^2 n). \, &lt;/math&gt;

To evaluate fast the constant &lt;math&gt;\gamma&lt;/math&gt; 
it is possible to apply the
FEE to other approximations. &lt;ref&gt;R.P. Brent and E.M. McMillan,
Some new algorithms for high-precision computation of Euler's
constant. Math. Comp., Vol.34 (1980).&lt;/ref&gt;

== FEE-computation of certain power series ==

By the FEE the two following series are calculated fast:

: &lt;math&gt; f_1 =
f_1(z) = \sum_{j=0}^{\infty}\frac{a(j)}{b(j)}z^j , &lt;/math&gt;

: &lt;math&gt;f_2 = f_2(z) =
\sum_{j=0}^{\infty}\frac{a(j)}{b(j)}\frac{z^j}{j!} , &lt;/math&gt;

under the assumption that &lt;math&gt;a(j) , \quad b(j)&lt;/math&gt; are
integers,

: &lt;math&gt;|a(j)|+|b(j)| \leq (Cj)^K; \quad |z| \ &lt; \ 1; \quad K&lt;/math&gt; 

and &lt;math&gt;C&lt;/math&gt; are constants, and &lt;math&gt;z&lt;/math&gt; is an algebraic number. The complexity of the evaluation of the series is

: &lt;math&gt;
s_{f_1}(n) = O\left(M(n)\log^2n \right), \,
&lt;/math&gt;

: &lt;math&gt;
s_{f_2}(n)=
O\left(M(n)\log n \right).
&lt;/math&gt;

== The FEE details on the example of fast calculation of the classical constant ''e'' ==

For the evaluation of the constant &lt;math&gt;e&lt;/math&gt; take &lt;math&gt;m = 2^k, \quad k \geq
1&lt;/math&gt;, terms of the Taylor series for &lt;math&gt;e,&lt;/math&gt;

: &lt;math&gt;
e = 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{(m-1)!} + R_m.
&lt;/math&gt;

Here we choose &lt;math&gt;m&lt;/math&gt;, requiring that for the remainder &lt;math&gt;R_m&lt;/math&gt; the
inequality &lt;math&gt;R_m \leq 2^{-n-1}&lt;/math&gt; is fulfilled. This is the case, for
example, when &lt;math&gt;m\geq \frac{4n}{\log n}.&lt;/math&gt; Thus, we take &lt;math&gt;m=2^k&lt;/math&gt;
such that the natural number &lt;math&gt;k&lt;/math&gt; is determined by the
inequalities:

: &lt;math&gt;
2^k \geq \frac{4n}{\log n} &gt; 2^{k-1}.
&lt;/math&gt;

We calculate the sum

: &lt;math&gt;
S = 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \frac{1}{(m-1)!} =
\sum_{j=0}^{m-1}\frac{1}{(m-1-j)!} ,
&lt;/math&gt;

in &lt;math&gt;k&lt;/math&gt; steps of the following process.

Step 1. Combining in &lt;math&gt;S&lt;/math&gt; the summands sequentially in pairs we
carry out of the brackets the &quot;obvious&quot; common factor and obtain

: &lt;math&gt;
\begin{align}
S &amp; = \left(\frac{1}{(m-1)!} + \frac{1}{(m-2)!}\right) +
\left(\frac{1}{(m-3)!} + \frac{1}{(m-4)!}\right) + \cdots \\
&amp; = \frac{1}{(m-1)!}(1+m-1) + \frac{1}{(m-3)!}(1+m-3) + \cdots .
\end{align}
&lt;/math&gt;

We shall compute only integer values of the expressions in the
parentheses, that is the values

: &lt;math&gt;
m  , m-2 ,  m-4 ,  \dots . \,
&lt;/math&gt;

Thus, at the first step the sum &lt;math&gt;S&lt;/math&gt; is into

: &lt;math&gt;
S = S(1) = \sum_{j=0}^{m_1-1}\frac{1}{(m-1-2j)!}\alpha_{m_1-j}(1) , &lt;/math&gt;

: &lt;math&gt; m_1 = \frac m2 , m = 2^k , k \geq 1 .&lt;/math&gt;

At the first step &lt;math&gt;\frac m2&lt;/math&gt; integers of the form

: &lt;math&gt;
\alpha_{m_1-j}(1) = m-2j , \quad j = 0 , 1 , \dots , m_1 - 1 ,
&lt;/math&gt;

are calculated. After that we act in a similar way: combining on
each step the summands of the sum &lt;math&gt;S&lt;/math&gt; sequentially in pairs, we
take out of the brackets the 'obvious' common factor and compute
only the integer values of the expressions in the brackets. Assume
that the first &lt;math&gt;i&lt;/math&gt; steps of this process are completed.

Step &lt;math&gt;i+1&lt;/math&gt; (&lt;math&gt;i+1 \leq k&lt;/math&gt;).

: &lt;math&gt;
S = S(i+1) = \sum_{j=0}^{m_{i+1}-1}\frac{1}{(m-1-2^{i+1}j)!}
\alpha_{m_{i+1}-j}(i+1) ,&lt;/math&gt;

: &lt;math&gt; m_{i+1} = \frac{m_i}{2} = \frac{m}{2^{i+1}} ,
&lt;/math&gt;

we compute only &lt;math&gt;\frac{m}{2^{i+1}}&lt;/math&gt; integers of the form

: &lt;math&gt;
\alpha_{m_{i+1}-j}(i+1) = \alpha_{m_i-2j}(i) +
\alpha_{m_i-(2j+1)}(i)\frac{(m-1-2^{i+1}j)!}{(m-1-2^i-2^{i+1}j)!}
,&lt;/math&gt;

: &lt;math&gt;j = 0 , 1 , \dots , \quad m_{i+1} - 1 , \quad m = 2^k , \quad k \geq i+1 .&lt;/math&gt; 

Here

: &lt;math&gt;\frac{(m-1-2^{i+1}j)!}{(m-1-2^i-2^{i+1}j)!}&lt;/math&gt;

is the product of &lt;math&gt;2^i&lt;/math&gt; integers.

Etc.

Step &lt;math&gt;k&lt;/math&gt;, the last one. We compute one integer value
&lt;math&gt;\alpha_1(k),&lt;/math&gt; we compute, using the fast algorithm described
above the value &lt;math&gt;(m-1)!,&lt;/math&gt; and make one division of the integer
&lt;math&gt;\alpha_1(k)&lt;/math&gt; by the integer &lt;math&gt;(m-1)!,&lt;/math&gt; 
with accuracy up to &lt;math&gt;n&lt;/math&gt;
digits. The obtained result is the sum &lt;math&gt;S,&lt;/math&gt; or the constant &lt;math&gt;e&lt;/math&gt; up
to &lt;math&gt;n&lt;/math&gt; digits. The complexity of all computations is

: &lt;math&gt;
O\left(M(m)\log^2 m\right) = O\left(M(n)\log n\right). \, 
&lt;/math&gt;

==References==
{{reflist}}

==See also==
*[[Fast algorithms]]
*[[AGM method]]
*[[Computational complexity]]

==External links==
*&lt;http://www.ccas.ru/personal/karatsuba/divcen.htm&gt;
*&lt;http://www.ccas.ru/personal/karatsuba/algen.htm&gt;

{{DEFAULTSORT:Fee Method}}
[[Category:Algorithms]]

[[ru:Метод БВЕ]]</text>
    </revision>
  </page>
  <page>
    <title>Exponential mechanism (differential privacy)</title>
    <id>22999896</id>
    <revision>
      <id>305308496</id>
      <timestamp>2009-07-31T18:14:30Z</timestamp>
      <contributor>
        <username>Kintetsubuffalo</username>
        <id>487310</id>
      </contributor>
      <minor/>
      <comment>[[WP:RBK|Reverted]] edits by [[Special:Contributions/207.46.55.31|207.46.55.31]] ([[User talk:207.46.55.31|talk]]) to last version by 96.235.234.100</comment>
      <text xml:space="preserve">{{orphan|date=June 2009}}The '''exponential mechanism''' is a technique for designing differentially private algorithms developed by [http://research.microsoft.com/en-us/people/mcsherry/ Frank McSherry] and [http://research.microsoft.com/en-us/people/kunal/default.aspx Kunal Talwar]. [[Differential privacy|Differential Privacy]] is a technique for releasing statistical information about a database without revealing information about its individual entries. &lt;/br&gt;
Most of the initial research in the field of [[Differential privacy|Differential Privacy]] revolved around real valued functions which have relatively low [[Differential privacy | sensitivity]] to change in the data of a single individual and whose usefulness is not hampered by small additive perturbations. A natural question is what happens in the situation when one wants to preserve more general sets of properties. The Exponential Mechanism helps to extend the notion of differential privacy to address these issues. Moreover, it describes a class of mechanisms that includes all possible differentially private mechanisms. 

== The exponential mechanism &lt;ref&gt;[http://research.microsoft.com/pubs/65075/mdviadp.pdf F.McSherry and K.Talwar. Mechasim Design via Differential Privacy. Proceedings of the 48th Annual Symposium of Foundations of Computer Science, 2007.]&lt;/ref&gt; ==

=== Algorithm ===
In very generic terms a privacy mechanism maps a set of &lt;math&gt;n\,\!&lt;/math&gt; inputs from domain &lt;math&gt;\mathcal{D}\,\!&lt;/math&gt;, to a range &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt;. The map may be randomized, in which case each element of the domain &lt;math&gt;D\,\!&lt;/math&gt; corresponds to the probability distribution over the range &lt;math&gt;R\,\!&lt;/math&gt;. The privacy mechanism we are going to design makes no assumption about the nature of &lt;math&gt;\mathcal{D}\,\!&lt;/math&gt; and &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt; apart from a base [[Measure (mathematics)|measure]] &lt;math&gt;\mu\,\!&lt;/math&gt; on &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt;. Let us define a function &lt;math&gt;q:\mathcal{D}^n\times\mathcal{R}\rightarrow\mathbb{R}\,\!&lt;/math&gt;. Intuitively this function assigns	 score to the pair &lt;math&gt;(d,r)\,\!&lt;/math&gt;, where &lt;math&gt;d\in\mathcal{D}^n\,\!&lt;/math&gt; and &lt;math&gt;r\in\mathcal{R}\,\!&lt;/math&gt;. The score reflects how appealing is the pair &lt;math&gt;(d,r)\,\!&lt;/math&gt;, i.e the higher the score, the more appealing the pair is.  
Once we are given the input &lt;math&gt;d\in\mathcal{D}^n\,\!&lt;/math&gt;, the mechanism's objective is to return an &lt;math&gt;r\in\mathcal{R}\,\!&lt;/math&gt; such that the function &lt;math&gt;q(d,r)\,\!&lt;/math&gt; is approximately maximized. To achieve this, we set up the mechanism &lt;math&gt;\mathcal{E}_{q}^{\epsilon}(d)\,\!&lt;/math&gt; as follows: &lt;/br&gt;
'''Definition:''' For any function &lt;math&gt;q:(\mathcal{D}^{n}\times\mathcal{R})\rightarrow\mathbb{R}\,\!&lt;/math&gt;, and a base measure &lt;math&gt;\mu\,\!&lt;/math&gt; over &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt;, we define:
:&lt;math&gt;\mathcal{E}_{q}^{\epsilon}(d):=\,\!&lt;/math&gt; Choose &lt;math&gt;r\,\!&lt;/math&gt; with probability proportional to &lt;math&gt;e^{\epsilon q(d,r)}\times\mu(r)\,\!&lt;/math&gt;, where &lt;math&gt;d\in\mathcal{D}^n,r\in R\,\!&lt;/math&gt;.
This definition implies the fact that the probability of returning an &lt;math&gt;r\,\!&lt;/math&gt; increases exponentially with the increase in the value of &lt;math&gt;q(d,r)\,\!&lt;/math&gt;. For now if we ignore the base measure &lt;math&gt;\mu\,\!&lt;/math&gt; then the value &lt;math&gt;r\,\!&lt;/math&gt; which maximizes &lt;math&gt;q(d,r)\,\!&lt;/math&gt; has the highest probability. Moreover we claim that this mechanism is differentially private. We will prove this claim shortly. One technicality that should be kept in mind is that in order to properly define &lt;math&gt;\mathcal{E}_{q}^{\epsilon}(d)\,\!&lt;/math&gt; the &lt;math&gt;\int_{r}e^{\epsilon q(d,r)}\times\mu(r)\,\!&lt;/math&gt; should be finite. 

'''Theorem (Differential Privacy):''' &lt;math&gt;\mathcal{E}_{q}^{\epsilon}(d)\,\!&lt;/math&gt; gives &lt;math&gt;(2\epsilon\Delta q)\,\!&lt;/math&gt;-differential privacy. 

Proof: The probability density of &lt;math&gt;\mathcal{E}_{q}^{\epsilon}(d)\,\!&lt;/math&gt; at &lt;math&gt;r\,\!&lt;/math&gt; equals 
:&lt;math&gt;\frac{e^{\epsilon q(d,r)}\mu(r)}{\int e^{\epsilon q(d,r)}\mu(r)dr}\,\!&lt;/math&gt;.
Now, if a single change in &lt;math&gt;d\,\!&lt;/math&gt; changes &lt;math&gt;q\,\!&lt;/math&gt; by at most &lt;math&gt;\Delta q\,\!&lt;/math&gt; then the numerator can change at most by a factor of &lt;math&gt;e^{\epsilon\Delta q}\,\!&lt;/math&gt; and the denominator minimum by a factor of &lt;math&gt;e^{-\epsilon\Delta q}\,\!&lt;/math&gt;. Thus, the ratio of the new probability density (i.e with new &lt;math&gt;d\,\!&lt;/math&gt;) and the earlier one is at most &lt;math&gt;exp(2\epsilon\Delta q)\,\!&lt;/math&gt;. 

=== Accuracy ===

We would ideally want the random draws of &lt;math&gt;r\,\!&lt;/math&gt; from the mechanism &lt;math&gt;\mathcal{E}_{q}^{\epsilon}(d)\,\!&lt;/math&gt; to nearly maximize &lt;math&gt;q(d,r)\,\!&lt;/math&gt;. If we consider &lt;math&gt;max_rq(d,r)\,\!&lt;/math&gt; to be &lt;math&gt;OPT\,\!&lt;/math&gt; then we can show that the probability of the mechanism deviating from &lt;math&gt;OPT\,\!&lt;/math&gt;is low, as long as there is a sufficient mass (in terms of &lt;math&gt;mu\,\!&lt;/math&gt;) of values &lt;math&gt;r\,\!&lt;/math&gt; with value &lt;math&gt;q\,\!&lt;/math&gt; close to the optimum.

'''Lemma:''' Let &lt;math&gt;S_{t}={r:q(d,r)&gt;OPT-t}\,\!&lt;/math&gt; and &lt;math&gt;\bar{S}_{2t}={r:q(d,r)\leq OPT-2t}\,\!&lt;/math&gt;, we have &lt;math&gt;p(\bar{S}_{2t})\,\!&lt;/math&gt; is at most &lt;math&gt;exp(-\epsilon t)/\mu(S_{t})\,\!&lt;/math&gt;. The probability is taken over &lt;math&gt;R\,\!&lt;/math&gt;. 

Proof: The probability &lt;math&gt;p(\bar{S}_{2t})\,\!&lt;/math&gt; is at most &lt;math&gt;p(\bar{S}_{2t})/p(S_{t})\,\!&lt;/math&gt;, as the denominator can be at most one. Since both the probabilities have the same normalizing term so,&lt;br&gt;
:&lt;math&gt;\frac{p(\bar{S}_{2t})}{p(S_{t})}=\frac{\int_{\bar{S}_{2t}}exp(\epsilon q(d,r))\mu(r)dr}{\int_{S_{t}}exp(\epsilon q(d,r))\mu(r)dr}\leq exp(-\epsilon t)\frac{\mu(\bar{S}_{2t})}{\mu(S_{t})}\,\!&lt;/math&gt;.
The value of &lt;math&gt;\mu(\bar{S}_{2t})\,\!&lt;/math&gt; is at most one, and so this bound implies the lemma statement.

'''Theorem (Accuracy):''' For those values of &lt;math&gt;t\geq ln(\frac{OPT}{t\mu(S_{t})})/\epsilon\,\!&lt;/math&gt;, we have &lt;math&gt; E[q(d,\mathcal{E}_{q}^{\epsilon}(d))]\geq OPT-3t\,\!&lt;/math&gt;.

Proof: It follows from the previous lemma that the probability of the score being at least &lt;math&gt;OPT-2t\,\!&lt;/math&gt; is &lt;math&gt;1-exp(-\epsilon t)/\mu(S_{t})\,\!&lt;/math&gt;. By Hypothesis, &lt;math&gt;t\geq ln(\frac{OPT}{t\mu(S_{t})})/\epsilon\,\!&lt;/math&gt;. Substituting the value of &lt;math&gt;t\,\!&lt;/math&gt; we get this probability to be at least &lt;math&gt;1-t/OPT\,\!&lt;/math&gt;. Multiplying with &lt;math&gt;OPT-2t\,\!&lt;/math&gt; yields the desired bound. 

We can assume &lt;math&gt;\mu(A)\,\!&lt;/math&gt; for &lt;math&gt;A\subseteq \mathcal{R}\,\!&lt;/math&gt; to be less than or equal to one in all the computations, because we can always normalize with &lt;math&gt;\mu(\mathcal{R})\,\!&lt;/math&gt; .

== Example application of the exponential mechanism &lt;ref&gt;[http://www.cs.cmu.edu/~alroth/Papers/dataprivacy.pdf Avrim Blum,Katrina Ligett,Aaron Roth. A Learning Theory Approach to Non-Iteractive Database Privacy.In Proceedings of the 40th annual ACM symposium on Theory of computing, 2008]&lt;/ref&gt;==

Before we get into the details of the example let us define some terms which we will be using extensively through out our discussion. 

'''Definition (Global Sensitivity):''' The global sensitivity of a query &lt;math&gt;Q\,\!&lt;/math&gt; is its maximum difference when evaluated on two neighbouring datasets &lt;math&gt;D_{1},D_{2}\in\mathcal{D}^n\,\!&lt;/math&gt;:
:&lt;math&gt;GS_{Q}=max_{D_{1},D_{2}:d(D_{1},D_{2})=1}|(Q(D_{1})-Q(D_{2}))|\,\!&lt;/math&gt;.

'''Definition:''' A predicate query &lt;math&gt;Q_{\varphi}\,\!&lt;/math&gt; for any predicate &lt;math&gt;\varphi\,\!&lt;/math&gt; is defined to be
:&lt;math&gt;Q_{\varphi}=\frac{|\{x\in D:\varphi(x)\}|}{|D|}\,\!&lt;/math&gt;.

Note that &lt;math&gt;GS_{Q_{\varphi}}\leq 1/n\,\!&lt;/math&gt; for any predicate &lt;math&gt;\varphi\,\!&lt;/math&gt;.

=== Release mechanism ===

The following is due to [http://www.cs.cmu.edu/~avrim/ Avrim Blum], [http://www.cs.cmu.edu/~katrina/ Katrina Ligett] and [http://www.cs.cmu.edu/~alroth/ Aaron Roth].

'''Definition (Usefulness):''' A [http://cryptowiki.cse.psu.edu/mediawiki/index.php/CSE546-Spring-2009/Differential-Privacy mechanism] &lt;math&gt;\mathcal{A}\,\!&lt;/math&gt; is &lt;math&gt;(\alpha,\delta)\,\!&lt;/math&gt;-useful for queries in class &lt;math&gt;H\,\!&lt;/math&gt; with probability &lt;math&gt;1-\delta\,\!&lt;/math&gt;, if &lt;math&gt;\forall h\in H\,\!&lt;/math&gt; and every dataset &lt;math&gt;D\,\!&lt;/math&gt;, for &lt;math&gt;\widehat{D}=\mathcal{A}(D)\,\!&lt;/math&gt;, &lt;math&gt;|Q_{h}(\widehat{D})-Q_{h}(D)|\leq \alpha\,\!&lt;/math&gt;.  

Informally, it means that with high probability the query &lt;math&gt;Q_{h}\,\!&lt;/math&gt; will behave in a similar way on the original dataset &lt;math&gt;D\,\!&lt;/math&gt; and on the synthetic dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt;. &lt;/br&gt;
Let us consider a common problem in Data Mining. Assume there is a database &lt;math&gt;D\,\!&lt;/math&gt; with &lt;math&gt;n\,\!&lt;/math&gt; entries. Each entry consist of &lt;math&gt;k\,\!&lt;/math&gt;-tuples of the form &lt;math&gt;(x_{1},x_{2},...,x_{k})\,\!&lt;/math&gt; where &lt;math&gt;x_{i}\in\{0,1\}\,\!&lt;/math&gt;. Now, a user wants to learn a [[Half-space|linear halfspace]] of the form &lt;math&gt;\pi_{1}x_{1}+\pi_{2}x_{2}+...+\pi_{k-1}x_{k-1}\geq x_{k}\,\!&lt;/math&gt;. In essence the user wants to figure out the values of &lt;math&gt;\pi_{1},\pi_{2},...,\pi_{k-1}\,\!&lt;/math&gt; such that maximum number of tuples in the database satisfy the inequality. The algorithm we describe below can generate a synthetic database &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; which will allow the user to learn (approximately) the same linear half-space while querying on this synthetic database. The motivation for such an algorithm being that the new database will be generated in a differentially private manner and thus asssure privacy to the individual records in the database &lt;math&gt;D\,\!&lt;/math&gt;. 

In this section we show that it is possible to release a dataset which is useful for concepts from a polynomial VC-Dimension class and at the same time adhere to &lt;math&gt;\epsilon\,\!&lt;/math&gt;-differential privacy as long as the size of the original dataset is at least polynomial on the VC-Dimension of the concept class. To state formally: 

'''Theorem:''' For any class of functions &lt;math&gt;H\,\!&lt;/math&gt; and any dataset &lt;math&gt;D\subset \{0,1\}^{k}\,\!&lt;/math&gt; such that
:&lt;math&gt;|D|\geq O\left(\frac{k\cdot VCDIM(H)log(1/\alpha)}{\alpha^{3}\epsilon}+\frac{log(1/\delta)}{\alpha\epsilon}\right)\,\!&lt;/math&gt;
we can output an &lt;math&gt;(\alpha,\delta)\,\!&lt;/math&gt;-useful dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; that preserves &lt;math&gt;\epsilon\,\!&lt;/math&gt;-differential privacy. As we had mentioned earlier the algorithm need not be efficient.

One interesting fact is that the algorithm which we are going to develop generates a synthetic dataset whose size is independent of the original dataset; in fact, it only depends on the [[VC dimension|VC-dimension]] of the concept class and the parameter &lt;math&gt;\alpha\,\!&lt;/math&gt;. The algorithm outputs a dataset of size &lt;math&gt;\tilde{O}(VCDIM(H)/\alpha^{2})\,\!&lt;/math&gt;

We borrow the [[Uniform convergence (combinatorics)|Uniform Convergence Theorem]] from [[combinatorics]] and state a corollary of it which aligns to our need. 

'''Lemma:''' Given any dataset &lt;math&gt;D\,\!&lt;/math&gt; there exists a dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; of size &lt;math&gt;=O(VCDIM(H)log(1/\alpha))/\alpha^{2}\,\!&lt;/math&gt; such that &lt;math&gt;max_{h\in H}|Q_{h}(D)-Q_{h}(\widehat{D})|\leq \alpha/2\,\!&lt;/math&gt;.

Proof: 

We know from the uniform convergence theorem that,

:&lt;math&gt;Pr[|Q_{h}(D)-Q_{h}(\widehat{D})|\geq \alpha/2\,\!&lt;/math&gt; for some &lt;math&gt;h\in H]\leq 2(\frac{em}{VCDIM(H)})^{VCDIM(H)}\cdot e^{-\frac{\alpha^{2}m}{8}}\,\!&lt;/math&gt;, 
where probability is over the distribution of the dataset. 
Thus, if the RHS is less than one then we know for sure that the data set &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; exists. To bound the RHS to less than one we need &lt;math&gt;m\geq\lambda(VCDIM(H)log(m/VCDIM(H))/\alpha^{2})\,\!&lt;/math&gt;, where &lt;math&gt;\lambda\,\!&lt;/math&gt; is some positive constant. Since we stated earlier that we will output a dataset of size &lt;math&gt;\tilde{O}(VCDIM(H)/\alpha^{2})\,\!&lt;/math&gt;, so using this bound on &lt;math&gt;m\,\!&lt;/math&gt; we get &lt;math&gt;m\geq\lambda(VCDIM(H)log(1/\alpha)/\alpha^{2})\,\!&lt;/math&gt;. Hence the lemma. 

Now we invoke the Exponential Mechanism. 

'''Definition:''' For any function &lt;math&gt;q:((\{0,1\}^{k})^{n}\times(\{0,1\}^{k})^{m})\rightarrow\mathbb{R}\,\!&lt;/math&gt; and input dataset &lt;math&gt;D\,\!&lt;/math&gt;, the Exponential mechanism outputs each dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; with probability proportional to &lt;math&gt;e^{q(D,\widehat{D})\epsilon n/2}\,\!&lt;/math&gt;.

From the Exponential Mechanism we know this preserves &lt;math&gt;(\epsilon nGS_{q})\,\!&lt;/math&gt;-differential privacy. 
Lets get back to the proof of the Theorem. 

We define &lt;math&gt;(q(D),q(\widehat{D}))=-max_{h\in H}|Q_{h}(D)-Q_{h}(\widehat{D})|\,\!&lt;/math&gt;. &lt;/br&gt;
To show that the mechanism satisfies the &lt;math&gt;(\alpha,\delta)\,\!&lt;/math&gt;-usefulness, we should show that it outputs some dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; with &lt;math&gt;q(D,\widehat{D})\geq -\alpha\,\!&lt;/math&gt; with probability &lt;math&gt;1-\delta\,\!&lt;/math&gt;. 
There are at most &lt;math&gt;2^{km}\,\!&lt;/math&gt; output datasets and the probability that &lt;math&gt;q(D,\widehat{D})\leq -\alpha\,\!&lt;/math&gt; is at most proportional to &lt;math&gt;e^{-\epsilon\alpha n/2}\,\!&lt;/math&gt;. Thus by union bound, the probability of outputting any such dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; is at most proportional to &lt;math&gt;2^{km}e^{-\epsilon\alpha n/2}\,\!&lt;/math&gt;.  
Again, we know that there exists some dataset &lt;math&gt;\widehat{D}\in(\{0,1\}^{k})^{m}\,\!&lt;/math&gt; for which &lt;math&gt;q(D,\widehat{D})\geq -\alpha/2\,\!&lt;/math&gt;. Therefore, such a dataset is output with probability at least proportional to &lt;math&gt;e^{-\alpha\epsilon n/4}\,\!&lt;/math&gt;. &lt;/br&gt; 
Let, &lt;math&gt;A:=\,\!&lt;/math&gt; the event that the Exponential mechanism outputs some dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; such that &lt;math&gt;q(D,\widehat{D})\geq-\alpha/2\,\!&lt;/math&gt;. &lt;/br&gt; 
&lt;math&gt;B:=\,\!&lt;/math&gt; the event that the Exponential mechanism outputs some dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; such that &lt;math&gt;q(D,\widehat{D})\leq-\alpha\,\!&lt;/math&gt;.
:&lt;math&gt;\therefore \frac{Pr[A]}{Pr[B]}\geq \frac{e^{-\alpha\epsilon n/4}}{2^{km}e^{-\alpha\epsilon n/2}}=\frac{e^{\alpha\epsilon n/4}}{2^{km}}.\,\!&lt;/math&gt;
Now setting this quantity to be at least &lt;math&gt;1/\delta\geq(1-\delta)/\delta\,\!&lt;/math&gt;, we find that it suffices to have 
:&lt;math&gt;n\geq\frac{4}{\epsilon\alpha}\left(km+ln\frac{1}{\delta}\right)\geq O\left(\frac{d\cdot VCDIM(H)log(1/\alpha)}{\alpha^{3}\epsilon}+\frac{log(1/\delta)}{\alpha\epsilon}\right)\,\!&lt;/math&gt;.
And hence we prove the theorem.

== The Exponential Mechanism in other domains ==

We just showed an example of the usage of Exponential Mechanism where one can output a synthetic dataset in a differentially private manner and can use the dataset to answer queries with good accuracy. Apart from these kinds of setting, the Exponential Mechanism has also been studied in the context of [[Auction theory]] and [[Statistical classification|Classification Algorithms]]&lt;ref&gt;[http://arxiv.org/PS_cache/arxiv/pdf/0803/0803.0924v2.pdf Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim,Sofya Raskhodnikova, Adam Smith. What Can We Learn Privately? Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science.]&lt;/ref&gt;. In the case of auctions the Exponential Mechanism helps to achieve a ''truthful'' auction setting.

==References==
{{Reflist}}

[[Category:Data privacy]]
[[Category:Data analysis]]
[[Category:Algorithms]]
[[Category:Machine learning]]</text>
    </revision>
  </page>
  <page>
    <title>Range Minimum Query</title>
    <id>21332467</id>
    <revision>
      <id>315061379</id>
      <timestamp>2009-09-20T08:12:18Z</timestamp>
      <contributor>
        <ip>87.68.164.35</ip>
      </contributor>
      <text xml:space="preserve">Given an array &lt;math&gt;A[1,n]&lt;/math&gt; of &lt;math&gt;n&lt;/math&gt; ordered objects (such as integers or reals), a '''Range Minimum Query''' (or '''RMQ''') from &lt;math&gt;i&lt;/math&gt; to &lt;math&gt;j&lt;/math&gt; asks for the position of a minimum element in the sub-array &lt;math&gt;A[i,j]&lt;/math&gt;.

For example, when &lt;math&gt;A=[3,5,2,5,4,3,1,6,3]&lt;/math&gt;, then a range minimum query from 3 to 8 returns 7, as &lt;math&gt;A[7]=1&lt;/math&gt; is the minimum in the sub-array &lt;math&gt;A[3,8]=[2,5,4,3,1,6]&lt;/math&gt;.

In the most typical setting, the array &lt;math&gt;A&lt;/math&gt; is static, and there are several-queries to be answered on-line (meaning that the queries are not known in advance to the algorithm). In this case it makes sense to preprocess the array into a data structure (often called a preprocessing scheme) that helps for a faster query evaluation.

It is known that a &lt;math&gt;O(n)&lt;/math&gt;-time preprocessing is sufficient to answer subsequent queries in &lt;math&gt;O(1)&lt;/math&gt; time. The space of the resulting scheme is actually very small, namely &lt;math&gt;O(n)&lt;/math&gt; bits (see {{harvtxt|Fischer|Heun|2007}}).

RMQs can be used to solve the [[lowest common ancestor]] problem, and is used as a tool for many tasks in exact and approximate string matching.

== References ==
*{{citation
 | last1 = Berkman | first1 = Omer
 | last2 = Vishkin | first2 = Uzi | author2-link = Uzi Vishkin
 | title = Recursive Star-Tree Parallel Data Structure
 | journal = SIAM Journal on Computing
 | volume = 22
 | issue = 2
 | pages = 221–242
 | year = 1993
 | doi = 10.1137/0222017}}

*{{citation
 | last1 = Fischer | first1 = Johannes
 | last2 = Heun | first2 = Volker
 | contribution = A New Succinct Representation of RMQ-Information and Improvements in the Enhanced Suffix Array.
 | title = Proceedings of the International Symposium on Combinatorics, Algorithms, Probabilistic and Experimental Methodologies 
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | volume = 4614
 | year = 2007
 | pages = 459–470
 | doi = 10.1007/978-3-540-74450-4_41}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Heuristic algorithms</title>
    <id>23173987</id>
    <revision>
      <id>295660700</id>
      <timestamp>2009-06-10T21:36:02Z</timestamp>
      <contributor>
        <username>Ruud Koot</username>
        <id>170083</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '{{catmain|Heuristic algorithm}}  [[Category:Algorithms]] [[Category:Heuristics]]'</comment>
      <text xml:space="preserve">{{catmain|Heuristic algorithm}}

[[Category:Algorithms]]
[[Category:Heuristics]]</text>
    </revision>
  </page>
  <page>
    <title>HAKMEM</title>
    <id>505526</id>
    <revision>
      <id>310122464</id>
      <timestamp>2009-08-26T08:16:02Z</timestamp>
      <contributor>
        <username>Piet Delport</username>
        <id>286305</id>
      </contributor>
      <minor/>
      <comment>narrow category</comment>
      <text xml:space="preserve">'''HAKMEM''', alternatively known as '''AI Memo 239''', is a 1972 &quot;memo&quot; ([[technical report]]) of the [[MIT AI Lab]] that describes a wide variety of [[Hack (technology slang)|hack]]s, primarily useful and clever [[algorithm]]s for mathematical computation. There are also some [[schematic diagram]]s for [[hardware]]. Contributors included about two dozen members and associates of the AI Lab.

HAKMEM is notable as an early compendium of algorithmic technique, particularly for its practical bent, and as an illustration of the wide-ranging interests of AI Lab people of the time, which included almost anything other than AI research.

HAKMEM contains original work in some fields, notably [[continued fractions]].

==References==
The bibliographical reference for this memo is:
*Beeler, Michael; [[Bill Gosper|Gosper, R. William]]; and [[Richard Schroeppel|Schroeppel, Rich]]; &quot;HAKMEM&quot;, Memo 239, Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, Mass., 1972.

==External links==
*[http://home.pipeline.com/~hbaker1/hakmem/hakmem.html HAKMEM]
*[ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-239.pdf HAKMEM facsimile (PDF)] ([http://w3.pppl.gov/~hammett/work/2009/AIM-239-ocr.pdf searchable version])

[[Category:Algorithms]]
[[Category:Computer science literature]]</text>
    </revision>
  </page>
  <page>
    <title>Holographic algorithm</title>
    <id>14609233</id>
    <revision>
      <id>325460519</id>
      <timestamp>2009-11-12T16:50:41Z</timestamp>
      <contributor>
        <username>4orty2wo</username>
        <id>4158145</id>
      </contributor>
      <minor/>
      <comment>/* How the algorithms work */  Created link to topic &quot;matchgate&quot; in the hope that someone might make this page</comment>
      <text xml:space="preserve">{{for|holographic storage (which is unrelated)|Holographic data storage}}

'''Holographic algorithms''' are a family of algorithms invented by [[Leslie Valiant]]&lt;ref name=&quot;valiant&quot;&gt;Valiant, L. G. 2004. [http://dx.doi.org/10.1109/FOCS.2004.34 Holographic Algorithms (Extended Abstract)]. In Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science (October 17 &amp;ndash; 19, 2004). FOCS. IEEE Computer Society, Washington, DC, 306&amp;ndash;315.&lt;/ref&gt; that can obtain exponential speedup on certain classes of problems.  These algorithms have received notable coverage due to speculation that they are relevant to the [[P = NP]] problem&lt;ref name=&quot;americanscientist&quot;&gt;[http://www.americanscientist.org/issues/pub/accidental-algorithms Accidental Algorithms]:  A strange new family of algorithms probes the boundary between easy and hard problems, by Brian Hayes, American Scientist, Jan&amp;ndash;Feb 2008&lt;/ref&gt; and their impact on [[computational complexity theory]].  Holographic algorithms are also called 'accidental algorithms' due to their unlikely, capricious quality.&lt;ref name=&quot;americanscientist&quot;/&gt; The algorithms are unrelated to laser [[holography]], except metaphorically: their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.&lt;ref name=&quot;americanscientist&quot;/&gt;  The algorithms have some similarities with [[quantum computation]], but they use classical computation.&lt;ref name=&quot;survey&quot;/&gt;

==How the algorithms work==
{{Expert-subject|computer science|date=November 2008}}
The algorithms are based on several concepts: counting [[perfect matching]]s, holographic [[Reduction (complexity)|reduction]] of sets of problems, and reduction to perfect matching problems.&lt;ref name=&quot;americanscientist&quot;/&gt;

The number of perfect matchings in a [[planar graph]] can be computed in polynomial time by using the [[FKT algorithm]], dating to the 1960s.  
The FKT algorithm converts the problem into a [[Pfaffian]] computation, which can be solved polynomially using standard [[determinant]] algorithms.  Note that although the basic formula for the determinant contains n[[factorial|!]] terms, the structure of the determinant allows it to be computed in polynomial time, as many terms cancel out and don't need to be computed.  This cancellation is a key to holographic algorithms.

The second concept in holographic algorithms is reducing a problem to a different problem via holographic reduction.  A standard technique in complexity is [[many-one reduction]], so an instance of a problem is reduced to an instance of a (hopefully simpler) problem.
However, holographic reductions between two computational problems preserve the sum of solutions without preserving correspondences between solutions.&lt;ref name=&quot;valiant&quot;/&gt;  For instance, the total number of solutions in both sets can be preserved, even though individual problems don't have matching solutions.  The sum can also be weighted, rather than simply counting the number of solutions, using [[Basis (linear algebra)|linear basis vectors]].
&lt;ref name=&quot;art&quot;&gt;Cai, J. and Lu, P. 2007. [http://doi.acm.org/10.1145/1250790.1250850 Holographic algorithms: from art to science]. In Proceedings of the Thirty-Ninth Annual ACM Symposium on theory of Computing (San Diego, California, USA, June 11&amp;ndash;13, 2007). STOC '07. ACM, New York, NY, 401&amp;ndash;410.&lt;/ref&gt;

The holographic reduction uses ''[[matchgate]]s'', which are graph-theoretic entities, analogous to [[logic gate]]s, that use perfect matching to perform operations.  The matchgates are combined into a planar graph called a ''matchgrid''.  By Valiant's Holant Theorem, the (polynomial-time) perfect matching algorithm gives the solution to the matchgrid problem.  Thus, the original problem is solved in polynomial time.  &lt;ref name=&quot;survey&quot;&gt;Cai, J. 2008. [http://doi.acm.org/10.1145/1388240.1388254 Holographic algorithms: guest column]. SIGACT News 39, 2 (Jun. 2008), 51&amp;ndash;81.&lt;/ref&gt;

==Research==
Holographic algorithms have been used to find [[P (complexity)|polynomial]] solutions to problems without formerly-known polynomial solutions in [[Boolean satisfiability problem|satisfiability]], [[vertex cover]], and other [[graph theory|graph problems]].&lt;ref name=&quot;art&quot;/&gt;  Although some of these problems are related to [[NP-complete]] problems, they are not themselves NP-complete, and thus do not prove P=NP.&lt;ref name=&quot;art&quot;/&gt;  Also, some of the solved problems are argued to be contrived (such as '[[modulo operation|#]]&lt;sub&gt;7&lt;/sub&gt;[[Planar graph|Pl]]-Rtw-[[monotone|Mon]]-3[[Conjunctive normal form|CNF]]').&lt;ref name=&quot;art&quot;/&gt;

A key research area is extending holographic algorithms to new problems.

==References==
{{Reflist}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Sardinas–Patterson algorithm</title>
    <id>23632960</id>
    <revision>
      <id>323185188</id>
      <timestamp>2009-10-31T23:19:56Z</timestamp>
      <contributor>
        <username>Dicklyon</username>
        <id>869314</id>
      </contributor>
      <comment>section order</comment>
      <text xml:space="preserve">{{orphan|date=October 2009}}

In [[coding theory]], the '''Sardinas–Patterson algorithm''' is a classical algorithm for determining whether a given [[variable-length code]] is uniquely decodable. The algorithm carries out a systematic search for a string which admits two different decompositions into codewords. As [[Donald Ervin Knuth|Knuth]] reports, the algorithm was rediscovered about ten years later in 1963 by [[Robert Floyd|Floyd]], despite the fact that it was at the time already well known in coding theory.&lt;ref&gt;Knuth (2003), p. 2&lt;/ref&gt;

==Idea of the algorithm==

Consider the code  &lt;math&gt;\{\, a \mapsto 1, b \mapsto 011, c\mapsto 01110, d\mapsto 1110, e\mapsto 10011\,\}&lt;/math&gt;. This code, which is based on an example by Berstel,&lt;ref&gt;Berstel et al. (2009), Example 2.3.1 p. 63&lt;/ref&gt; is an example of a code which is not uniquely decodable, since the string 

:011101110011 

can be interpreted as the sequence of codewords 

:01110 – 1110 – 011, 

but also as the sequence of codewords 

:011 – 1 – 011 – 10011.

Two possible decodings of this encoded string are thus given by ''cdb'' and ''babe''.

In general, a codeword can be found by the following idea: In the first round, we choose two codewords &lt;math&gt;x_1&lt;/math&gt; and &lt;math&gt;y_1&lt;/math&gt; such that &lt;math&gt;x_1&lt;/math&gt; is a [[prefix]] of &lt;math&gt;y_1&lt;/math&gt;, that is, 
&lt;math&gt;x_1w = y_1&lt;/math&gt; for some &quot;dangling suffix&quot; &lt;math&gt;w&lt;/math&gt;. If one tries first &lt;math&gt;x_1=011&lt;/math&gt; and &lt;math&gt;y_1=01110&lt;/math&gt;, the dangling [[suffix]] is &lt;math&gt;w = 10&lt;/math&gt;. If we manage to find two sequences &lt;math&gt;x_2,\ldots,x_p&lt;/math&gt; and &lt;math&gt;y_2,\ldots,y_q&lt;/math&gt; of codewords such that
&lt;math&gt;x_2\cdots x_p = wy_2\cdots y_q&lt;/math&gt;, then we are finished: For then the string &lt;math&gt;x = x_1x_2\cdots x_p&lt;/math&gt; can alternatively be decomposed as &lt;math&gt;y_1y_2\cdots y_q&lt;/math&gt;, and we have found the desired string having at least two different decompositions into codewords. In the second round, we try out two different approaches: the first, perhaps more obvious trial is to look for a codeword that has ''w'' as prefix. Then we obtain a new dangling suffix ''w''', with which we can continue our search. If we eventually encounter a dangling suffix that is itself a codeword (or even better: the [[empty word]]), then the search will terminate, as we know there exists a string with two decompositions. The second, and less maybe less obvious trial is to seek for a codeword that is itself a prefix of ''w''. In our example, we have &lt;math&gt;w = 10&lt;/math&gt;, and the sequence ''1'' is a codeword. We can thus also continue with ''w'=0'' as the new dangling suffix.

==Precise description of the algorithm==

The algorithm is described most conveniently using quotients of [[formal language]]s. In general, for two sets of strings ''D'' and ''N'', the (left) quotient &lt;math&gt;N^{-1}D&lt;/math&gt; is defined as the residual words obtained from ''D'' by removing some prefix in ''N''. Formally, &lt;math&gt;N^{-1}D = \{\,y \mid xy\in D ~\textrm{ and }~ x \in N \,\}&lt;/math&gt;. Now let &lt;math&gt;C&lt;/math&gt; denote the (finite) set of codewords in the given code.

The algorithm proceeds in rounds, where we maintain in each round not only one dangling suffix as described above, but the (finite) set of all potential dangling suffixes. Starting with round &lt;math&gt;i=1&lt;/math&gt;, the set of potential dangling suffixes will be denoted by &lt;math&gt;S_i&lt;/math&gt;. The sets &lt;math&gt;S_i&lt;/math&gt; are defined [[recursive definition|inductively]] as follows:

&lt;math&gt;S_1 = C^{-1}C \setminus \{\varepsilon\}&lt;/math&gt;. Here, the symbol &lt;math&gt;\varepsilon&lt;/math&gt; denotes the [[empty word]]. 

&lt;math&gt;S_{i+1} = C^{-1}S_i\cup S_i^{-1}C&lt;/math&gt;, for all &lt;math&gt;i\ge 1&lt;/math&gt;.

The algorithm computes the sets &lt;math&gt;S_i&lt;/math&gt; in increasing order of &lt;math&gt;i&lt;/math&gt;. As soon as one of the &lt;math&gt;S_i&lt;/math&gt; contains a word from ''C'' or the empty word, then the algorithm terminates and answers that the given code is not uniquely decodable. Otherwise, once a set &lt;math&gt;S_i&lt;/math&gt;
equals a previously encountered set &lt;math&gt;S_j&lt;/math&gt; with &lt;math&gt;j&lt;i&lt;/math&gt;, then the algorithm would enter in principle an endless loop. Instead of continuing endlessly, it answers that the given code is uniquely decodable. 
==Termination and correctness of the algorithm==
Since all sets &lt;math&gt;S_i&lt;/math&gt; are sets of suffixes of a finite set of codewords, there are only finitely many different candidates for &lt;math&gt;S_i&lt;/math&gt;. Since visiting one of the sets for the second time will cause the algorithm to stop, the algorithm cannot continue endlessly and thus must always [[terminate]]. A proof that the algorithm is [[correctness|correct]], i.e. that it always gives the correct answer, is given in the monograph by Berstel et al.&lt;ref&gt;Berstel et al. (2009), Chapter 2.3&lt;/ref&gt;

==See also==

*[[Kraft's inequality]] in some cases provides a quick way to exclude the possibility that a given code is uniquely decodable.
*[[Prefix code]]s and [[block code]]s are important classes of codes which are uniquely decodable by definition.
*[[Timeline of information theory]]

==Notes==

&lt;references/&gt;

==References==

*Donald E. Knuth: Robert W Floyd, In Memoriam. ''SIGACT News'' 34(4):3–13, December 2003.
*Jean Berstel, Dominique Perrin and Christophe Reutenauer: ''Codes and Automata.'' Cambridge University Press, to appear (estimated Nov. 2009). [http://www-igm.univ-mlv.fr/~berstel/LivreCodes/Codes.html Draft available online]

;Further reading

*August Albert Sardinas and George W. Patterson: A necessary and sufﬁcient condition for the unique decomposition of coded messages. ''Convention Record of the [[Institute of Radio Engineers|I.R.E.]]'', ''1953 National Convention'', Part 8: Information Theory, pp.&amp;nbsp;104–108, 1953.
* [[Robert G. Gallager]]: ''Information Theory and Reliable Communication.'' Wiley, 1968

[[Category:Algorithms]]
[[Category:Coding theory]]
[[Category:Data compression]]</text>
    </revision>
  </page>
  <page>
    <title>Dinic's algorithm</title>
    <id>23635821</id>
    <revision>
      <id>316999918</id>
      <timestamp>2009-09-30T01:14:56Z</timestamp>
      <contributor>
        <ip>128.253.102.38</ip>
      </contributor>
      <comment>/* Definition */</comment>
      <text xml:space="preserve">In [[optimization theory]], the '''Dinic's algorithm''' is a [[strongly polynomial]] algorithm for computing the [[maximum flow]] in a [[flow network]]. The algorithm runs in ''O''(''V''&lt;sup&gt;2&lt;/sup&gt;''E'') time and is similar to [[Edmonds–Karp algorithm]], which runs in ''O''(''VE''&lt;sup&gt;2&lt;/sup&gt;) time. Dinic's algorithm runs faster based on the idea of level graph and blocking flow.

==Definition==
Let ''G'' = ((''V'', ''E''), ''c'', ''s'', ''t'') be a network with ''c''(''u'',''v'') and ''f''(''u'',''v'') being the capacity and the flow of the edge (''u'',''v'') respectively.

:The '''residual capacity''' is a mapping ''c''&lt;sub&gt;''f''&lt;/sub&gt; : ''V''×''V''→''R''&lt;sup&gt;+&lt;/sup&gt; defined as,
:# if (''u'',''v'')∈''E'',
:#: ''c''&lt;sub&gt;''f''&lt;/sub&gt; (''u'',''v'') = ''c''(''u'',''v'') - ''f''(''u'',''v'') 
:#: ''c''&lt;sub&gt;''f''&lt;/sub&gt; (''v'',''u'') = ''f''(''u'',''v'')
:# ''c''&lt;sub&gt;''f''&lt;/sub&gt; (''u'',''v'') = 0 otherwise.

:The '''residual graph''' is the graph ''G''&lt;sub&gt;''f''&lt;/sub&gt; = ((''V'', ''E''&lt;sub&gt;''f''&lt;/sub&gt;), ''c''&lt;sub&gt;''f''&lt;/sub&gt;|&lt;sub&gt;E&lt;sub&gt;f&lt;/sub&gt;&lt;/sub&gt;, ''s'', ''t''), where
:: ''E''&lt;sub&gt;''f''&lt;/sub&gt; = {(''u'',''v'')∈''V''×''V'' : ''c''&lt;sub&gt;''f''&lt;/sub&gt; (''u'',''v'') &gt; 0}.

:An '''augmenting path''' is an ''s''-''t'' path in the residual graph ''G''&lt;sub&gt;''f''&lt;/sub&gt;.

:Define '''dist'''(''v'') to be the number of edges in the shortest path from ''s'' to ''v'' in ''G''&lt;sub&gt;''f''&lt;/sub&gt;. Then the '''level graph''' of ''G''&lt;sub&gt;''f''&lt;/sub&gt; is the graph ''G''&lt;sub&gt;''L''&lt;/sub&gt; = (''V'', ''E''&lt;sub&gt;''L''&lt;/sub&gt;, ''c''&lt;sub&gt;''f''&lt;/sub&gt;|&lt;sub&gt;E&lt;sub&gt;L&lt;/sub&gt;&lt;/sub&gt;, ''s'', ''t''), where
:: ''E''&lt;sub&gt;''L''&lt;/sub&gt; = {(''u'',''v'')∈''E''&lt;sub&gt;''f''&lt;/sub&gt; : ''dist''(''v'') = ''dist''(''u'') + 1}.

:A '''blocking flow''' is an ''s''-''t'' flow ''f'' such that the graph ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; = (''V'',''E''&lt;sub&gt;''L''&lt;/sub&gt;&lt;nowiki&gt;'&lt;/nowiki&gt;, ''s'', ''t'') with ''E''&lt;sub&gt;''L''&lt;/sub&gt;&lt;nowiki&gt;'&lt;/nowiki&gt; = {(''u'',''v'') : ''f''(''u'',''v'') &lt; ''c''&lt;sub&gt;''f''&lt;/sub&gt;|&lt;sub&gt;E&lt;sub&gt;L&lt;/sub&gt;&lt;/sub&gt;(''u'',''v'')} contains no ''s''-''t'' path.

==Algorithm==
'''Dinic's Algorithm'''
: ''Input'': A network ''G'' = ((''V'', ''E''), ''c'', ''s'', ''t'').
: ''Output'': An ''s''-''t'' flow ''f'' of maximum value.
# Set ''f''(''e'') = 0 for each ''e'' in ''E''.
# Construct ''G''&lt;sub&gt;''L''&lt;/sub&gt; from ''G''&lt;sub&gt;''f''&lt;/sub&gt; of ''G''. If ''dist''(''t'') = ∞, stop and output ''f''.
# Find a blocking flow ''f'' &lt;nowiki&gt;'&lt;/nowiki&gt; in ''G''&lt;sub&gt;''L''&lt;/sub&gt;. 
# Augment flow ''f'' by ''f'' &lt;nowiki&gt;'&lt;/nowiki&gt; and go back to step 2.

==Analysis==
It can be shown that the number of edges in each blocking flow increases by at least 1 each time and thus there are at most ''n''-1 blocking flows in the algorithm, where ''n'' is the number of vertices in the network. The level graph ''G''&lt;sub&gt;''L''&lt;/sub&gt; can be constructed by [[Breadth-first search]] in ''O''(''E'') time and a blocking flow in each level graph can be found in ''O''(''VE'') time. Hence, the running time of Dinic's algorithm is ''O''(''V''&lt;sup&gt;2&lt;/sup&gt;''E'').

Using a data structure called [[dynamic trees]], the running time of finding a blocking flow in each phase can be reduced to ''O''(''E'' log(''V'')) and therefore the running time of Dinic's algorithm can be improved to ''O''(''VE'' log(''V'')).

==Example==
The following is a simulation of the Dinic's algorithm. In the level graph ''G''&lt;sub&gt;''L''&lt;/sub&gt;, the vertices with labels in red are the values ''dist''(''v''). The paths in blue form a blocking flow.

{| class=&quot;wikitable&quot; style=&quot;text-align:center; width:915px;&quot; border=&quot;1&quot;
|-
! width=&quot;15px&quot; |

! ''G''
! ''G''&lt;sub&gt;''f''&lt;/sub&gt;
! ''G''&lt;sub&gt;''L''&lt;/sub&gt;
|-
! 1.
| [[Image:Dinic_algorithm_G1.svg | 300px]]
| [[Image:Dinic_algorithm_Gf1.svg | 300px]]
| [[Image:Dinic_algorithm_GL1.svg | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;3&quot; |
The blocking flow consists of
# {''s'', 1, 3, ''t''} with 4 units of flow, 
# {''s'', 1, 4, ''t''} with 6 units of flow, and 
# {''s'', 2, 4, ''t''} with 4 units of flow.
Therefore the blocking flow is of 14 units and the value of flow | ''f'' | is 14. Note that each augmenting path in the blocking flow has ''3'' edges.
|-
! 2.
| [[Image:Dinic_algorithm_G2.svg | 300px]]
| [[Image:Dinic_algorithm_Gf2.svg | 300px]]
| [[Image:Dinic_algorithm_GL2.svg | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;3&quot; |
The blocking flow consists of
# {''s'', 2, 4, 3, ''t''} with 5 units of flow.
Therefore the blocking flow is of 5 units and the value of flow | ''f'' | is 14 + 5 = 19. Note that each augmenting path has 4 edges.
|-
! 3.
| [[Image:Dinic_algorithm_G3.svg | 300px]]
| [[Image:Dinic_algorithm_Gf3.svg‎ | 300px]]
| [[Image:Dinic_algorithm_GL3.svg | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;3&quot; |
Since ''t'' cannot be reached in ''G''&lt;sub&gt;''f''&lt;/sub&gt;. The algorithm terminates and returns a flow with maximum value of 19. Note that in each blocking flow, the number of edges in the augmenting path increases by at least 1.
|-
|}

==History==
The Dinic's algorithm was published by E. A. Dinic in 1970, earlier than [[Edmonds–Karp algorithm]], which was published in 1972 but was discovered earlier. They independently showed that in the [[Ford-Fulkerson method]], if each augmenting path is the shortest one, the length of the augmenting paths is non-decreasing.

==See also==
* [[Ford-Fulkerson method]]
* [[Maximum flow problem]]

==References==
* {{cite book | author = Yefim Dinitz | editor =  [[Oded Goldreich]], Arnold  L. Rosenberg, and Alan L. Selman | title = Theoretical Computer Science: Essays in Memory of [[Shimon Even]] | chapter = Dinitz' Algorithm: The Original Version and Even's Version | year = 2006 | publisher = Springer | isbn = 978-3540328803 | pages = 218–240 | url = http://www.cs.bgu.ac.il/~dinitz/Papers/Dinitz_alg.pdf}}
* {{cite book | author =  B. H. Korte, Jens Vygen | title = Combinatorial Optimization: Theory and Algorithms (Algorithms and Combinatorics, 21) | chapter = 8.4 Blocking Flows and Fujishige's Algorithm | year = 2008 | publisher = Springer Berlin Heidelberg | isbn = 978-3-540-71844-4 | pages = 174–176}}

[[Category:Network flow]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Sequential algorithm</title>
    <id>23868049</id>
    <revision>
      <id>306113364</id>
      <timestamp>2009-08-05T00:23:19Z</timestamp>
      <contributor>
        <username>DixonD</username>
        <id>4665366</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with ''''Sequential algorithm''' - in general, any algorithm executed sequentially, but, specifically, one for decoding a [[convolutional code]]&lt;ref&gt;{{cite web|url=www.en...'</comment>
      <text xml:space="preserve">'''Sequential algorithm''' - in general, any algorithm executed sequentially, but, specifically, one for decoding a [[convolutional code]]&lt;ref&gt;{{cite web|url=www.encyclopedia.com/doc/1O11-sequentialalgorithm.html|title=A Dictionary of Computing at Encyclopedia.com}}&lt;/ref&gt;.

== References ==
{{reflist}}

{{Compu-stub}}

[[Category:Algorithms]]

[[uk:Послідовний алгоритм]]</text>
    </revision>
  </page>
  <page>
    <title>Parallel-TEBD</title>
    <id>20593011</id>
    <revision>
      <id>318465924</id>
      <timestamp>2009-10-07T15:08:22Z</timestamp>
      <contributor>
        <username>A5b</username>
        <id>722418</id>
      </contributor>
      <minor/>
      <comment>Distributed shared memory</comment>
      <text xml:space="preserve">The '''parallel-TEBD''' is a version of the [[TEBD]] algorithm adapted to run on multiple hosts. The task of parallelizing ''TEBD'' could be achieved in various ways. 
* As a first option, one could use the '''[[OpenMP]]''' [[API]] (this would probably be the simplest way to do it), using preprocessor directives to decide which portion of the code should be parallelized. The drawback of this is that one is confined to [[Symmetric multiprocessing]] (SMP) architectures and the user has no control on how the code is parallelized. An Intel extension of ''OpenMP'', called '''Cluster OpenMP''' [http://software.intel.com/en-us/articles/cluster-openmp-for-intel-compilers], is a socket-based implementation of ''OpenMP'' which can make use of a whole cluster of ''SMP'' machines; this spares the user of explicitly writing messaging code while giving access to multiple hosts via a [[Distributed shared memory|distributed shared-memory]] system. The OpenMP paradigm (hence its extension Cluster OpenMP as well) allows the user a straightforward parallelization of serial code by embedding a set of directives in it. 

* The second option is using the [[Message Passing Interface]] ('''MPI''') API. MPI can treat each core of the multi-core machines as separate execution host, so a cluster of, let's say, 10 compute nodes with dual-core processors will appear as 20 compute nodes, on which the MPI application can be distributed. MPI offers the user more control over the way the program is parallelized. The drawback of MPI is that is not very easy to implement and the programmer  has to have a certain understanding of parallel simulation systems.   

* For the determined programmer the third option would probably be the most appropriate: to write ones own routines, using a combination of '''[[Thread (computer science) |threads]]''' and '''[[Internet socket| TCP/IP sockets]]''' to complete the task. The threads are necessary in order to make the socket-based communication between the programs non-blocking (the communication between programs has to take place in threads, so that the main thread doesn't have to wait for the communication to end and can execute other parts of the code). This option offers the programmer complete control over the code and eliminates any overhead which might come from the use of the Cluster OpenMP or MPI libraries. 

This article introduces the conceptual basis of the implementation, using ''MPI''-based pseudo-code for exemplification, while not restricting itself to MPI - the same basic schema could be implemented with the use of home-grown messaging routines. 

==Introduction==

The TEBD algorithm is a good candidate for [[parallel computing]] due to the fact that the exponential operators used to calculate the time-evolution factorize under the Suzuki-Trotter expansion. A detailed presentation of the way TEBD works is given in the [[TEBD|main article]]. Here we concern ourselves only with its parallel implementation.

==Implementation==
For our purposes, we will use the canonical form of the MPS as introduced by Vidal in his original papers. Hence, we will write the function of state &lt;math&gt;| \Psi \rangle &lt;/math&gt; as:


&lt;math&gt;| \Psi \rangle=\sum\limits_{i_1,..,i_N=1}^{M}\sum\limits_{\alpha_1,..,\alpha_{N-1}=0}^{\chi}\Gamma^{[1]i_1}_{\alpha_1}\lambda^{[1]}_{\alpha_1}\Gamma^{[2]i_2}_{\alpha_1\alpha_2}\lambda^{[2]}_{\alpha_2}\Gamma^{[3]i_3}_{\alpha_2\alpha_3}\lambda^{[3]}_{\alpha_3}\cdot..\cdot\Gamma^{[{N-1}]i_{N-1}}_{\alpha_{N-2}\alpha_{N-1}}\lambda^{[N-1]}_{\alpha_{N-1}}\Gamma^{[N]i_N}_{\alpha_{N-1}} | {i_1,i_2,..,i_{N-1},i_N} \rangle&lt;/math&gt;

This function describes a '''N'''-point lattice which we would like to compute on '''P''' different compute nodes. Let us suppose, for the sake of simplicity, that N=2k*P , where k is an integer number. This means that if we distribute the lattice points evenly among the compute nodes (the easiest scenario), an even number of lattice points 2k is assigned to each compute node. Indexing the lattice points from 0 to N-1 (note that the usual indexing is 1,N) and the compute nodes from 0 to P-1, the lattice points would be distributed as follows among the nodes:
  NODE 0: 0, 1, ..., 2k-1
  NODE 1: 2k, 2k+1, ..., 4k-1
  ...
  NODE m: m*2k, ..., (m+1)*2k - 1
  ...
  NODE P-1: (P-1)*2k, ..., N-1
Using the canonical form of the MPS, we define &lt;math&gt;\lambda^{[l]}_{\alpha_l}&lt;/math&gt; as &quot;belonging&quot; to node m if m*2k ≤ l ≤ (m+1)*2k - 1. Similarly, we use the index l to assign the &lt;math&gt;{\Gamma}'s&lt;/math&gt; to a certain lattice point. This means that 
&lt;math&gt;\Gamma^{[0]i_0}_{\alpha_{0}}&lt;/math&gt; and &lt;math&gt;\Gamma^{[l]i_l}_{\alpha_{l-1}\alpha_{l}},  l=1,2k-1&lt;/math&gt;, belong to NODE 0, as well as &lt;math&gt;\lambda^{[l]}_{\alpha_l}, l = 0,2k-2&lt;/math&gt;. A parallel version of TEBD implies that the computing nodes need to exchange information among them. The information exchanged will be the MPS matrices and singular values lying at the border between neighbouring compute nodes. How this is done, it will be explained below.

The TEBD algorithm divides the exponential operator performing the time-evolution into a sequence of two-qubit gates of the form:

&lt;math&gt; e^{\frac{-i\delta}{\hbar}H_{k,k+1}}.&lt;/math&gt;

Setting the Plank constant to 1, the time-evolution is expressed as:

&lt;math&gt;| \Psi(t+\delta) \rangle = e^{{-i\delta}\frac{F}{2}}e^{{-i\delta}G}e^{{-i\delta}\frac{F}{2}}|\Psi(t) \rangle,&lt;/math&gt;

where H = F + G, 

&lt;math&gt;F \equiv \sum_{k=0}^{\frac{N}{2}-1}(H_{2k,2k+1}) = \sum_{k=0}^{\frac{N}{2}-1}(F_{2k}),&lt;/math&gt;

&lt;math&gt;G \equiv \sum_{k=0}^{\frac{N}{2}-2}(H_{2k+1,2k+2}) = \sum_{k=0}^{\frac{N}{2}-2}(G_{2k+1}).&lt;/math&gt;

What we can explicitly compute in parallel is the sequence of gates &lt;math&gt; e^{{-i}\frac{\delta}{2}F_{2k}}, e^{{-i\delta}{G_{2k+1}}}.&lt;/math&gt; 
Each of the compute node can apply most of the two-qubit gates without needing information from its neighbours. The compute nodes need to exchange information only at the borders, where two-qubit gates cross them, or just need information from the other side. We will now consider all three sweeps, two even and one odd and see what information has to be exchanged. Let us see what is happening on node ''m'' during the sweeps. 


=== First (even) sweep ===
The sequence of gates that has to be applied in this sweep is:

&lt;math&gt;e^{{-i}\frac{\delta}{2}F_{m*2k}}, e^{{-i}\frac{\delta}{2}F_{m*2k + 2}},...,e^{{-i}\frac{\delta}{2}F_{(m+1)*2k - 2}}&lt;/math&gt;

Already for computing the first gate, process ''m'' needs information from its lowest neighbour, ''m-1''. On the other side, ''m'' doesn't need anything from its &quot;higher&quot; neighbour, ''m+1'', because it has all the information it needs to apply the last gate. So the best strategy for ''m'' is to send a request to ''m-1'', postponing the calculation of the first gate for later, and continue with the calculation of the other gates. What ''m'' does is called [[Non-blocking_I/O | non-blocking communication]]. Let's look at this in detail. The tensors involved in the calculation of the first gate are &lt;ref name=vidal&gt;Guifré Vidal, ''Efficient Classical Simulation of Slightly Entangled Quantum Computations'', PRL 91, 147902 (2003)[http://www.citebase.org/cgi-bin/citations?id=oai:arXiv.org:quant-ph/0301063]&lt;/ref&gt; :

&lt;references/&gt;

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Gomory–Hu tree</title>
    <id>24093092</id>
    <revision>
      <id>310479371</id>
      <timestamp>2009-08-28T02:42:28Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <text xml:space="preserve">In [[combinatorial optimization]], the '''Gomory–Hu tree''' of an undirected graph with capacities is a weighted [[Tree (graph theory)|tree]] that consists of edges representing all pairs minimum ''s''-''t'' cut in the graph. The Gomory–Hu tree can be constructed in |&amp;nbsp;''V''&amp;nbsp;|&amp;nbsp;&amp;minus;&amp;nbsp;1 times [[minimum cut]] computation.

==Definition==
Let ''G'' = ((''V''&lt;sub&gt;G&lt;/sub&gt;, ''E''&lt;sub&gt;G&lt;/sub&gt;), ''c'') be an undirected graph with ''c''(''u'',''v'') being the capacity of the edge (''u'',''v'') respectively.

: Denote the minimum capacity of an ''s''-''t'' cut by λ&lt;sub&gt;st&lt;/sub&gt; for each ''s'', ''t'' ∈ ''V''&lt;sub&gt;G&lt;/sub&gt;.
: Let ''T'' = (''V''&lt;sub&gt;T&lt;/sub&gt;,''E''&lt;sub&gt;T&lt;/sub&gt;) be a tree with ''V''&lt;sub&gt;T&lt;/sub&gt; = ''V''&lt;sub&gt;G&lt;/sub&gt;, denote the set of edges in an ''s''-''t'' path by ''P''&lt;sub&gt;st&lt;/sub&gt; for each ''s'',''t'' ∈ ''V''&lt;sub&gt;T&lt;/sub&gt;.
Then ''T'' is said to be a '''Gomory–Hu tree''' of ''G'' if
: λ&lt;sub&gt;st&lt;/sub&gt; = min&lt;sub&gt;e∈P&lt;sub&gt;st&lt;/sub&gt;&lt;/sub&gt; ''c''(''S''&lt;sub&gt;e&lt;/sub&gt;, ''T''&lt;sub&gt;e&lt;/sub&gt;) for all ''s'', ''t'' ∈ ''V''&lt;sub&gt;G&lt;/sub&gt;,
where 
# ''S''&lt;sub&gt;e&lt;/sub&gt; and ''T''&lt;sub&gt;e&lt;/sub&gt; are the two connected components of ''T''∖{''e''} in the sense that (''S''&lt;sub&gt;e&lt;/sub&gt;, ''T''&lt;sub&gt;e&lt;/sub&gt;) form a  ''s''-''t'' cut in ''G'', and
# ''c''(''S''&lt;sub&gt;e&lt;/sub&gt;, ''T''&lt;sub&gt;e&lt;/sub&gt;) is the capacity of the cut in ''G''.

==Algorithm==
'''Gomory–Hu Algorithm'''
:''Input'': A weighted undirected graph G = ((''V''&lt;sub&gt;G&lt;/sub&gt;, ''E''&lt;sub&gt;G&lt;/sub&gt;), ''c'').
: ''Output'': A Gomory–Hu Tree ''T'' = (''V''&lt;sub&gt;T&lt;/sub&gt;, ''E''&lt;sub&gt;T&lt;/sub&gt;).
:1. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = {''V''&lt;sub&gt;G&lt;/sub&gt;} and ''E''&lt;sub&gt;T&lt;/sub&gt; = ∅.
:2. Choose some ''X''∈''V''&lt;sub&gt;T&lt;/sub&gt; with | ''X'' | ≥ 2 if such ''X'' exists. Otherwise, go to step 6.
:3. For each connected component ''C'' = (''V''&lt;sub&gt;C&lt;/sub&gt;, ''E''&lt;sub&gt;C&lt;/sub&gt;) in ''T''∖''X''. Let ''S''&lt;sub&gt;C&lt;/sub&gt; = ∪&lt;sub&gt;v&lt;sub&gt;T&lt;/sub&gt;∈V&lt;sub&gt;C&lt;/sub&gt;&lt;/sub&gt; ''v''&lt;sub&gt;T&lt;/sub&gt;. Let ''S'' = { ''S''&lt;sub&gt;C&lt;/sub&gt; | ''C'' is a connected component in ''T''∖''X''}.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Contract the components to form ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; = ((''V''&lt;sub&gt;G&lt;nowiki&gt;'&lt;/nowiki&gt;&lt;/sub&gt;, ''E''&lt;sub&gt;G&lt;nowiki&gt;'&lt;/nowiki&gt;&lt;/sub&gt;), ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;), where 
:: ''V''&lt;sub&gt;G&lt;nowiki&gt;'&lt;/nowiki&gt;&lt;/sub&gt; = ''X'' ∪ ''S''.
:: ''E''&lt;sub&gt;G&lt;nowiki&gt;'&lt;/nowiki&gt;&lt;/sub&gt; = ''E''&lt;sub&gt;G&lt;/sub&gt;|&lt;sub&gt;X×X&lt;/sub&gt; ∪ {(''u'', ''S''&lt;sub&gt;C&lt;/sub&gt;) ∈ ''X''×''S'' | (''u'',''v'')∈''E''&lt;sub&gt;G&lt;/sub&gt; for some ''v''∈''S''&lt;sub&gt;C&lt;/sub&gt;}.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt; : ''V''&lt;sub&gt;G&lt;nowiki&gt;'&lt;/nowiki&gt;&lt;/sub&gt;×''V''&lt;sub&gt;G&lt;nowiki&gt;'&lt;/nowiki&gt;&lt;/sub&gt;→''R''&lt;sup&gt;+&lt;/sup&gt; is the capacity function defined as,
::# if (''u'',''S''&lt;sub&gt;C&lt;/sub&gt;)∈''E''&lt;sub&gt;G&lt;/sub&gt;|&lt;sub&gt;X×S&lt;/sub&gt;, ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''u'',''S''&lt;sub&gt;C&lt;/sub&gt;) = Σ&lt;sub&gt;v∈S&lt;sub&gt;C&lt;/sub&gt;:(u,v)∈E&lt;sub&gt;G&lt;/sub&gt;&lt;/sub&gt;''c''(''u'',''v''),
::# ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''u'',''v'') = ''c''(''u'',''v'') otherwise.
:4. Choose two vertices ''s'', ''t'' ∈ ''X'' and find a minimum ''s''-''t'' cut (''A''&lt;nowiki&gt;'&lt;/nowiki&gt;,''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''A'' = (∪&lt;sub&gt;S&lt;sub&gt;C&lt;/sub&gt;∈''A''&lt;nowiki&gt;'&lt;/nowiki&gt;∩''S''&lt;/sub&gt; ''S''&lt;sub&gt;C&lt;/sub&gt;) ∪ (''A''&lt;nowiki&gt;'&lt;/nowiki&gt; ∩ ''X'') and ''B'' = (∪&lt;sub&gt;S&lt;sub&gt;C&lt;/sub&gt;∈''B''&lt;nowiki&gt;'&lt;/nowiki&gt;∩''S''&lt;/sub&gt; ''S''&lt;sub&gt;C&lt;/sub&gt;) ∪ (''B''&lt;nowiki&gt;'&lt;/nowiki&gt; ∩ ''X'').
: 5. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = (''V''&lt;sub&gt;T&lt;/sub&gt;∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''}.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; For each ''e'' = (''X'', ''Y'') ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; do
:: If ''Y''⊂''A'', set ''e''&lt;nowiki&gt;'&lt;/nowiki&gt; = (''A'' ∩ ''X'', ''Y''), else set ''e''&lt;nowiki&gt;'&lt;/nowiki&gt; = (''B'' ∩ ''X'', ''Y'').
:: Set ''E''&lt;sub&gt;T&lt;/sub&gt; = (''E''&lt;sub&gt;T&lt;/sub&gt;∖{''e''}) ∪ {''e''&lt;nowiki&gt;'&lt;/nowiki&gt;} and ''w''(''e''&lt;nowiki&gt;'&lt;/nowiki&gt;) = ''w''(''e'').
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''E''&lt;sub&gt;T&lt;/sub&gt; = ''E''&lt;sub&gt;T&lt;/sub&gt; ∪ {(''A''∩''X'', ''B''∩''X'')}.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''w''((''A''∩''X'', ''B''∩''X'')) = ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;).
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Go to step 2.
: 6. Replace each {''v''} ∈ ''V''&lt;sub&gt;T&lt;/sub&gt; by ''v'' and each ({''u''},{''v''}) ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; by (''u'',''v''). Output ''T''.

==Analysis==
Using the [[submodular]] property of the capacity function ''c'', one has
: ''c''(''X'') + ''c''(''Y'') ≥ ''c''(''X'' ∩ ''Y'') + ''c''(''X'' ∪ ''Y'').
Then it can be shown that the minimum ''s''-''t'' cut in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; is also a minimum ''s''-''t'' cut in ''G'' for any ''s'', ''t'' ∈ ''X''.

To show that for all (''P'', ''Q'') ∈ ''E''&lt;sub&gt;T&lt;/sub&gt;, ''w''(''P'',''Q'') = λ&lt;sub&gt;''pq''&lt;/sub&gt; for some ''p'' ∈ ''P'', ''q'' ∈ ''Q'' throughout the algorithm, one makes use of the following Lemma,
: For any ''i'', ''j'', ''k'' in ''V''&lt;sub&gt;G&lt;/sub&gt;, λ&lt;sub&gt;ik&lt;/sub&gt; ≥ min(λ&lt;sub&gt;ij&lt;/sub&gt;, λ&lt;sub&gt;jk&lt;/sub&gt;).

The Lemma can be used again repeatedly to show that the output ''T'' satisfies the properties of a Gomory–Hu Tree.

==Example==
The following is a simulation of the Gomory–Hu's algorithm, where
# ''green'' circles are vertices of ''T''.
# ''red'' and ''blue'' circles are the vertices in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;.
# ''grey'' vertices are the chosen ''s'' and ''t''.
# ''red'' and ''blue'' coloring represents the ''s''-''t'' cut.
# ''dashed'' edges are the ''s''-''t'' cut-set.
# ''A'' is the set of vertices circled in ''red'' and ''B'' is the set of vertices circled in ''blue''.

{| class=&quot;wikitable&quot; style=&quot;text-align:center; width:800px;&quot; border=&quot;1&quot;
|-
! width=&quot;15px&quot; |

! ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;
! ''T''
|-
|
| [[Image:Gomory–Hu_G.svg | 300px]]
| [[Image:Gomory–Hu_T.svg | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
:1. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = {''V''&lt;sub&gt;G&lt;/sub&gt;} = { {0, 1, 2, 3, 4, 5} } and ''E''&lt;sub&gt;T&lt;/sub&gt; = ∅.
:2. Since ''V''&lt;sub&gt;T&lt;/sub&gt; has only one vertex, choose ''X'' = ''V''&lt;sub&gt;G&lt;/sub&gt; = {1, 2, 3, 4, 5, 6}. Note that | ''X'' | = 6 ≥ 2.
|-
! 1.
| [[Image:Gomory–Hu_Gp1.svg | 300px]]
| [[Image:Gomory–Hu_T1.svg | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
: 3. Since ''T''∖''X'' = ∅, there is no contraction and therefore ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; = ''G''. 

: 4. Choose ''s'' = 1 and ''t'' = 5. The minimum ''s''-''t'' cut (''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) is ({0, 1, 2, 4}, {3, 5}) with ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 6.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''A'' = {0, 1, 2, 4} and ''B'' = {3, 5}.
: 5. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = (''V''&lt;sub&gt;T&lt;/sub&gt;∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {0, 1, 2, 4}, {3, 5} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''E''&lt;sub&gt;T&lt;/sub&gt; = { ({0, 1, 2, 4}, {3, 5}) }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''w''( ({0, 1, 2, 4}, {3, 5}) ) = ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 6.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Go to step 2.
: 2. Choose ''X'' = {3, 5}. Note that | ''X'' | = 2 ≥ 2.
|-
! 2.
| [[Image:Gomory–Hu_Gp2.svg | 300px]]
| [[Image:Gomory–Hu_T2.svg‎ | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
: 3. {0, 1, 2, 4} is the connected component in ''T''∖''X'' and thus ''S'' = { {0, 1, 2, 4} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Contract {0, 1, 2, 4} to form ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;, where
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (3, {0, 1, 2 ,4}) ) = ''c''( (3, 1) ) + ''c''( (3, 4) ) = 4.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (5, {0, 1, 2, 4}) ) = ''c''( (5, 4) ) = 2.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (3, 5)) = ''c''( (3, 5) ) = 6.
: 4. Choose ''s'' = 3, ''t'' = 5. The minimum ''s''-''t'' cut (''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; is ( {{0, 1, 2, 4}, 3}, {5} ) with ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 8.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''A'' = {0, 1, 2, 3, 4} and ''B'' = {5}.
: 5. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = (''V''&lt;sub&gt;T&lt;/sub&gt;∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {0, 1, 2, 4}, {3}, {5} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Since (''X'', {0, 1, 2, 4}) ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; and {0, 1, 2, 4} ⊂ ''A'', replace it with (''A'' ∩ ''X'', ''Y'') = ({3}, {0, 1, 2 ,4}).
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''E''&lt;sub&gt;T&lt;/sub&gt; = { ({3}, {0, 1, 2 ,4}), ({3}, {5}) } with
:: ''w''(({3}, {0, 1, 2 ,4})) = ''w''((''X'', {0, 1, 2, 4})) = 6.
:: ''w''(({3}, {5})) = ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 8.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Go to step 2.
: 2. Choose ''X'' = {0, 1, 2, 4}. Note that | ''X'' | = 4 ≥ 2.
|-
! 3.
| [[Image:Gomory–Hu_Gp3.svg | 300px]]
| [[Image:Gomory–Hu_T3.svg | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
: 3. { {3}, {5} } is the connected component in ''T''∖''X'' and thus ''S'' = { {3, 5} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Contract {3, 5} to form ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;, where
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (1, {3, 5}) ) = ''c''( (1, 3) ) = 3.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (4, {3, 5}) ) = ''c''( (4, 3) ) + ''c''( (4, 5) ) = 3.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''u'',''v'') = ''c''(''u'',''v'') for all ''u'',''v'' ∈ ''X''.
: 4. Choose ''s'' = 1, ''t'' = 2. The minimum ''s''-''t'' cut (''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; is ( {1, {3, 5}, 4}, {0, 2} ) with ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 6.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''A'' = {1, 3, 4, 5} and ''B'' = {0, 2}.
: 5. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = (''V''&lt;sub&gt;T&lt;/sub&gt;∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {3}, {5}, {1, 4}, {0, 2} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Since (''X'', {3}) ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; and {3} ⊂ ''A'', replace it with (''A'' ∩ ''X'', ''Y'') = ({1, 4}, {3}).
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''E''&lt;sub&gt;T&lt;/sub&gt; = { ({1, 4}, {3}), ({3}, {5}), ({0, 2}, {1, 4}) } with
:: ''w''(({1, 4}, {3})) = ''w''((''X'', {3})) = 6.
:: ''w''(({0, 2}, {1, 4})) = ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 6.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Go to step 2.
: 2. Choose ''X'' = {1, 4}. Note that | ''X'' | = 2 ≥ 2.
|-
! 4.
| [[Image:Gomory–Hu_Gp4.svg | 300px]]
| [[Image:Gomory–Hu_T4.svg‎ | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
: 3. { {3}, {5} }, { {0, 2} } are the connected components in ''T''∖''X'' and thus ''S'' = { {0, 2}, {3, 5} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Contract {0, 2} and {3, 5} to form ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;, where
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (1, {3, 5}) ) = ''c''( (1, 3) ) = 3.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (4, {3, 5}) ) = ''c''( (4, 3) ) + ''c''( (4, 5) ) = 3.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (1, {0, 2}) ) = ''c''( (1, 0) ) + ''c''( (1, 2) ) = 2.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (4, {0, 2}) ) = ''c''( (4, 2) ) = 4.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''u'',''v'') = ''c''(''u'',''v'') for all ''u'',''v'' ∈ ''X''.
: 4. Choose ''s'' = 1, ''t'' = 4. The minimum ''s''-''t'' cut (''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; is ( {1, {3, 5}}, {{0, 2}, 4} ) with ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 7.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''A'' = {1, 3, 5} and ''B'' = {0, 2, 4}.
: 5. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = (''V''&lt;sub&gt;T&lt;/sub&gt;∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {3}, {5}, {0, 2}, {1}, {4} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Since (''X'', {3}) ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; and {3} ⊂ ''A'', replace it with (''A'' ∩ ''X'', ''Y'') = ({1}, {3}).
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Since (''X'', {0, 2}) ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; and {0, 2} ⊂ ''B'', replace it with (''B'' ∩ ''X'', ''Y'') = ({4}, {0, 2}).
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''E''&lt;sub&gt;T&lt;/sub&gt; = { ({1}, {3}), ({3}, {5}), ({4}, {0, 2}), ({1}, {4}) } with
:: ''w''(({1}, {3})) = ''w''((''X'', {3})) = 6.
:: ''w''(({4}, {0, 2})) = ''w''((''X'', {0, 2})) = 6.
:: ''w''(({1}, {4})) = ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 7.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Go to step 2.
: 2. Choose ''X'' = {0, 2}. Note that | ''X'' | = 2 ≥ 2.
|-
! 5.
| [[Image:Gomory–Hu_Gp5.svg | 300px]]
| [[Image:Gomory–Hu_T5.svg‎ | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
: 3. { {1}, {3}, {4}, {5} } is the connected component in ''T''∖''X'' and thus ''S'' = { {1, 3, 4, 5} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Contract {1, 3, 4, 5} to form ''G''&lt;nowiki&gt;'&lt;/nowiki&gt;, where
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (0, {1, 3, 4, 5}) ) = ''c''( (0, 1) ) = 1.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (2, {1, 3, 4, 5}) ) = ''c''( (2, 1) ) + ''c''( (2, 4) ) = 5.
:: ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;( (0, 2) ) = ''c''( (0, 2) ) = 7.
: 4. Choose ''s'' = 0, ''t'' = 2. The minimum ''s''-''t'' cut (''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) in ''G''&lt;nowiki&gt;'&lt;/nowiki&gt; is ( {0}, {2, {1, 3, 4, 5}} ) with ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 8.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''A'' = {0} and ''B'' = {1, 2, 3 ,4 ,5}.
: 5. Set ''V''&lt;sub&gt;T&lt;/sub&gt; = (''V''&lt;sub&gt;T&lt;/sub&gt;∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {3}, {5}, {1}, {4}, {0}, {2} }.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Since (''X'', {4}) ∈ ''E''&lt;sub&gt;T&lt;/sub&gt; and {4} ⊂ ''B'', replace it with (''B'' ∩ ''X'', ''Y'') = ({2}, {4}).
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Set ''E''&lt;sub&gt;T&lt;/sub&gt; = { ({1}, {3}), ({3}, {5}), ({2}, {4}), ({1}, {4}), ({0}, {2}) } with
:: ''w''(({2}, {4})) = ''w''((''X'', {4})) = 6.
:: ''w''(({0}, {2})) = ''c''&lt;nowiki&gt;'&lt;/nowiki&gt;(''A''&lt;nowiki&gt;'&lt;/nowiki&gt;, ''B''&lt;nowiki&gt;'&lt;/nowiki&gt;) = 8.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Go to step 2.
: 2. There does not exist ''X''∈''V''&lt;sub&gt;T&lt;/sub&gt; with | ''X'' | ≥ 2. Hence, go to step 6.
|-
! 6.
| align=&quot;center&quot; colspan=&quot;2&quot; |
[[Image:Gomory–Hu_output.svg‎ | 300px]]
|-
| 
| align=&quot;left&quot; colspan=&quot;2&quot; |
: 6. Replace ''V''&lt;sub&gt;T&lt;/sub&gt; = { {3}, {5}, {1}, {4}, {0}, {2} } by {3, 5, 1, 4, 0, 2}.
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Replace ''E''&lt;sub&gt;T&lt;/sub&gt; = { ({1}, {3}), ({3}, {5}), ({2}, {4}), ({1}, {4}), ({0}, {2}) } by { (1, 3), (3, 5), (2, 4), (1, 4), (0, 2) }. 
:&amp;nbsp;&amp;nbsp;&amp;nbsp; Output ''T''. Note that exactly |&amp;nbsp;''V''&amp;nbsp;|&amp;nbsp;&amp;minus;&amp;nbsp;1 =&amp;nbsp;6&amp;nbsp;&amp;minus;&amp;nbsp;1&amp;nbsp;=&amp;nbsp;5 times min-cut computation is performed.
|-
|}

==Implementation==
The Gusfield's algorithm can be used to find a Gomory–Hu tree without any vertex contraction in the same running time-complexity, which simplifies the implementation of constructing a Gomory–Hu Tree.

==History==
The Gomory–Hu tree was introduced by [[Ralph E. Gomory|R. E. Gomory]] and T. C. Hu in 1961.

==See also==
* [[Cut (graph theory)]]
* [[Max-flow min-cut theorem]]
* [[Maximum flow problem]]

==References==
*{{cite journal
 | author = Dan Gusfield
 | title = Very Simple Methods for All Pairs Network Flow Analysis
 | journal = SIAM J. Comput.
 | volume = 19
 | issue = 1
 | pages = 143–155
 | year = 1990
}}
* {{cite book | author =  B. H. Korte, Jens Vygen | title = Combinatorial Optimization: Theory and Algorithms (Algorithms and Combinatorics, 21) | chapter = 8.6 Gomory–Hu Trees | year = 2008 | publisher = Springer Berlin Heidelberg | isbn = 978-3-540-71844-4 | pages = 180–186}}

[[Category:Combinatorial optimization]]
[[Category:Network flow]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Category:Algorithm stubs</title>
    <id>24109263</id>
    <revision>
      <id>310012757</id>
      <timestamp>2009-08-25T18:13:05Z</timestamp>
      <contributor>
        <username>RobinK</username>
        <id>10041416</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '{{WPSS-cat}}  {{Stub Category |article=[[Algorithm]]s |newstub=algorithm-stub |category=Algorithms}}  [[Category:Computer science stubs|*Algorithms]]'</comment>
      <text xml:space="preserve">{{WPSS-cat}}

{{Stub Category
|article=[[Algorithm]]s
|newstub=algorithm-stub
|category=Algorithms}}

[[Category:Computer science stubs|*Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Algorithmica</title>
    <id>22882248</id>
    <revision>
      <id>324811404</id>
      <timestamp>2009-11-09T09:20:31Z</timestamp>
      <contributor>
        <username>RussBot</username>
        <id>279219</id>
      </contributor>
      <minor/>
      <comment>Robot: change redirected category [[:Category:Springer scientific journals|Springer scientific journals]] to [[:Category:Springer academic journals|Springer academic journals]]</comment>
      <text xml:space="preserve">{{italictitle}}
{{refimprove|date=September 2009}}

'''''Algorithmica''''' is a monthly [[computer science]] journal, published by [[Springer Science+Business Media|Springer]] [[New York City|New York]]. It includes both theoretical papers on [[algorithm]]s addressing problems arising in practical areas and experimental papers covering practical aspects or techniques.

''Algorithmica'' is available in print and electronic form:

* {{ISSN|0178-4617}} (print)
* {{ISSN|1432-0541}} (electronic)

==References==
*{{cite web
| title=Journal Rankings
| date=July 2008
| work=CORE: The Computing Research and Education Association of Australasia
| accessdate=2009-10-28
| url=http://www.core.edu.au/journal%20rankings/Journal%20Rankings.html
}} ''Algorithmica'' received the highest possible ranking “A*”.

== External links ==
* [http://www.springer.com/computer/foundations/journal/453 Springer information]

{{sci-journal-stub}}
{{comp-stub}}

[[Category:Theoretical computer science]]
[[Category:Computer science journals]]
[[Category:Algorithms]]
[[Category:Springer academic journals]]</text>
    </revision>
  </page>
  <page>
    <title>Kronecker substitution</title>
    <id>24439531</id>
    <revision>
      <id>315642762</id>
      <timestamp>2009-09-23T03:32:49Z</timestamp>
      <contributor>
        <username>SmackBot</username>
        <id>433328</id>
      </contributor>
      <minor/>
      <comment>Date maintenance tags and general  fixes</comment>
      <text xml:space="preserve">{{Unreferenced|date=September 2009}}

'''Kronecker substitution''' is a technique for evaluating coefficients of [[polynomial]] expressions using [[integer arithmetic]]. It works by evaluating the polynomials involved at a value ''x'' that is a large power of 2 that is sufficiently large that the individual integer powers of ''x'' are sufficiently widely spaced so that the binary representations of the coefficients of each term of the output value can be read directly out of the binary representation of the overall sum.

== See also ==
* [[Kronecker product]]

[[Category:Algebra]]
[[Category:Algorithms]]


{{math-stub}}</text>
    </revision>
  </page>
  <page>
    <title>Flowchart</title>
    <id>527453</id>
    <revision>
      <id>328176451</id>
      <timestamp>2009-11-27T08:18:13Z</timestamp>
      <contributor>
        <username>EnOreg</username>
        <id>4413032</id>
      </contributor>
      <comment>[[Talk:Flowchart#Tools]]</comment>
      <text xml:space="preserve">[[Image:LampFlowchart.svg|thumb|right| A simple flowchart representing a process for dealing with a non-functioning lamp.]] 
A '''flowchart''' is a common type of [[diagram]], that represents an [[algorithm]] or [[process]], showing the steps as boxes of various kinds, and their order by connecting these with arrows. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields.&lt;ref name =&quot;SSEV&quot;&gt;[http://pascal.computer.org/sev_display/index.action SEVOCAB: Software and Systems Engineering Vocabulary]. Term: ''Flow chart''. retrieved 31 July 2008.&lt;/ref&gt;

== History ==
The first structured method for documenting process flow, the &quot;flow process chart&quot;, was introduced by [[Frank Gilbreth]] to members of [[American Society of Mechanical Engineers|ASME]] in 1921 as the presentation “Process Charts—First Steps in Finding the One Best Way”.  Gilbreth's tools quickly found their way into [[industrial engineering]] curricula.  In the early 1930s, an industrial engineer, Allan H. Mogensen began training business people in the use of some of the tools of industrial engineering at his Work Simplification Conferences in [[Lake Placid, New York|Lake Placid]], [[New York]]. 

A 1944 graduate of Mogensen's class, Art Spinanger, took the tools back to [[Procter and Gamble]] where he developed their Deliberate Methods Change Program.  Another 1944 graduate, [[Benjamin S. Graham|Ben S. Graham]], Director of Formcraft Engineering at Standard Register Corporation, adapted the flow process chart to information processing with his development of the multi-flow process chart to displays multiple documents and their relationships.  In 1947, ASME adopted a symbol set derived from Gilbreth's original work as the ASME Standard for Process Charts by Mishad,Ramsan,Raiaan.

[[Douglas Hartree]] explains that [[Herman Goldstine]] and [[John von Neumann]] developed the flow chart (originally, diagram) to plan computer programs.&lt;ref&gt;{{cite book | last = Hartree | first = Douglas | authorlink = Douglas R. Hartree | title = Calculating Instruments and Machines | publisher = The University of Illinois Press | date = 1949 | page = 112}}&lt;/ref&gt; His contemporary account is endorsed by IBM engineers&lt;ref&gt;{{cite book | last = Bashe | first = Charles | title = IBM's Early Computers | publisher = The MIT Press | date = 1986 | page = 327}}&lt;/ref&gt; and by Goldstine's personal recollections.&lt;ref&gt;{{cite book  | last = Goldstine  | first = Herman  | authorlink = Herman H. Goldstine  | title = The Computer from Pascal to Von Neumann  | publisher = Princeton University Press   | date = 1972  | pages = 266–267  | isbn = 0-691-08104-2 }}&lt;/ref&gt; The original programming flow charts of Goldstine and von Neumann can be seen in their unpublished report, &quot;Planning and coding of problems for an electronic computing instrument, Part II, Volume 1,&quot; 1947, which is reproduced in von Neumann's collected works.&lt;ref&gt;{{cite book | last = Taub | first = Abraham | authorlink = Abraham H. Taub | title = John von Neumann Collected Works | publisher = Macmillan | date = 1963 | volume = 5 | pages = 80–151}}&lt;/ref&gt;

Flowcharts used to be a popular means for describing [[computer algorithm]]s and are still used for this purpose.&lt;ref&gt; Bohl, Rynn: &quot;Tools for Structured and Object-Oriented Design&quot;, Prentice Hall, 2007.&lt;/ref&gt; Modern techniques such as [[Unified Modeling Language|UML]] [[activity diagram]]s can be considered to be extensions of the flowchart. Back in the 1970s the popularity of flowcharts as an own method decreased when interactive [[computer terminal]]s and [[third-generation programming language]]s became the common tools of the [[computer programming|trade]], since algorithms can be expressed much more concisely and readably as [[source code]] in such a [[programming language|language]]. Often, [[pseudo-code]] is used, which uses the common idioms of such languages without strictly adhering to the details of a particular one.

== Flowchart building blocks ==
=== Examples ===
[[Image:FlowchartExample.png|thumb|right|A simple flowchart for computing factorial N (N!)]]
A flowchart for computing [[factorial]] N (N!) where N! = (1 * 2 * 3 * ... * N), see image.  This flowchart represents a &quot;loop and a 
half&quot; — a situation discussed in introductory programming textbooks that requires either a duplication of a component (to be both inside and outside the loop) or the component to be put inside a branch in the loop.

=== Symbols ===  
A typical flowchart from older Computer Science textbooks may have the following kinds of symbols:   
;Start and end symbols: Represented as circles, ovals or rounded rectangles, usually containing the word &quot;Start&quot; or &quot;End&quot;, or another phrase signaling the start or end of a process, such as &quot;submit enquiry&quot; or &quot;receive product&quot;.   

;Arrows: Showing what's called &quot;[[control flow|flow of control]]&quot; in computer science. An arrow coming from one symbol and ending at another symbol represents that control passes to the symbol the arrow points to.   

;Processing steps: Represented as rectangles. Examples: &quot;Add 1 to X&quot;; &quot;replace identified part&quot;; &quot;save changes&quot; or similar.

;Input/Output:Represented as a [[parallelogram]]. Examples: Get X from the user; display X.   

;Conditional or decision: Represented as a diamond ([[rhombus]]). These typically contain a Yes/No question or True/False test. This symbol is unique in that it has two arrows coming out of it, usually from the bottom point and right point, one corresponding to Yes or True, and one corresponding to No or False. The arrows should always be labeled. More than two arrows can be used, but this is normally a clear indicator that a complex decision is being taken, in which case it may need to be broken-down further, or replaced with the &quot;pre-defined process&quot; symbol.   

A number of other symbols that have less universal currency, such as:   
* A ''Document'' represented as a [[rectangle]] with a wavy base;   
* A ''Manual input'' represented by [[parallelogram]], with the top irregularly sloping up from left to right. An example would be to signify data-entry from a form;   
* A ''Manual operation'' represented by a [[trapezoid]] with the longest parallel side at the top, to represent an operation or adjustment to process that can only be made manually.   
* A ''Data File'' represented by a cylinder.

Flowcharts may contain other symbols, such as connectors, usually represented as circles, to represent converging paths in the flowchart. Circles will have more than one arrow coming into them but only one going out. Some flowcharts may just have an arrow point to another arrow instead. These are useful to represent an [[Iteration|iterative]] process (what in Computer Science is called a [[Control flow#Loop|loop]]). A loop may, for example, consist of a connector where control first enters, processing steps, a conditional with one arrow exiting the loop, and one going back to the connector. Off-page connectors are often used to signify a connection to a (part of another) process held on another sheet or screen. It is important to remember to keep these connections logical in order. All processes should flow from top to bottom and left to right.

== Types of flowcharts ==
[[File:(1) 2008-04-07 Information Management- Help Desk.jpg|thumb|Example of a system flowchart.]] 
Sterneckert (2003) suggested that flowcharts can be modelled from the perspective of different user groups (such as managers, system analysts and clerks) and that there are four general types:&lt;ref name=&quot;Ster03&quot;&gt; Alan B. Sterneckert (2003)''Critical Incident Management''. [http://books.google.co.uk/books?id=8z93xStbEpAC&amp;lpg=PP126&amp;pg=PA126#v=onepage&amp;q=&amp;f=false p. 126]&lt;/ref&gt; 
* ''Document flowcharts'', showing controls over a document-flow through a system
* ''Data flowcharts'', showing controls over a data flows in a system
* ''System flowcharts'' showing controls at a physical or resource level
* ''Program flowchart'', showing the controls in a program within a system 
Notice that every type of flowchart focusses on some kind of control, rather than on the particular flow itself.&lt;ref name=&quot;Ster03&quot;/&gt;

However there are several of these classifications. For example Andrew Veronis (1978) named three basic types of flowcharts: the ''system flowchart'', the ''general flowchart'', and the ''detailed flowchart''.&lt;ref&gt;Andrew Veronis (1978) ''Microprocessors: Design and Applications‎''. [http://books.google.co.uk/books?id=GZ9QAAAAMAAJ&amp;q=%22three+basic+types+of+flowcharts+%28ie,+the+system+flowchart,+the+general+flowchart,+and+the+detailed+flowchart%29.%22&amp;dq=%22three+basic+types+of+flowcharts+%28ie,+the+system+flowchart,+the+general+flowchart,+and+the+detailed+flowchart%29.%22&amp;as_brr=0 p. 111]&lt;/ref&gt; That same year Marilyn Bohl (1978)  stated &quot;in practice, two kinds of flowcharts are used in solution planning: ''system flowcharts'' and ''program flowcharts''...&quot;.&lt;ref&gt;Marilyn Bohl (1978) ''A Guide for Programmers‎''. p. 65.&lt;/ref&gt; More recently Mark A. Fryman (2001) stated that there are more differences: &quot;Decision flowcharts, logic flowcharts, systems flowcharts, product flowcharts, and process flowcharts are just a few of the different types of flowcharts that are used in business and government&quot;.&lt;ref&gt;Mark A. Fryman (2001) ''Quality and Process Improvement‎''. [http://books.google.co.uk/books?id=M-_B7czAy0kC&amp;pg=PA169#v=onepage&amp;q=&amp;f=false p. 169].&lt;/ref&gt;

In addition, many diagram techniques exist that are similar to flowcharts but carry a different name, such as [[Unified Modeling Language|UML]] [[activity diagram]]s.

== Software ==
=== Manual ===
[[File:Flowchart-template.jpg|thumb|A paper-and-pencil template for drawing flowcharts, late 1970's. ]] 
Any [[List of vector graphics editors|vector-based drawing program]] can be used to create flowchart diagrams, but these will have no underlying data model to share data with databases or other programs such as [[project management]] systems or [[spreadsheet]]s. Some tools offer special support for flowchart drawing, e.g., [[SmartDraw]], [[Visio]] and [[OmniGraffle]].

=== Automatic ===
Many software packages exist that can create flowcharts automatically, either directly from source code, or from a flowchart description language.

=== Web-based ===
Recently, online flowchart solutions have become available, e.g., [[Lucid Chart]] and [[DrawAnywhere]]. They are easy to use, collaborative and flexible but do not yet meet the power of off-line software.

== See also ==
{{multicol}}
;Related diagrams
* [[Control flow diagram]]
* [[Control flow graph]]
* [[Data flow diagram]]
* [[Deployment flowchart]]
* [[Flow map]]
* [[Functional flow block diagram]]
* [[Nassi-Shneiderman diagram]]
* [[N2 Chart]]
* [[Petri nets]]
* [[Sankey diagram]]
* [[State diagram]]
* [[Warnier-Orr]]
{{multicol-break}}
;Related subjects
* [[Augmented transition network]]
* [[Business process illustration]]
* [[Business Process Mapping]]
* [[Diagramming software]]
* [[Process architecture]]
* [[Pseudocode]]
* [[Recursive transition network]]
* [[Unified Modeling Language]] (UML)
{{multicol-end}}

== Notes ==
{{Reflist}}

== Further reading ==
*{{cite book  | last = ISO  | year = 1985  | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=11955  | title = Information processing -- Documentation symbols and conventions for data, program and system flowcharts, program network charts and system resources charts | publisher = International Organization for Standardization| id = ISO 5807:1985}}

== External links ==
{{Commons|Flowchart}}
*[http://www.fh-jena.de/~kleine/history/software/IBM-FlowchartingTechniques-GC20-8152-1.pdf  Flowcharting Techniques] An IBM manual from 1969 (5MB PDF format)
*[http://www.allclearonline.com/applications/DocumentLibraryManager/upload/program_intro.pdf Introduction to Programming in C++ flowchart and pseudocode] (PDF)
* [http://www.tipskey.com/manufacturing/advanced_flowchart.htm Advanced Flowchart] - Why and how to create advanced flowchart

[[Category:Algorithms]]
[[Category:Quality control tools]]
[[Category:Diagrams]]
[[Category:Technical communication]]
[[Category:Computer programming]]
[[Category:Articles with example code]]
[[Category:American inventions]]

[[ar:خارطة انسياب]]
[[bs:Dijagram toka]]
[[bg:Блок схема]]
[[cs:Vývojový diagram]]
[[de:Programmablaufplan]]
[[el:Διάγραμμα ροής]]
[[es:Diagrama de flujo]]
[[eo:Fludiagramo]]
[[fa:روندنما]]
[[fr:Organigramme de programmation]]
[[hi:फ्लो चार्ट]]
[[id:Diagram alur]]
[[is:Flæðirit]]
[[it:Diagramma a blocchi]]
[[he:תרשים זרימה]]
[[ka:ბლოკ-სქემა]]
[[lt:Blokinė schema]]
[[nl:Stroomdiagram]]
[[ja:フローチャート]]
[[pms:Diagrama ëd fluss]]
[[pl:Schemat blokowy]]
[[pt:Fluxograma]]
[[ru:Блок-схема]]
[[sl:Diagram poteka]]
[[sv:Flödesschema]]
[[ta:செயல்வழிப் படம்]]
[[tr:Akış şeması]]
[[uk:Блок-схема]]
[[zh:流程图]]</text>
    </revision>
  </page>
  <page>
    <title>Landau's algorithm</title>
    <id>24610350</id>
    <revision>
      <id>318885346</id>
      <timestamp>2009-10-09T15:44:47Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <text xml:space="preserve">In mathematics, '''Landau's algorithm''', named after [[Susan Landau]], is an [[algorithm]] for deciding which [[nested radical]]s can be denested.&lt;ref&gt;{{citation
|first=Susan|last=Landau
|title=Simplification of nested radicals
|journal=[[SIAM Journal on Computing]]
|volume=21
|number=1
|year=1992
|pages=85–110
|doi=10.1137/0221009}} ([http://www2.computer.org/portal/web/csdl/doi/10.1109/SFCS.1989.63496 link to a conference version that can be viewed by anyone])&lt;/ref&gt;

== Notes and references ==

{{reflist}}

* [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.5512&amp;rep=rep1&amp;type=pdf Susan Landau, &quot;A note on 'Zippel Denesting'&quot;]
* Susan Landau, &quot;How to Tangle with a Nested Radical.&quot; ''[[Mathematical Intelligencer]]'', 16, 49&amp;ndash;55, 1994.

{{algebra-stub}}

[[Category:Algorithms]]
[[Category:Algebra]]
[[Category:Algebraic number theory]]</text>
    </revision>
  </page>
  <page>
    <title>The Art of Computer Programming</title>
    <id>31358</id>
    <revision>
      <id>328318703</id>
      <timestamp>2009-11-28T03:56:03Z</timestamp>
      <contributor>
        <username>Ninly</username>
        <id>368533</id>
      </contributor>
      <minor/>
      <comment>/* Critical response */ wp date/era style</comment>
      <text xml:space="preserve">[[Image:ArtOfComputerProgramming.jpg|right|thumb|200px|Third edition of volume 1]]
'''''The Art of Computer Programming'''''&lt;ref&gt;[http://www-cs-faculty.stanford.edu/~uno/taocp.html The Art of Computer Programming&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; is a comprehensive [[monograph]] written by [[Donald Knuth]] that covers many kinds of [[programming]] [[algorithm]]s and their analysis. 
At the end of 1999, it was named among the best twelve physical-science monographs of the century by American Scientist&lt;ref&gt;http://www.americanscientist.org/bookshelf/pub/100-or-so-books-that-shaped-a-century-of-science&lt;/ref&gt;, along with: Dirac on quantum mechanics, Einstein on relativity, Mandelbrot on fractals, Pauling on the chemical bond, Russell and Whitehead on foundations of mathematics, von Neumann and Morgenstern on game theory, Wiener on cybernetics, Woodward and Hoffmann on orbital symmetry, Feynman on quantum electrodynamics, Smith on the search for structure, and Einstein's collected papers.
Knuth began the project, originally conceived as a single book, in 1962.  The first three of what
were then expected to be seven volumes were published in rapid succession in 1968, 1969, and 1973.  The first installment of Volume 4 was not published until February 2005. Additional installments are planned for release approximately biannually with a break before [[fascicle]] 5 to finish the &quot;Selected Papers&quot; series.&lt;ref&gt;http://webofstories.com/person.html?cat=3&amp;pers=1038&amp;st=17155&lt;/ref&gt;

==History==
[[Image:KnuthAtOpenContentAlliance.jpg|right|thumb|200px|Donald Knuth in 2005]]
Considered an expert at writing [[compilers]], Knuth started to write a book about compiler design in 1962, and soon realized that the scope of the book needed to be much larger.  In June 1965, Knuth finished the first draft of what was originally planned to be a single volume of twelve chapters. His hand-written manuscript was 3,000 pages long: he had assumed that about five hand-written pages would translate into one printed page, but his publisher said instead that about 1½ hand-written pages translated to one printed page. This meant the book would be approximately 2,000 pages in length. At this point, the plan was changed: the book would be published in seven volumes, each with just one or two chapters.  Due to the growth in the material, the plan for Volume 4 has since expanded to include Volumes 4A, 4B, 4C, and possibly 4D. 

In 1976, Knuth prepared a second edition of Volume 2, requiring it to be [[typesetting|typeset]] again, but the style of type used in the first edition (called [[Hot metal typesetting|hot type]]) was no longer available.  In 1977, he decided to spend a few months working up something more suitable.  Eight years later, he returned with [[TeX]], which is currently used for all volumes.

The famous offer of a [[Knuth reward check|reward check]] worth &quot;one hexadecimal dollar&quot; (0x100 [[Hexadecimal|Base 16]] cents, in [[decimal]], is $2.56) for any errors found, and the correction of these errors in subsequent printings, has contributed to the highly polished and still-authoritative nature of the work, long after its first publication. Another characteristic of the volumes is the variation in the difficulty of the exercises. The level of difficulty ranges from &quot;warm-up&quot; exercises to unsolved research problems, providing a challenge for any reader. Knuth's dedication is also famous: 

&lt;blockquote&gt;This series of books is affectionately dedicated&lt;br&gt;to the [[IBM 650|Type 650 computer]] once installed at&lt;br&gt;[[Case Institute of Technology]],&lt;br&gt;in remembrance of many pleasant evenings.&lt;ref group=&quot;nb&quot;&gt;The dedication was worded slightly differently in the first edition.&lt;/ref&gt;&lt;/blockquote&gt;

==Assembly language in the book==
All examples in the books use a language called &quot;MIX assembly language&quot;, which runs on the hypothetical [[MIX]] computer. (Currently, the MIX computer is being replaced by the [[MMIX]] computer, which is a [[RISC]] version.) Software such as [[GNU MDK]] exists to provide emulation of the MIX architecture.

Some readers are put off by the use of [[assembly language]], but Knuth considers this necessary because algorithms need to be in context in order for their speed and memory usage to be judged.  This does, however, limit the accessibility of the book for many readers, and limits its usefulness as a &quot;cookbook&quot; for practicing programmers, who may not be familiar with assembly, or who may have no particular desire to translate assembly language code into a high-level language. A number of more accessible algorithms textbooks using high-level language examples exist and are popular for precisely these reasons.

==Critical response==
''[[American Scientist]]'' has included this work among the best twelve physical-science [[monograph]]s of the 20th century,&lt;ref&gt;{{citation |first= Philip |last=Morrison |first2=Phylis |last2=Morrison |url=http://www.americanscientist.org/bookshelf/pub/100-or-so-books-that-shaped-a-century-of-science|title=100 or so Books that shaped a Century of Science |volume=87 |issue=6 |journal=American Scientist |year=1999 |month=November-December |publisher=Sigma Xi, The Scientific Research Society |accessdate=2008-01-11 }}&lt;/ref&gt; and within the computer science community it is regarded as the first and still the best comprehensive treatment of its subject. Covers of the third edition of Volume 1 quote [[Bill Gates]] as saying, &quot;If you think you're a really good programmer .&amp;nbsp;.&amp;nbsp;. read (Knuth's) Art of Computer Programming .&amp;nbsp;.&amp;nbsp;. You should definitely send me a resume if you can read the whole thing.&quot; (According to folklore, [[Steve Jobs]] made this claim.&lt;ref&gt;{{cite web |url=http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Close_Encounters_of_the_Steve_Kind.txt |title=Close Encounters of the Steve Kind |first=Tom |last=Zito |accessdate=2008-01-11 |work=folklore.org }}&lt;/ref&gt;)

==Chapter outline of published and unpublished volumes==
*Volume 1 - Fundamental Algorithms
**Chapter 1 - Basic concepts
**Chapter 2 - Information structures
*Volume 2 - Seminumerical Algorithms
**Chapter 3 - [[Random number]]s
**Chapter 4 - Arithmetic
*Volume 3 - Sorting and Searching
**Chapter 5 - [[Sorting]]
**Chapter 6 - [[Searching]]
*Volume 4 - [[combinatorics|Combinatorial]] Algorithms, in preparation (five [[fascicle]]s have been published as of April 2009) and alpha-test versions of additional fascicles are downloadable from Knuth's page [[#External links|below]]).
**Volume [[#Outline of Volume 4A Enumeration and Backtracking|4A]] - [[Enumeration]] and [[Backtracking]]
***Chapter 7 - Combinatorial searching
**Volume 4B - [[graph theory|Graph]] and [[Network flow|Network]] Algorithms 
***Chapter 7 ''continued''
**Volume 4C and possibly 4D - Optimization and [[Recursion]]
***Chapter 7 ''continued''
***Chapter 8 - Recursion
*Volume 5 - Syntactic Algorithms, planned (as of August 2006, ''estimated'' in 2015). 
**Chapter 9 - Lexical scanning
**Chapter 10 - Parsing techniques
*Volume 6 - Theory of [[Context-free language|Context-Free Language]]s, planned.
*Volume 7 - Compiler Techniques, planned.

==Detailed outline of unpublished Volume 4==
===Subvolume 4A Enumeration and Backtracking===
*7 - Introduction (82pp) - published in Volume 4, Fascicle 0
**7.1 - Zeros and ones
***7.1.1 - Boolean basics (88 pp) - published in Volume 4, Fascicle 0
***7.1.2 - Boolean evaluation (67 pp) - published in Volume 4, Fascicle 0
***7.1.3 - Bitwise tricks and techniques (122 pp) - published in Volume 4, Fascicle 1
***7.1.4 - [[Binary decision diagrams]] (150 pp) - published in Volume 4, Fascicle 1
**7.2 - Generating all possibilities
***7.2.1 - Combinatorial generators (397 pp)
****7.2.1.1 - Generating all n-tuples - published in Volume 4, Fascicle 2 
****7.2.1.2 - Generating all permutations - published in Volume 4, Fascicle 2 
****7.2.1.3 - Generating all combinations - published in Volume 4, Fascicle 3 
****7.2.1.4 - Generating all partitions - published in Volume 4, Fascicle 3 
****7.2.1.5 - Generating all set partitions - published in Volume 4, Fascicle 3 
****7.2.1.6 - Generating all trees - published in Volume 4, Fascicle 4 
****7.2.1.7 - History and further references - published in Volume 4, Fascicle 4
***7.2.2 - Basic backtrack 
***7.2.3 - Efficient backtracking
**7.3 - Shortest paths

===Subvolume 4B Graph and Network Algorithms===
**7.4 - Graph algorithms
***7.4.1 - Components and traversal
***7.4.2 - Special classes of graphs
***7.4.3 - Expander graphs
***7.4.4 - Random graphs
**7.5 - Network algorithms
***7.5.1 - Distinct representatives
***7.5.2 - The assignment problem
***7.5.3 - Network flows
***7.5.4 - Optimum subtrees
***7.5.5 - Optimum matching
***7.5.6 - Optimum orderings
**7.6 - Independence theory
***7.6.1 - Independence structures
***7.6.2 - Efficient matroid algorithms

===Subvolumes 4C and 4D Optimization and Recursion===
**7.7 - Discrete [[dynamic programming]]
**7.8 - [[Branch-and-bound]] techniques
**7.9 - Herculean tasks (aka [[NP-hard]] problems)
**7.10 - [[Approximation algorithm|Near-optimization]]
*8 - [[Recursion]]

==English editions==
===Current editions===
In order by volume number:

*''Volume 1: Fundamental [[Algorithm]]s''. Third Edition (Reading, Massachusetts: Addison-Wesley, 1997), xx+650pp. ISBN 0-201-89683-4
*''Volume 1, [[Fascicle]] 1: [[MMIX]] -- A [[RISC]] Computer for the New Millennium''. (Addison-Wesley, February 14, 2005)  ISBN 0-201-85392-2 (will be in the fourth edition of volume 1)
*''Volume 2: Seminumerical Algorithms''. Third Edition (Reading, Massachusetts: Addison-Wesley, 1997), xiv+762pp. ISBN 0-201-89684-2
*''Volume 3: [[Sorting algorithm|Sorting]] and [[Search algorithms|Searching]]''. Second Edition (Reading, Massachusetts: Addison-Wesley, 1998), xiv+780pp.+foldout. ISBN 0-201-89685-0
*''Volume 4, Fascicle 0: Introduction to [[Combinatorics|Combinatorial Algorithms]] and [[Boolean]] [[Function (mathematics)|Functions]]'', (Addison-Wesley Professional, April 28, 2008) vi+240pp, ISBN 0-321-53496-4
*''Volume 4, Fascicle 1: Bitwise tricks &amp; techniques;  Binary Decision Diagrams'' (Addison-Wesley Professional, March 27, 2009) viii+260pp, ISBN 0-321-58050-8
*''Volume 4, Fascicle 2: Generating All [[Tuple]]s and [[Permutation]]s'', (Addison-Wesley, February 14, 2005) v+127pp, ISBN 0-201-85393-0
*''Volume 4, Fascicle 3: Generating All [[Combination]]s and [[Partition (number theory)|Partition]]s''. (Addison-Wesley, July 26, 2005) vi+150pp, ISBN 0-201-85394-9
*''Volume 4, Fascicle 4: Generating all [[Tree (graph theory)|Trees]] -- History of Combinatorial Generation'', (Addison-Wesley, February 6, 2006) vi+120pp, ISBN 0-321-33570-8

===Previous editions===
In order by publication date:
*''Volume 1'', first edition, 1968, xxi+634pp, ISBN 0-201-03801-3.
*''Volume 2'', first edition, 1969, xi+624pp, ISBN 0-201-03802-1.
*''Volume 3'', first edition, 1973, xi+723pp+centerfold, ISBN 0-201-03803-X
*''Volume 1'', second edition, 1973, xxi+634pp, ISBN 0-201-03809-9.
*''Volume 2'', second edition, 1981, xiii+ 688pp, ISBN 0-201-03822-6.

==Notes==
&lt;references group=&quot;nb&quot; /&gt;

==Inline and general references==
{{Reflist}}
{{Refbegin}}
* {{cite book |title=Portraits in Silicon |first=Robert |last=Slater |year=1987 |publisher=MIT Press |isbn=0-262-19262-4 }}
* {{cite book |title=Out of Their Minds: The Lives and Discoveries of 15 Great Computer Scientists |first=Dennis |last=Shasha |coauthors=Cathy Lazere |year=1995 |publisher=Copernicus |isbn=0-387-97992-1 }}
{{Refend}}

==External links==
*[http://www-cs-faculty.stanford.edu/~knuth/taocp.html Overview of topics] (Knuth's personal homepage)
*[http://www.cbi.umn.edu/oh/display.phtml?id=319 Oral history interview with Donald E. Knuth] at [[Charles Babbage Institute]], University of Minnesota, Minneapolis.  Knuth discusses software patenting, [[structured programming]], collaboration and his development of [[TeX]]. The oral history discusses the writing of ''[[The Art of Computer Programming]]''.
*[http://sigact.acm.org/floyd/  &quot;Robert W Floyd, In Memoriam&quot;, by Donald E. Knuth] -(on the influence of [[Robert Floyd|Bob Floyd]])
*[http://tal.forum2.org/injokes/art Injokes in the books]
*[http://gosper.org/bill.html Who is Bill Gosper?] (on the influence of [[Bill Gosper]] on the 2nd Edition of Volume 2.)
*[http://www.softpanorama.org/People/Knuth/taocp.shtml TAoCP and its Influence of Computer Science(Softpanorama)]

{{DEFAULTSORT:Art Of Computer Programming, The}}
[[Category:1968 books]]
[[Category:1969 books]]
[[Category:1973 books]]
[[Category:1981 books]]
[[Category:Computer books]]
[[Category:Computer programming]]
[[Category:Computer science books]]
[[Category:Algorithms]]
[[Category:Analysis of algorithms]]
[[Category:Monographs]]
[[Category:Books by Donald Knuth]]
[[Category:Addison-Wesley books]]

&lt;!-- Interwikis--&gt;
[[ar:فن برمجة الحاسوب]]
[[ca:The Art of Computer Programming]]
[[cs:The Art of Computer Programming]]
[[de:The Art of Computer Programming]]
[[fr:The Art of Computer Programming]]
[[ko:컴퓨터 프로그래밍의 예술]]
[[hr:The Art of Computer Programming]]
[[id:The Art of Computer Programming]]
[[ja:The Art of Computer Programming]]
[[pl:Sztuka programowania]]
[[pt:The Art of Computer Programming]]
[[ro:Arta programării calculatoarelor]]
[[ru:Искусство программирования]]
[[sk:The Art of Computer Programming]]
[[sv:The Art of Computer Programming]]
[[tr:The Art of Computer Programming]]
[[vi:The Art of Computer Programming]]
[[zh:计算机程序设计艺术]]</text>
    </revision>
  </page>
  <page>
    <title>Galley division</title>
    <id>1726089</id>
    <revision>
      <id>320650239</id>
      <timestamp>2009-10-18T19:20:15Z</timestamp>
      <contributor>
        <username>Bequw</username>
        <id>1956452</id>
      </contributor>
      <comment>/* How it works */ +other versions</comment>
      <text xml:space="preserve">{{Expert-subject|Mathematics|date=November 2008}}

&lt;!-- Unsourced image removed: [[Image:Galley_Method1.png|thumb|270px|16th century example of galley division]] --&gt;

In [[arithmetic]], the '''galley method''', also known as the '''batello''' or the '''scratch method''', was the most widely used method of [[division (mathematics)|division]] in use prior to 1600.  The names [[galea (boat)|galea]] and batello refer to a boat which the outline of the work was thought to resemble.

An earlier version of this method was used as early as 825 by [[Al-Khwarizmi]].  The Galley method is thought to be [[Hindu]] in origin and is most effective when used on a sand [[abacus]].

The galley method writes few figures than [[long division]], and results in interesting shapes and pictures as it expands both above and below the initial lines.  It was the preferred method of division for 17 centuries longer than long division's 4 centuries.

==How it works==
[[Image:Galley Method3.png|thumb|right|280px|65284/594 using galley division]]
[[Image:Galley Method4.png|thumb| right |180px|The completed problem]]
[[Image:Galley Method2.png|thumb|right|110px|65284/594 using &quot;modern&quot; long division for comparison]]
Set up the problem by writing the dividend and then a bar. The quotient will be written after the bar. Steps:
:(a1) Write the divisor below the dividend. Align the divisor so that it is smaller than the digits of the dividend above and to the left (if the divisor is 700, for instance, it would be written an additional space to the right, so that the &quot;7&quot; would appear below the&amp;nbsp;&quot;5&quot;). 
:(a2) Dividing 652 by 594 yields the quotient 1 which is written to the right of the bar.
Now multiply each digit of the divisor by the new digit of the quotient and subtract that from the left-hand segment of the dividend. Where the subtrahend and the dividend segment differ, cross out the dividend digit and write if necesary the subtrahend digit and next vertical empty space. Cross out the divisor digit used.
:(b) Compute 6&amp;nbsp;&amp;minus;&amp;nbsp;5&amp;times;1&amp;nbsp;=&amp;nbsp;1. Cross out the 6 of the divident and above it write a 1. Cross out the 5 of the divisor. The resulting dividend is now read off as the topmost un-crossed digits: 15284.
:(c) Using the left-hand segment of the resulting dividend we get 15&amp;nbsp;&amp;minus;&amp;nbsp;9&amp;times;1&amp;nbsp;=&amp;nbsp;6. Cross out the 1 and 5 and write 6 above. Cross out the 9. The resulting dividend is 6284.
:(d) Compute 62&amp;nbsp;&amp;minus;&amp;nbsp;4&amp;times;1&amp;nbsp;=&amp;nbsp;58. Cross out the 6 and 2 and write 5 and 8 above. Cross out the 4. The resulting dividend is 5884.
:(e) Write the divisor one step to the right of where it was originally written using empty spaces below existing crossed out digits. 
:(f1) Dividing 588 by 594 yields 0 which is written as the new digit of the quotient. 
:(f2) As 0 times any digit of the divisor is 0, the dividend will remain unchanged. We therefore can cross out all the digits of the divisor.
:(f3) We write the divisor again one space to the right
:(omitted) Dividing 5884 by 594 yields 9 which is written as the new digit of the quotient. 58&amp;nbsp;&amp;minus;&amp;nbsp;5&amp;times;9&amp;nbsp;=&amp;nbsp;13 so cross out the 5 and 8 and above them write 1 and 3. Cross out the 5 of the divisor. The resulting divident is now 1384. 138&amp;nbsp;&amp;minus;&amp;nbsp;9&amp;times;9&amp;nbsp;=&amp;nbsp;57. Cross out 1,3, and 8 of the dividend and write 5 and 7 above. Cross out the 9 of the divisor. The resulting dividend is 574. 574&amp;nbsp;&amp;minus;&amp;nbsp;4&amp;times;9&amp;nbsp;=&amp;nbsp;538. Cross out the 7 and 4 of the dividend and write 3 and 8 above them. Cross out the 4 of the divisor. The resulting dividend is 538. The process is done, the quotient is 109 and the remainder is 538.
===Other versions===

The above is called the cross-out version and is the most common. An erasure version exists for situations where erasure is acceptable and there is not need to keep track of the intermediate steps. This is the method used with a sand abacus. Finally, there is a printers method that uses neither erasure or crossouts. Only the top digit in each column of the dividend is active with a zero used to denote an entirely inactive column.
{| border=&quot;0&quot; cellpadding=&quot;5&quot; cellspacing=&quot;5&quot; align=&quot;center&quot;
|[[Image:Galley Method erase animated.gif|thumb|left|120px|65284/594 using galley division (erasure version)]]
|[[Image:Galley Method printers final.png|thumb|left|120px|65284/594 using galley division (printers version)]]
|}

==Modern usage==
Galley division was the favorite method of division with arithmeticians through the 18th century and it is thought that it fell out of use due to the lack of cancelled types in printing. It is still taught in the [[Moorish]] schools of [[North Africa]] and other parts of the [[Middle East]].

== See also==
*[[Group (mathematics)|Group]]
*[[Field (algebra)]]
*[[Division algebra]]
*[[Division ring]]
*[[Long division]]
*[[Vinculum (symbol)|Vinculum]]

== External links ==
* [http://www.pballew.net/arithme1.html#divide  Excellent historical exegenesis of the origins of division, including the Galley Method]
* [http://mathforum.org/library/drmath/view/61872.html Galley or Scratch Method of Division] at The Math Forum

[[Category:Arithmetic]]
[[Category:Division]]
[[Category:Algorithms]]
[[Category:History of mathematics]]

[[de:Division (Mathematik)]]
[[es:División]]
[[fr:Division]]
[[is:Deiling]]
[[nl:Delen]]
[[ja:除法]]
[[pl:Dzielenie]]
[[simple:Division]]
[[sv:Division (matematik)]]
[[zh:除法]]</text>
    </revision>
  </page>
  <page>
    <title>Fourier division</title>
    <id>21281625</id>
    <revision>
      <id>320629724</id>
      <timestamp>2009-10-18T17:24:46Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* External links */ Category:Algorithms</comment>
      <text xml:space="preserve">'''Fourier division''' or '''cross division''' is a pencil-and-paper method of [[division (mathematics)|division]] which helps to simplify the process when the divisor has more than two digits. It was invented by [[Joseph Fourier]].

==Method==
The following exposition assumes that the numbers are broken into two-digit pieces, separated by commas: e.g. 3456 becomes 34,56. In general ''x,y'' denotes ''x''&amp;middot;100 + ''y'' and ''x,y,z'' denotes ''x''&amp;middot;10000&amp;nbsp;+&amp;nbsp;''y''&amp;middot;100&amp;nbsp;+&amp;nbsp;''z'', etc. Note that if a number has an odd number of digits, the first digit is on its own: so 345 becomes 3,45.

Suppose that we wish to divide ''c'' by ''a'', to obtain the result ''b''. (So ''a''&amp;nbsp;&amp;times;&amp;nbsp;''b'' = ''c''.)

:&lt;math&gt;\frac{c}{a}=\frac{c_1,c_2,c_3,c_4,c_5\dots}{a_1,a_2,a_3,a_4,a_5\dots}=b_1,b_2,b_3,b_4,b_5\dots = b&lt;/math&gt;

We can find the successive terms ''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;, etc., using the following formulae:

:&lt;math&gt;b_1=\frac{c_1,c_2}{a_1}\mbox{ with remainder }r_1&lt;/math&gt;

:&lt;math&gt;b_2=\frac{r_1,c_3 - b_1\times a_2}{a_1}\mbox{ with remainder }r_2&lt;/math&gt;

:&lt;math&gt;b_3=\frac{r_2,c_4 - b_2\times a_2 - b_1\times a_3}{a_1}\mbox{ with remainder }r_3&lt;/math&gt;

:&lt;math&gt;b_4=\frac{r_3,c_5 - b_3\times a_2 - b_2\times a_3 - b_1\times a_4}{a_1}\mbox{ with remainder }r_4 \dots&lt;/math&gt;

Each time we add a term to the numerator until it has as many terms as ''a''. From then on, the number of terms remains constant, so there is no increase in difficulty. Once we have as much precision as we need, we use an estimate to place the decimal point.

It will often be the case that one of the ''b'' terms will be negative. For example, 93,&amp;minus;12 denotes 9288, while &amp;minus;16,32 denotes &amp;minus;1600&amp;nbsp;+&amp;nbsp;32 or &amp;minus;1568. (Note: 45,&amp;minus;16,32 denotes 448432.) Care must be taken with the signs of the remainders also.

The general term is

:&lt;math&gt;b_i=\frac{r_{i-1},c_{i+1} - \textstyle \sum_{j=2}^i b_{i-j+1}\times a_j}{a_1}\mbox{ with remainder }r_i&lt;/math&gt;

==Example==
Find the reciprocal of [[pi|&amp;pi;]] &amp;asymp; 3.14159.

:&lt;math&gt;\frac{1}{\pi}=\frac{10,00,00\dots}{31,41,59\dots}=b_1,b_2,b_3\dots = b&lt;/math&gt;

:&lt;math&gt;b_1=\frac{10,00}{31}=32\mbox{ with remainder }8&lt;/math&gt;

:&lt;math&gt;b_2=\frac{8,00 - 32\times 41}{31}=\frac{-512}{31}=-17\mbox{ with remainder }15&lt;/math&gt;

:&lt;math&gt;b_3=\frac{15,00 + 17\times 41 - 32\times 59}{31}=\frac{309}{31}=10\mbox{ with remainder }-1.&lt;/math&gt;

The result is 32,-17,10 or 31,83,10 yielding 0.318310.

==Bibliography==
* Ronald W Doerfler. ''Dead Reckoning: Calculating without Instruments.'' Gulf Publishing, 1993.

==External links==
*Alternative Division Algorithms: [http://www.doubledivision.org Double Division], [http://www.math.nyu.edu/~braams/links/em-arith.html Partial Quotients &amp; Column Division], [http://mb.msdpt.k12.in.us/Math/PartialQuotients.wmv Partial Quotients Movie]

[[Category:Division]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Interpolation search</title>
    <id>69895</id>
    <revision>
      <id>327303186</id>
      <timestamp>2009-11-22T16:06:42Z</timestamp>
      <contributor>
        <username>Pichpich</username>
        <id>6196463</id>
      </contributor>
      <comment>+ alt name</comment>
      <text xml:space="preserve">{{Unreferenced|date=April 2009}}
{{Original research|date=April 2009}}
'''Interpolation search''' (sometimes referred to as '''extrapolation search''') is an [[algorithm]] for [[Search algorithm|searching]] for a given key value in an indexed array that has been [[Collation|ordered]] by the values of the key. It parallels how humans search through a telephone book for a particular name, the key value by which the book's entries are ordered. In each search step it calculates where in the remaining [[search space]] the sought item might be based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. Only if calculations on the size of differences between key values are sensible will this method work.

By comparison, the [[binary search]] chooses always the middle of the remaining search space, discarding one half or the other, again depending on the comparison between the key value found at the estimated position and the key value sought. The remaining search space is reduced to the part before or after the estimated position. The [[linear search]] uses equality only as it compares elements one-by-one from the start, ignoring any sorting.

On average the interpolation search makes about log(log(''n'')) comparisons (if the elements are uniformly distributed), where ''n'' is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to [[Big O notation|O]](''n'') comparisons.

==Performance==

In reality, an interpolation search is often no faster than a binary search due to the complexity of the arithmetic calculations for estimating the next probe position {{Fact|date=March 2009}}. The key may not be easily regarded as a number, as when it is text such as names in a telephone book. Yet it is always possible to regard a bit pattern as a number, and text could be considered a number in base twenty-six (or however many distinct symbols are allowable) that could then be used in the interpolation arithmetic, but each such conversion is more trouble than a few binary chop probes. Nevertheless, a suitable numeric value could be pre-computed for every name and searching would employ those numerical values. Further, it may be easy enough to form a cumulative histogram of those values that follows a shape that can be approximated by a simple function (such as a straight line) or a few such curves over segments of the range. If so, then a suitable inverse interpolation is easily computed and the desired index found with very few probes. The particular advantage of this is that a probe of a particular index's value may involve access to slow storage (on disc, or not in the on-chip memory) whereas the interpolation calculation involves a small local collection of data which with many searches being performed is likely in fast storage. This approach shades into being a special-case [[Hash table]] search.

Such analysis should also detect the troublesome case of equal key values. If a run of equal key values exists, then the search will not necessarily select the first (or last) element of such a run, and if not carefully written, runs the risk of attempting a divide by zero during its interpolation calculation.

In the simple case, no analysis of values still less cumulative histogram approximations are prepared. With a sorted list, clearly the minimum value is at index one, and the maximum value is at index ''n''. Assuming a uniform distribution of values amounts to assuming that the cumulative histogram's shape is a straight line from the minimum to the maximum and so linear interpolation will find that index whose associated value should be the sought value.

Using big-O notation, the performance of the interpolation algorithm can be shown to be O(log log N).&lt;ref&gt;Weiss, Mark Allen (2006). ''Data structures and problem solving using Java'', Pearson Addison Wesley&lt;/ref&gt;

==Sample implementation==

The following code example for the [[Java (programming language)|Java programming language]] is a simple implementation. At each stage it computes a probe position then as with the binary search, moves either the upper or lower bound in to define a smaller interval containing the sought value. Unlike the binary search which guarantees a halving of the interval's size with each stage, a misled interpolation may reduce/increase the mid index by only one, thus resulting in a worst-case efficiency of O(''n'').

&lt;source lang=&quot;Java&quot;&gt;
 public int interpolationSearch(int[] sortedArray, int toFind){
  // Returns index of toFind in sortedArray, or -1 if not found
  int low = 0;
  int high = sortedArray.length - 1;
  int mid;
 
  while (sortedArray[low] &lt;= toFind &amp;&amp; sortedArray[high] &gt;= toFind) {
   mid = low + ((toFind - sortedArray[low]) * (high - low)) / (sortedArray[high] - sortedArray[low]);
 
   if (sortedArray[mid] &lt; toFind)
    low = mid + 1;
   else if (sortedArray[mid] &gt; toFind)      // Repetition of the comparison code is forced by syntax limitations.
    high = mid - 1;
   else
    return mid;
  }
 
  if (sortedArray[low] == toFind)
   return low;
  else
   return -1; // Not found
 }

&lt;/source&gt;
Notice that having probed the list at index ''mid'', for reasons of loop control administration, this code sets either ''high'' or ''low'' to be not ''mid'' but an adjacent index, which location is then probed during the next iteration. Since an adjacent entry's value will not be much different the interpolation calculation is not much improved by this one step adjustment, at the cost of an additional reference to distant memory such as disc.

Each iteration of the above code requires between five and six comparisons (the extra is due to the repetitions needed to distinguish the three states of &lt; &gt; and = via binary comparisons in the absence of a [[three-way comparison]]) plus some messy arithmetic, while the binary search algorithm can be written with one comparison per iteration and uses only trivial integer arithmetic. It would thereby search an array of a million elements with no more than twenty comparisons (involving accesses to slow memory where the array elements are stored); to beat that the interpolation search as written above would be allowed no more than three iterations.

==Book-based searching==
The conversion of names in a telephone book to some sort of number clearly will not provide numbers having a uniform distribution (except via immense effort such as sorting the names and calling them name #1, name #2, etc.) and further, it is well-known that some names are much more common than others (Smith, Jones,) Similarly with dictionaries, where there are many more words starting with some letters than others. Some publishers go to the effort of preparing marginal annotations or even cutting into the side of the pages to show markers for each letter so that at a glance a segmented interpolation can be performed.

==See also==
*[[Linear search]]
*[[Binary search]]
*[[Ternary search]]
*[[Hash table]]

==External links==
*[http://www.dcc.uchile.cl/~rbaeza/handbook/algs/3/322.srch.p.html Interpolation search]
*[http://www.nist.gov/dads/HTML/interpolationSearch.html National Institute of Standards and Technology]

[[Category:Search algorithms]] | [[Category:Algorithms]]

&lt;references/&gt;

[[de:Interpolationssuche]]
[[fa:جستجوی درون‌یابی]]
[[ru:Интерполирующий поиск]]</text>
    </revision>
  </page>
  <page>
    <title>One-pass algorithm</title>
    <id>8756788</id>
    <revision>
      <id>323674619</id>
      <timestamp>2009-11-03T11:52:35Z</timestamp>
      <contributor>
        <username>Rich Farmbrough</username>
        <id>82835</id>
      </contributor>
      <minor/>
      <comment>clean up, added [[:Category:Orphaned articles|orphan]] tag using [[Project:AutoWikiBrowser|AWB]]</comment>
      <text xml:space="preserve">{{Unreferenced|date=January 2007}}
{{orphan|date=November 2009}}

In computing, a '''one-pass algorithm''' is one which reads its input exactly once, in order, without unbounded [[Buffer (computer science)|buffering]].  A one-pass algorithm generally requires O(n) (see [[Big O Notation|'big O' notation]]) time and less than O(n) storage (typically O(1)), where n is the size of the input.

==Example problems solvable by one-pass algorithms==
Given any list as an input:
* Count the number of elements.
* Find the ''n''th element (or report that the list has fewer than ''n'' elements).
* Find the ''n''th element from the end (or report that the list has fewer than ''n'' elements).

Given a list of numbers:
* Find the ''k'' largest or smallest elements, ''k'' given in advance.
* Find the [[sum]], [[mean]], [[variance]] and [[standard deviation]] of the elements of the list.

Given a list of symbols from an alphabet of ''k'' symbols, given in advance.
* Count the number of times each symbol appears in the input.
* Find the most or least frequent elements.
* Sort the list according to some order on the symbols (possible since the number of symbols is limited).
* Find the maximum gap between two appearances of a given symbol.

==Example problems not solvable by one-pass algorithms==
Given any list as an input:
* Find the middle element of the list.

Given a list of numbers:
* Find the [[median]].
* Find the [[mode (statistics)|modes]] (This is not the same is finding the most frequent symbol from a limited alphabet).
* Sort the list.

{{DEFAULTSORT:One-Pass Algorithm}}
[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Reservoir sampling</title>
    <id>25190127</id>
    <revision>
      <id>327701013</id>
      <timestamp>2009-11-24T17:55:44Z</timestamp>
      <contributor>
        <username>MuffledThud</username>
        <id>8756489</id>
      </contributor>
      <minor/>
      <comment>Quick-adding category [[:Category:Algorithms|Algorithms]] (using [[WP:HOTCAT|HotCat]])</comment>
      <text xml:space="preserve">Reservoir sampling is a a family of [[Randomized_algorithms]] for randomly choosing k samples from a list of S items where S is either a very large or unknown number. Typically S is large enough that the list doesn't fit into main memory. The most common example was labelled Algorithm R by [[Jeffrey_Vitter]] in his paper&lt;ref&gt;[http://www.cs.umd.edu/~samir/498/vitter.pdf Random Sampling with a Reservoir]&lt;/ref&gt; on the subject.

This simple O(n) algorithm as described in the Dictionary of Algorithms and Data Structures&lt;ref&gt;[http://www.itl.nist.gov/div897/sqg/dads/HTML/reservoirSampling.html  Dictionary of Algorithms and Data Structures]&lt;/ref&gt; consists of the following steps:
&lt;pre&gt;
Fill up the 'reservoir' with the first k items from S
For every item j from k+1 to length(S):
    Choose an integer between 0 and j's position in S - 1
    If the integer chosen is in the reservoir then replace that element in the reservoir with j
&lt;/pre&gt;

The following is a simple implementation of the algorithm in Python that samples the set of English Wikipedia page titles: 
&lt;pre&gt;
import random
SAMPLE_COUNT = 10

# Force the value of the seed so the results are repeatable
random.seed(12345)

sample_titles = []
for index, line in enumerate(open(&quot;enwiki-20091103-all-titles-in-ns0&quot;)):
        # Generate the reservoir
        if index &lt; SAMPLE_COUNT:
                sample_titles.append(line)
        else:                   
                # Randomly replace elements in the reservoir with a decreasing probability                
                # Choose an integer between 0 and (index - 1) (inclusive)                
                r = random.randint(0, index)                
                if r &lt; SAMPLE_COUNT:                        
                        sample_titles[r] = line
print sample_titles

&lt;/pre&gt;

==References==
{{reflist}}

[[Category:Algorithms]]</text>
    </revision>
  </page>
  <page>
    <title>Algorithmic version for Szemerédi regularity partition</title>
    <id>22721120</id>
    <revision>
      <id>328038484</id>
      <timestamp>2009-11-26T13:24:57Z</timestamp>
      <contributor>
        <username>RobinK</username>
        <id>10041416</id>
      </contributor>
      <minor/>
      <comment>Quick-adding category [[:Category:Algorithms|Algorithms]] (using [[WP:HOTCAT|HotCat]])</comment>
      <text xml:space="preserve">{{Wikify|date=May 2009}}

This paper 
&lt;ref&gt;
{{cite article
|author= A. Frieze and R. Kannan
|title=A Simple Algorithm for Constructing Szemerédi's Regularity Partition
|journal=  Electr. J. Comb.
|volume = 6
|year= 1999
|url=http://www.math.cmu.edu/~af1p/Texfiles/svreg.pdf}}
&lt;/ref&gt;
is a combined work by [[Alan M. Frieze]] and [[Ravindran Kannan]]. They use two lemmas to derive the algorithmic version of the [[Szemerédi regularity lemma]] to find an &lt;math&gt;\epsilon&lt;/math&gt;-regular partition.

==Formal statement of the regularity lemma==

The formal statement of '''Szemerédi's regularity lemma''' requires some definitions. Let &lt;math&gt; G &lt;/math&gt;  be a graph. The density &lt;math&gt;d(X,Y)&lt;/math&gt; of a pair of disjoint vertex sets &lt;math&gt;X, Y&lt;/math&gt; is defined as,

:&lt;math&gt;
d(X,Y)=E(X,Y)/|X||Y| \,
&lt;/math&gt;

where &lt;math&gt;E(X,Y)&lt;/math&gt; denotes the set of edges having one end vertex in &lt;math&gt;X&lt;/math&gt; and one in &lt;math&gt;Y&lt;/math&gt;. For &lt;math&gt;e &gt; 0&lt;/math&gt;, a pair of vertex sets &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; is called &lt;math&gt;\epsilon&lt;/math&gt;-regular, if for all subsets &lt;math&gt;A&lt;/math&gt; of &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; of &lt;math&gt;Y&lt;/math&gt; satisfying &lt;math&gt;|A|&lt;/math&gt; ≥ &lt;math&gt;\epsilon |X|&lt;/math&gt; and &lt;math&gt;|B|&lt;/math&gt; ≥ &lt;math&gt;\epsilon |Y|&lt;/math&gt;, we have &lt;math&gt;|d(X,Y)-d(A,B)|&lt;/math&gt; ≤ &lt;math&gt;\epsilon&lt;/math&gt;.

A partition of the vertex set of &lt;math&gt;G&lt;/math&gt; into &lt;math&gt;k&lt;/math&gt; sets  is called an &lt;math&gt;\epsilon&lt;/math&gt;-regular partition, if  for all &lt;math&gt;i, j&lt;/math&gt;, and all except of at most &lt;math&gt;k\epsilon^2&lt;/math&gt; pairs &lt;math&gt;V_i, V_j, i &lt; j&lt;/math&gt;, are &lt;math&gt;\epsilon&lt;/math&gt;-regular. Such a partition is called equitable partition.

Now we are ready to state the regularity lemma.

'''Regularity lemma.''' For every &lt;math&gt;\epsilon &gt; 0&lt;/math&gt; and positive integer &lt;math&gt;m&lt;/math&gt; there exist integers &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;M&lt;/math&gt; such that if &lt;math&gt;G&lt;/math&gt; is a graph with at least &lt;math&gt;N&lt;/math&gt; vertices, there exists an integer &lt;math&gt;k&lt;/math&gt; in the range &lt;math&gt;m = k =M&lt;/math&gt; and an &lt;math&gt;\epsilon&lt;/math&gt;-regular partition of the vertex set of &lt;math&gt;G&lt;/math&gt; into &lt;math&gt;k&lt;/math&gt; sets.

It is a common variant in the definition of an &lt;math&gt;\epsilon&lt;/math&gt;-regular partition to require that the vertex sets 
all have the same size, while collecting the leftover vertices in an &quot;error&quot;-set &lt;math&gt;V_0&lt;/math&gt; whose size is 
at most an &lt;math&gt;\epsilon&lt;/math&gt;-fraction of the size of the vertex set of &lt;math&gt;G&lt;/math&gt;.

Szemerédi regularity lemma is one of the most powerful tools of extremal graph theory. It says that, in some sense, 
all graphs can be approximated by random-looking graphs. Therefore the lemma helps in proving theorems for arbitrary graphs whenever the corresponding result is easy for random graphs. The first constructive version was provided by Alon, Duke, Leffman, Rodl and Yuster &lt;ref&gt;
{{cite article
|author=N. Alon and R. A. Duke and H. Lefmann and V. Rödl and R. Yuster,
|title= The Algorithmic Aspects of the Regularity Lemma,
|journal= J. Algorithms,
|year= 1994,
|url= http://www.math.cmu.edu/~af1p/Texfiles/svreg.pdf}}
&lt;/ref&gt;. Subsequently Frieze and Kannan gave a different version and extended it to hypergraphs &lt;ref&gt; 
{{ cite article 
|author= A. Frieze,
|title= The regularity lemma and approximation schemes for dense problems,
|journal= FOCS '96: Proceedings of the 37th Annual Symposium on Foundations of Computer Science,
|year= 1996,
|url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=548459 }}
&lt;/ref&gt;. The paper 
&lt;ref&gt;{{citation|title=Szemeredi's Regularity Lemma and its applications in graph theory|first1=János|last1=Komlós|author1-link=János Komlós (mathematician)|first2=Miklós|last2=Simonovits|publisher=Technical Report: 96-10, [[DIMACS]]|year=1996|url=http://dimacs.rutgers.edu/TechnicalReports/abstracts/1996/96-10.html}}.&lt;/ref&gt; is a nice survey on regularity lemma and its various applications. Here we will briefly describe a different construction due to  Alan Frieze and Ravi Kannan that uses singular values of matrices.

==Constructive version of Szemerédi regularity lemma by Frieze and Kannan==

The algorithm is based on two crucial lemmas:

'''Lemma 1:''' &lt;br /&gt;Fix k and &lt;math&gt;\gamma&lt;/math&gt; and let &lt;math&gt;G=(V,E)&lt;/math&gt; be a graph with &lt;math&gt;n&lt;/math&gt; vertices. Let &lt;math&gt;P&lt;/math&gt; be an equitable partition of &lt;math&gt;V&lt;/math&gt; in classes &lt;math&gt;V_0, V_1, ... ,V_k&lt;/math&gt;. Assume &lt;math&gt;|V_1| &gt; 4^{2k}&lt;/math&gt; and &lt;math&gt;4^k &gt;600 \gamma ^2&lt;/math&gt;. Given proofs that more that &lt;math&gt;\gamma k^2&lt;/math&gt; pairs &lt;math&gt;(V_r,V_s)&lt;/math&gt; are not &lt;math&gt;\gamma&lt;/math&gt;-regular, it is possible to find in O(n) time an equitable partition &lt;math&gt;P'&lt;/math&gt; (which is a refinement of &lt;math&gt;P&lt;/math&gt;) into &lt;math&gt;1+k4^k&lt;/math&gt; classes, with an exceptional class of cardinality at most &lt;math&gt;|V_0|+n/4^k&lt;/math&gt; and such that &lt;math&gt;ind(P')&lt;/math&gt; ≥ &lt;math&gt;ind(P) + \gamma^5/20&lt;/math&gt;

'''Lemma 2:''' &lt;br /&gt;Let &lt;math&gt;W&lt;/math&gt; be a &lt;math&gt;R&lt;/math&gt;×&lt;math&gt;C&lt;/math&gt; matrix with &lt;math&gt;|R|=p&lt;/math&gt;, &lt;math&gt;|C|=q&lt;/math&gt; and &lt;math&gt;\|W\|_\inf\leq1&lt;/math&gt; and &lt;math&gt;\gamma&lt;/math&gt; be a positive real.
&lt;br /&gt;(a) If there exist &lt;math&gt;S&lt;/math&gt; ⊆ &lt;math&gt;R&lt;/math&gt;, &lt;math&gt;T&lt;/math&gt; ⊆ &lt;math&gt;C&lt;/math&gt; such that &lt;math&gt;|S|&lt;/math&gt;≥&lt;math&gt;\gamma p&lt;/math&gt;, &lt;math&gt;|T|&lt;/math&gt;≥&lt;math&gt;\gamma q&lt;/math&gt; and &lt;math&gt;|W(S,T)|&lt;/math&gt;≥&lt;math&gt;\gamma |S||T|&lt;/math&gt; then &lt;math&gt;\sigma_1(W)\geq\gamma^3\sqrt{pq}&lt;/math&gt;
&lt;br /&gt;(b) If &lt;math&gt;\sigma_1(W)\geq\gamma\sqrt{pq}&lt;/math&gt;, then there exist &lt;math&gt;S&lt;/math&gt;⊆&lt;math&gt;R&lt;/math&gt;, &lt;math&gt;T&lt;/math&gt;⊆&lt;math&gt;C&lt;/math&gt; such that &lt;math&gt;|S|&lt;/math&gt;≥&lt;math&gt;\gamma'p&lt;/math&gt;, &lt;math&gt;|T|&lt;/math&gt;≥&lt;math&gt;\gamma'q&lt;/math&gt; and &lt;math&gt;W(S,T)&lt;/math&gt;≥&lt;math&gt;\gamma'|S||T|&lt;/math&gt; where &lt;math&gt;\gamma'=\gamma^3/108&lt;/math&gt;. Furthermore &lt;math&gt;S&lt;/math&gt;, &lt;math&gt;T&lt;/math&gt; can be constructed in polynomial time.

These 2 lemmas are combined in the following algorithmic construction of the [[Szemerédi regularity lemma]].

'''[Step 1]''' Arbitrarily divide the vertices of &lt;math&gt;G&lt;/math&gt; into an equitable partition &lt;math&gt;P_1&lt;/math&gt; with classes &lt;math&gt;V_0,V_1,...,V_b&lt;/math&gt; where &lt;math&gt;|V_i|=\lfloor n/b\rfloor&lt;/math&gt; and hence &lt;math&gt;|V_0|&lt;b&lt;/math&gt;. denote &lt;math&gt;k_1=b&lt;/math&gt;.
&lt;br /&gt;'''[Step 2]''' For every pair &lt;math&gt;(V_r,V_s)&lt;/math&gt; of &lt;math&gt;P_i&lt;/math&gt;, compute &lt;math&gt;\sigma_1(W_{r,s})&lt;/math&gt;. If the pair &lt;math&gt;(V_r,V_s)&lt;/math&gt; are not &lt;math&gt;\epsilon-&lt;/math&gt;regular then by Lemma 2 we obtain a proof that they are not &lt;math&gt;\gamma=\epsilon^9/108-&lt;/math&gt;regular.
&lt;br /&gt;'''[Step 3]''' If there are at most &lt;math&gt;\epsilon{k_1\choose 2}&lt;/math&gt; pairs that produce proofs of non &lt;math&gt;\gamma-&lt;/math&gt;regularity that halt. &lt;math&gt;P_i&lt;/math&gt; is &lt;math&gt;\epsilon-&lt;/math&gt;regular.
&lt;br /&gt;'''[Step 4]''' Apply Lemma 1 where &lt;math&gt;P=P_i&lt;/math&gt;, &lt;math&gt;k=k_i&lt;/math&gt;, &lt;math&gt;\gamma=\epsilon^9/108&lt;/math&gt; and obtain &lt;math&gt;P'&lt;/math&gt; with &lt;math&gt;1+k_i4^{k_i}&lt;/math&gt; classes
&lt;br /&gt;'''[Step 5]''' Let &lt;math&gt;k_i+1 = k_i4^{k_i}&lt;/math&gt;, &lt;math&gt;P_i+1=P'&lt;/math&gt;, &lt;math&gt;i=i+1&lt;/math&gt; and go to Step 2.

The algorithm will terminate with an &lt;math&gt;\epsilon&lt;/math&gt;-regular partition in &lt;math&gt;O(\epsilon^{-45})&lt;/math&gt; steps since the improvement at each step is &lt;math&gt;\gamma^5/20=O(\epsilon^{45})&lt;/math&gt;.

==References==
&lt;references/&gt;

{{DEFAULTSORT:Algorithmic version for Szemeredi regularity partition}}
[[Category:Graph theory]]
[[Category:Algorithms]]</text>
    </revision>
  </page>
</mediawiki>
